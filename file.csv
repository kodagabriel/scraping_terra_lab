Titulo,Imagem,Conteudo
Saiba como implementar o serviço de notificações no seu App React Native utilizando a Firebase,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/09/imagem-blog-e-twitter-1-730x350.jpeg,"No tutorial de hoje, vamos ensinar você a implementar o serviço de notificações em um aplicativo React Native. Para tal, vamos utilizar a funcionalidade Cloud messaging da Firebase, e o módulo React Native Firebase que, na data de conclusão desse tutorial, se encontra na versão 6! Esse guia pode não funcionar em versões futuras. Vamos lá?

Antes de começarmos, esse tutorial parte do princípio de que você tem um projeto React Native e um projeto Firebase ativo. Se não tiver algum desses requisitos, você pode conferir os seguintes tutoriais: 

Iniciando com React Native
Criando um novo projeto Firebase
Instalação 

O módulo app do Firebase deve ser instalado antes de se utilizar qualquer outro serviço disponível pela Firebase.

# Usando npm
npm install --save @react-native-firebase/app

# Usando Yarn
yarn add @react-native-firebase/app
Configurando no Android 

Para permitir que o Android faça uma conexão segura com seu projeto Firebase, um arquivo de configuração deve ser baixado e adicionado ao seu projeto.

No console Firebase, adicione uma nova aplicação Android e entre com os detalhes do seu projeto. O “package name do android” (Android Package Name) deve ser exatamente o que está no package name do seu projeto local. Esse nome pode ser encontrado dentro da tag manifest no arquivo /android/app/src/main/AndroidManifest.xml. O caminho se inicia na pasta raiz do seu projeto. 

Baixe o arquivo google-services.json e coloque dentro do seu projeto na seguinte localização: /android/app/google-services.json.

Configurando o Firebase com as credenciais Android

Para permitir que o Firebase use suas credenciais no Android, o plugin google-services precisa estar ativo nesse projeto. Isso requer que dois arquivos sejam modificados na pasta Android.
Primeiramente, adicione o plugin google-services como uma dependência dentro do arquivo /android/build.gradle:

buildscript {
  dependencies {
    // ... outras dependências
    classpath 'com.google.gms:google-services:4.3.3'
    // Adicione a linha acima --- /\
  }
}

Depois, execute o plugin adicionando o código a seguir ao seu arquivo /android/app/build.gradle

apply plugin: 'com.android.application'
apply plugin: 'com.google.gms.google-services' // <- Adicione essa linha
Configurando no iOS 

No console da Firebase, adicione uma nova aplicação iOS e entre com os detalhes do projeto. O “identificador bundle iOS” (iOS bundle ID) deve ser exatamente o bundle ID do seu projeto local. O bundle ID pode ser encontrado dentro da aba “General” quando seu projeto está aberto no Xcode. 

Baixe o arquivo GoogleService-Info-plist. 

Utilizando o Xcode, abra o arquivo /ios/{projectName}.xcodeproj (ou o arquivo /ios/{projectName.xcworkspace se você estiver utilizando Pods. Essa segunda opção é geralmente a utilizada quando se está desenvolvendo um app em React Native). 
Clique com o botão direito no nome do projeto e em “Adicionar arquivos” (Add files), como demonstrado abaixo:

Selecione o arquivo GoogleService-Info.plist que você salvou no seu computador e certifique-se que a opção “Copiar itens se necessário” (Copy items if needed) está marcada.

Configurando o Firebase com as credenciais iOS

Para permitir que o Firebase use suas credenciais no iOS, o SDK do Firebase deve ser configurado durante a fase bootstrap do seu aplicativo. 

Para fazer isso, abra o seu arquivo /ios/{projectName}/AppDelegate.m e siga os passos abaixo.

No início do arquivo, importe o Firebase SDK:

#import <Firebase.h>

Dentro da função já existente didFinishLauchingWithOptions, adicione as linhas indicadas no começo do método: 

- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions {
  // Adicione essa parte --- \/
  if ([FIRApp defaultApp] == nil) {
    [FIRApp configure];
  }
  // Adicione essa parte --- /\
  // ...
}
AutoLinking & fazendo build novamente

Assim que os passos acima tiverem sido concluídos, a biblioteca React Native Firebase deve ser ligada (linked) ao seu projeto e sua aplicação precisará de um novo build.

Usuários do React Native 0.60+ tem acesso ao “link automático” (autolinking), de forma que nenhum passo a mais é necessário. Para automaticamente realizar o link, faça build novamente do seu projeto: 

# Android apps
npx react-native run-android

# iOS apps
cd ios/
pod install --repo-update
cd ..
npx react-native run-ios

Assim que o build tiver terminado, sua aplicação vai estar conectada no firebase utilizando o módulo app. Esse módulo por si só não tem muitas funcionalidades! Dessa forma, para utilizar outros serviços da Firebase, cada um dos módulos individuais deve ser instalado separadamente. Nesse tutorial, falaremos apenas do módulo de notificações. 

Link manual

Se você estiver utilizando uma versão antiga do React Native que não suporta autolinking, ou deseja integrar o Firebase a um projeto já existente, você pode seguir os passos de instalação manual para o iOS ou para o Android.

Passo adicional, pode ou não ser necessário!
Android: Habilitando Multidex

Conforme sua aplicação começa a crescer com mais e mais dependências, suas builds podem começar a falhar com um erro bem comum: Execution failed for task ‘:app:mergeDexDebug’. Esse erro ocorre quando o Android chega no limite de 64K  métodos. 

Uma solução comum é adicionar suporte multidex para o Android. Essa solução é comumente utilizada e resolve o problema, mas é recomendado que você leia a documentação do Android e compreenda como ela pode afetar sua aplicação.

Você pode encontrar outros erros comuns e suas devidas soluções na documentação oficial do React Native Firebase, na seção Miscellaneous.

Utilizando o Cloud Messaging para o envio de notificações
Instalação

Esse módulo requer que o módulo app do Firebase já esteja devidamente instalado e configurado. Nos passos acima, já garantimos isso! Dessa forma, o próximo passo é instalar o módulo de messaging:

# Instalando o módulo messaging com o yarn
yarn add @react-native-firebase/messaging

# Instalando o módulo messaging com o npm
npm install @react-native-firebase/messaging


# No iOS, execute o seguinte comando após o final da instalação
cd ios/ && pod install
Próximos passos, iOS: 

O iOS precisa de configurações adicionais antes que você possa receber notificações em seus aparelhos! Existem também uma série de pré-requisitos que são necessários para que o messaging possa ser ativado e devidamente utilizado: 

Você deve ter uma conta de desenvolvedor Apple ativa. 
Você deve ter um dispositivo físico com iOS para receber mensagens
Firebase Cloud Messaging (o serviço de envio de notificações) funciona integrado ao Serviço de Notificações Push da Apple (APNs), entretanto, APNs só funcionam em dispositivos reais. Dessa forma, não é possível testar essa funcionalidade em um emulador.
Configurando seu App iOS

Antes de começar a receber notificações, você deve explicitamente habilitar “Notificações Push” (Push Notifications) e “Modos em plano de fundo” (Background Modes) no Xcode.

Abra o workspace do seu projeto (encontrado dentro da pasta /ios). O nome do arquivo é iniciado com o nome do seu projeto, por exemplo: /ios/{myappname}.xcworkspace. Após aberto, siga os passos abaixo: 

Selecione o seu projeto
Selecione o target do projeto
Selecione a aba “Signing & Capabilities”.
Habilitando as notificações Push

O próximo passo é adicionar o recurso (capability) de notificações Push (Push notifications). Isso pode ser feito pela opção Capability dentro da aba “Signing & Capabilities” selecionada no passo anterior.

Clique no botão “+ Capabilities”
Procure por “Push notifications”

Após selecionado, o recurso será listado abaixo de outros recursos habilitados no projeto. Se nenhuma opção aparecer, o recurso pode já estar ativo no projeto. 

Habilitando o modo plano de fundo (Background Modes)

Depois disso, o recurso Background Modes precisa ser ativado, ativando também os submódulos Background fetch e Remote notifications. Isso pode ser feito pela opção Capability dentro da aba “Signing & Capabilities” (mesmo local do passo anterior). 

Clique no botão “+ Capabilities”
Procure por “Background Modes”

Após selecionado, o recurso será listado abaixo dos outros recursos já ativos. Se nenhum resultado aparecer na busca, o recurso pode já estar ativo no projeto. 
Agora, certifique-se de que os submódulos “Background fetch” e “Remote notifications” estão ativos, conforme indicado abaixo:

Ligando o APNs com o Firebase Cloud Messaging (FCM) no iOS

Alguns passos são necessários: 

Registrar uma key
Registrar um App Identifier
Gerar um provisioning profile

Todos esses passos requerem que você tenha acesso à sua Conta de desenvolvedor Apple. Assim que estiver devidamente logado na conta, navegue até a aba Certificates, Identifiers & Profiles na barra lateral: 

Registrando uma chave

Uma chave pode ser adicionada, dando ao FMC acesso completo ao serviço de Notificações da Apple (APN).  Na opção “Keys”, registre uma nova chave. O nome da chave não importa, mas você deve garantir que o serviço de APN está ativo, conforme imagem abaixo: 

Clique em “Continue” e depois em “Salvar”. Após salvar, você verá uma tela que te mostrará o Key ID privado e te permitirá baixar a chave. Copie o ID privado e faça download da chave para sua máquina. 

O arquivo e o Key ID agora podem ser adicionados ao seu projeto na Firebase. No Firebase Console, navegue até configurações de projeto (Project Settings) e selecione a aba Cloud Messaging. Selecione sua aplicação iOS dentro do menu iOS app configuration. 


Faça upload do arquivo baixado e entre com a key ID: 

Registrando um identificador do App (App Identifier)

Para que as notificações funcionem quando seu aplicativo for para produção, você deve criar um App Identifier que será linkado à aplicação que você está desenvolvendo.

No Menu “Identifiers”, registre um App Identifier. Selecione a opção “App IDs” e clique “Continue”. 

A tela abaixo mostra como colocar o App Identifier na sua aplicação utilizando o Bundle ID. Essa é uma string única gerada quando você inicia seu projeto React Native. Seu Bundle ID pode ser obtido no Xcode, dentro da opção “Target” e dentro da aba “General”. Veja imagem abaixo: 

Próximos passos: 

Entre com uma descrição para o identificador
Entre com o Bundle ID copiado do Xcode
Desça a página e ative a opção Push Notifications (e também qualquer outro serviço que seu app utilizar)

Salve o identificador, ele vai ser utilizado para criar um provisioning profile no próximo passo.

Gerando um provisioning profile

Um provisioning profile permite uma comunicação assinada entre a Apple e sua aplicação. Como as notificações apenas podem ser testadas em dispositivos reais, um certificado assinado garante que o aplicativo a ser instalado no seu dispositivo é um app genuíno e tem as permissões corretas ativas para funcionar. 

No menu “Profiles”, registre um novo perfil. Selecione a opção iOS App Development e clique em Continue. Se você seguiu o passo dois corretamente, seu App Identifier vai estar disponível no menu para ser escolhido:

Clique em Continue. Na próxima tela você vai poder ver os certificados na sua conta Apple. Selecione o certificado de usuário para o qual você deseja designar esse provisioning profile. Se você ainda não tiver criado um certificado de usuário, siga a documentação oficial da apple. 

O provisioning profile criado pode agora ser utilizado quando você for fazer build da sua aplicação para um dispositivo real utilizando o Xcode (tanto no modo debug  como no modo release). Dentro do Xcode, selecione seu projeto na opção target e selecione a aba “Signing & Capabilities”. Se nas preferências do Xcode ele já estiver ligado à sua conta Apple, ele pode automaticamente sincronizar o perfil criado acima. Senão, você pode adicioná-lo manualmente seguindo a imagem abaixo: 

Utilizando as notificações
Solicitando permissões – iOS 

O iOS não permite que notificações sejam exibidas a não ser que você tenha solicitado permissão explícita do usuário. O trecho de código abaixo provém um método que chama uma função nativa de permissão de notificação:

import messaging from '@react-native-firebase/messaging';

async function requestUserPermission() {
  const authStatus = await messaging().requestPermission();
  const enabled =
    authStatus === messaging.AuthorizationStatus.AUTHORIZED ||
    authStatus === messaging.AuthorizationStatus.PROVISIONAL;

  return enabled;
}

Recomendamos que você chame esse método o mais cedo possível na sua aplicação, de forma a garantir que as notificações funcionem corretamente quando você precisar delas! 

Tokens de dispositivo

Para enviar uma notificação para um aparelho, você precisa de acesso ao seu token único. Um token é automaticamente gerado e pode ser acessado utilizando o módulo Cloud Messaging. Ele deve ser salvo no banco de dados do seu sistema e deve ser facilmente acessível quando necessário. 

Salvando tokens

Assim que sua aplicação estiver iniciada, você pode chamar o método getToken do módulo Cloud messaging para receber o token único do dispositivo. Faça conforme indica o exemplo abaixo:

import React, { useEffect } from 'react';
import messaging from '@react-native-firebase/messaging';
import { Platform } from 'react-native';

async function saveTokenToDatabase(token) {
  //Salve o token no banco de dados do seu sistema, como salva qualquer outro conteúdo que você utiliza
}

function App() {
  useEffect(() => {
    // Pega o token do dispositivo
    messaging()
      .getToken()
      .then(token => {
        return saveTokenToDatabase(token);
      });
      
   
    // escuta mudanças no token
    return messaging().onTokenRefresh(token => {
      saveTokenToDatabase(token);
    });
  }, []);
}

Você pode ler mais sobre como essas funções funcionam na documentação oficial. 

Futuramente, traremos aqui um tutorial de como enviar notificações a partir do backend em nodejs utilizando os tokens salvos e gerados pelo front-end. Se já quiser implementar, você pode seguir o exemplo da documentação oficial, que utiliza o Cloud Firestore para armazenamento, e adaptar conforme for necessário para seu projeto. 

Notificações em background ou com o app fechado

Para que as notificações em background funcionem, chame a função setBackgroundMessageHandler o mais cedo possível na lógica da sua aplicação. O exemplo abaixo faz a chamada no index.js: 

// index.js
import { AppRegistry } from 'react-native';
import messaging from '@react-native-firebase/messaging';
import App from './App';

// Register background handler
messaging().setBackgroundMessageHandler(async remoteMessage => {
  console.log('Message handled in the background!', remoteMessage);
});

AppRegistry.registerComponent('app', () => App);
Lidando com notificações em background – iOS

No iOS, é necessário adicionar algumas configurações para que as notificações em background funcionem corretamente. Você pode ler sobre o porquê isso é necessário aqui. No seu arquivo index.js, faça as seguintes modificações: 

// index.js
import { AppRegistry } from 'react-native';
import messaging from '@react-native-firebase/messaging';

messaging().setBackgroundMessageHandler(async remoteMessage => {
  console.log('Message handled in the background!', remoteMessage);
});

function HeadlessCheck({ isHeadless }) {
  if (isHeadless) {
    // O app foi aberto pelo iOS em plano de fundo, ignore
    return null;
  }

  return <App />;
}

function App() {
  // Sua aplicação vem aqui
}

AppRegistry.registerComponent('app', () => HeadlessCheck);

Para que a propriedade isHeadless funcione corretamente, atualize seu arquivo AppDelegate.m como indicado abaixo:

// adicione esse import no topo do seu arquivo `AppDelegate.m` 
#import ""RNFBMessagingModule.h""

// no método ""(BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions"" 
// Use `addCustomPropsToUserProps` para passar propriedades para a inicialização do seu app 
// Ou passe ‘nil’ se você não tiver nenhuma, conforme exemplo abaixo.
// Para o `withLaunchOptions` por favor, passe o objeto `launchOptions` 

NSDictionary *appProperties = [RNFBMessagingModule addCustomPropsToUserProps:nil withLaunchOptions:launchOptions];

// Encontre a instância do `RCTRootView` e atualize o `initialProperties` com a sua  instância de  `appProperties`
RCTRootView *rootView = [[RCTRootView alloc] initWithBridge:bridge
                                             moduleName:@""nameOfYourApp""
                                             initialProperties:appProperties];

Lidando com interações

Quando um usuário interage com a sua notificação clicando nela, o comportamento padrão é abrir a aplicação. Em vários casos, é útil saber se o app foi aberto por uma notificação (dessa forma, você pode fazer algo com os dados enviados nela, como ir para uma tela em específico, por exemplo). 

Para isso, a API provê duas funções: getIniticialNotification (quando a aplicação é aberta estando previamente fechada) e onNotificationOpenedApp (quando a aplicação está aberta, mas está em plano de fundo).

Você pode ver um exemplo de uso dessas duas funções abaixo.

import React, { useState, useEffect } from 'react';
import messaging from '@react-native-firebase/messaging';
import { NavigationContainer, useNavigation } from '@react-navigation/native';
import { createStackNavigator } from '@react-navigation/stack';

const Stack = createStackNavigator();

function App() {
  const navigation = useNavigation();
  const [loading, setLoading] = useState(true);
  const [initialRoute, setInitialRoute] = useState('Home');

  useEffect(() => {
    // Assume que uma notificação do tipo mensagem contém uma propriedade “tipo” no payload que indica a tela a ser aberta

    messaging().onNotificationOpenedApp(remoteMessage => {
      console.log(
        'Notification caused app to open from background state:',
        remoteMessage.notification,
      );
      navigation.navigate(remoteMessage.data.type);
    });

    // Checando se uma notificação inicial está disponível
    messaging()
      .getInitialNotification()
      .then(remoteMessage => {
        if (remoteMessage) {
          console.log(
            'Notification caused app to open from quit state:',
            remoteMessage.notification,
          );
          setInitialRoute(remoteMessage.data.type); // exemplo: ""Configuracoes""
        }
        setLoading(false);
      });
  }, []);

  if (loading) {
    return null;
  }

  return (
    <NavigationContainer>
      <Stack.Navigator initialRouteName={initialRoute}>
        <Stack.Screen name=""Home"" component={HomeScreen} />
        <Stack.Screen name=""Settings"" component={SettingsScreen} />
      </Stack.Navigator>
    </NavigationContainer>
  );
}

A chamada de getInitialNotification deve sempre ocorrer dentro de um método do lifecycle do React que ocorre após a montagem (como o componentDidMount ou o useEffect). 

No Android, você pode testar as notificações em um emulador. No iOS, apenas em um dispositivo real. 

Notificações em primeiro plano!

Para escutar por notificações em primeiro plano (ou seja, quando seu app está aberto), chame o método onMessage dentro da sua aplicação. O exemplo abaixo gera um Alert quando uma nova notificação chega e o app está aberto:

import React, { useEffect } from 'react';
import { Alert } from 'react-native';
import messaging from '@react-native-firebase/messaging';

function App() {
  useEffect(() => {
    const unsubscribe = messaging().onMessage(async remoteMessage => {
      Alert.alert('A new FCM message arrived!', JSON.stringify(remoteMessage));
    });

    return unsubscribe;
  }, []);
}

Você pode ler mais sobre notificações que contém apenas o campo data e outras especificidades e variações na documentação oficial. Sempre consulte primeiro a documentação oficial quando tiver alguma dúvida! 

Conclusão

Gostou do nosso tutorial? Ele foi útil para você ou para sua equipe? Tem alguma dúvida? Conta pra gente aqui nos comentários!

Post escrito por Koda Gabriel e revisado por Arilton Aguilar, Ramon Barros e Camilla Silva."
Dicas para facilitar a portabilidade de aplicativos React Native entre iOS e Android,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/09/KodaPost2-730x350.png,"OReact Native surgiu como uma ótima ferramenta de desenvolvimento híbrido, visando facilitar o dia a dia dos desenvolvedores e diminuir significativamente a carga de trabalho de portar aplicativos para iOS e/ou Android. 

Entretanto, apesar de possuir boa parte de seu framework compatível com ambos os sistemas, ainda existem particularidades que só funcionam em um deles. Nessas horas, é importante prestar atenção, de forma a tornar o desenvolvimento híbrido de fato, e diminuir a quantidade de bugs e dificuldades ao testar em dispositivos diferentes.

Hoje trazemos algumas dicas que podem facilitar o seu desenvolvimento híbrido com React Native, diminuindo os problemas causados por compatibilidade de módulos!

Se possível, desenvolva primeiro testando em dispositivos iOS

Nem sempre você tem uma máquina iOS disponível, mas, caso consiga, dê preferência em iniciar o desenvolvimento testando em dispositivos iOS. Aqui, no blog do TerraLAB, já fizemos um tutorial de como instalar uma Máquina Virtual MacOS Catalina no seu computador Windows, então você pode testar essa alternativa, caso não tenha um computador Mac disponível.

Pela nossa experiência, observamos que garantir que o App está responsivo e harmônio no iOS, na maioria das vezes garante que ele estará responsivo e harmônico também no Android. 

Cuidado ao gerenciar toques na tela!

Para lidar com toques na tela em geral (sejam em botões ou qualquer outro componente), temos componentes Touchable para gerenciar essas ações. Entretanto, algumas delas tem uma “reação” ao toque mais de acordo com o esperado no iOS, e outra, mais de acordo com o Android. 

Os quatro componentes mais comuns são: 

TouchableHighlight 
Funciona no iOS e no Android, mas tem uma reação ao toque que é própria do iOS
TouchableNativeFeedback
Funciona apenas no Android e tem uma reação ao toque que é própria para ele
TouchableOpacity
Funciona no iOS e no Android, e reage reduzindo a opacidade do botão ao ser tocado
TouchableWithoutFeedback
Funciona no iOS e no Android, não possui reação ao toque

Portanto, nota-se que existem botões com feedback mais apropriado para cada sistema operacional. Nossa dica, nesses casos, é criar um componente que retorne o Touchable apropriado para cada OS (TouchableHighlight ou TouchableNativeFeedback), e utilizar os outros dois apenas em casos específicos. 

Um exemplo de componente é o seguinte:

import React, {useRef} from 'react';
import {
  Platform,
  TouchableNativeFeedback,
  TouchableOpacity,
} from 'react-native';
 
const Touchable = ({innerElement, onPressFunc, style, disabled}) => {
  const touchableOpacityRef = useRef(null);
  const onPressIOS = () => {
    onPressFunc();
    touchableOpacityRef.current.setOpacityTo(50);
  };
  if (Platform.OS === 'ios') { // aqui garantimos que o componente certo será retornado
    return (
      <TouchableOpacity
        disabled={disabled}
        ref={touchableOpacityRef}
        onPressIn={() => onPressIOS()}
        style={style ? style : []}>
        {innerElement}
      </TouchableOpacity>
    );
  } else {
    return (
      <TouchableNativeFeedback
        disabled={disabled}
        onPress={onPressFunc}
        style={style ? style : []}>
        {innerElement}
      </TouchableNativeFeedback>
    );
  }
};
 
export default Touchable;

Utilizando o componente Platform, do React Native, conseguimos testar qual sistema operacional está sendo utilizado naquele momento no dispositivo e retornar o componente correto para uso. 

Recebemos alguns parâmetros extras para garantir o funcionamento correto e possibilitar o melhor uso do componente. 

innerElement (obrigatório)
Os componentes que ficarão dentro do Touchable e deverão ser tocáveis
onPressFunc (obrigatório)
Função que será chamava ao tocar no Touchable
style (opcional)
Variável de estilo que, se enviada, aplicará o seu estilo ao componente
disabled
Variável booleana que, se enviada, desativará ou ativará o Touchable

Após criar esse componente, basta utilizá-lo sempre que precisar de um TouchableHighlight ou TouchableNativeFeedback. Ele retornará o que for adequado ao sistema operacional em utilização no momento, sempre garantindo o funcionamento do Touchable.

Recentemente, o React Native liberou em sua nova versão o componente Pressable, que será, ao que tudo indica, um novo componente de manutenção longa que permitirá lidar com toques na tela. Ele funciona tanto em iOS como em Android e possui por padrão o estilo de feedback do iOS. Entretanto, é possível ativar o estilo de feedback do Android passando uma propriedade extra. Você pode ler sobre esse novo componente aqui. 

Caso esteja começando um projeto agora, ele é a melhor solução! Para manutenção em projetos que já estão em andamento, recomendamos a criação do componente listado acima, que facilitará na hora do uso.

Preste atenção nos notchs

Você sabe o que é um notch? “Também chamado de entalhe, o notch é uma área na parte frontal de um smartphone que fica sobre a tela, normalmente para acomodar a câmera frontal ou sensores, como o de luminosidade, para ajustar o brilho de forma automática.” (TechnoBlog, 2018). Ele se popularizou há alguns anos, e agora é comum de ser encontrado em vários dispositivos Android e é encontrado nos iPhone X e 11. 

Por ele ficar parcialmente para “dentro” da tela, ele ocupa um espaço no centro que, caso não se tome cuidado, pode acabar tampando parte da sua aplicação. Ela precisa levar em conta esses aparelhos, de forma a não causar sobreposição de informações. 

É importante ressaltar o comportamento padrão de cada SO ao lidar com notchs na tela. No Android, o notch faz parte da StatusBar, que assume uma cor sólida. No iOS, a StatusBar assume uma cor transparente, deixando em evidência o que está atrás dele — no caso, seu aplicativo. Isso também deve ser levado em conta durante as decisões de design. 

Para resolver esses problemas, recomendamos o uso de uma pequena biblioteca, disponível no npm e yarn, que retorna a altura da StatusBar do dispositivo, área onde fica o notch, possibilitando que você tome decisões a partir disso. 

Você pode ler sobre essa biblioteca aqui.

Como descrito na documentação, ela retorna o valor conforme não somente o SO (iOS ou Android), mas também considerando o modelo do aparelho iOS ao retornar, enviando o valor correto se o aparelho for a versão X ou 11. Para o Android, o valor é calculado em tempo real e retornado ao usuário.

No Android, também é possível utilizar o componente StatusBar, disponível já no React Native de forma nativa. Apenas nesse sistema, a propriedade currentHeight retorna a altura atual da StatusBar. Você pode ver mais sobre esse componente aqui.

Sempre teste na maior quantidade de dispositivos possível!

Isso vale tanto para o desenvolvimento híbrido, quanto para qualquer outro tipo de desenvolvimento! Sempre teste bastante, em vários dispositivos, de forma a garantir que nada está fora do lugar, não está cabendo na tela ou tendo um comportamento diferente do esperado.

Em máquinas MacOS, físicas ou virtuais, é possível testar em todos os dispositivos iOS atualmente suportados pela Apple. É importante usufruir desse recurso! 

No Android, é possível instalar diversas versões, simulando vários aparelhos com tamanhos e proporções diferentes de tela usando o Android Studio. O recurso é pesado, mas vale a pena ser utilizado conforme possível para garantir que seu App não terá comportamento inadequado em nenhum aparelho!

Conclusão

Essas foram nossas dicas de hoje! Esperamos que elas te ajudem ao desenvolver em React Native e que minimizem o trabalho em garantir compatibilidade com os dois sistemas operacionais. Gostou das nossas dicas? Tem alguma que poderia nos ajudar, ou ajudar outras pessoas? Comenta aqui embaixo sua opinião!"
Criando uma Máquina Virtual para desenvolvimento iOS,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/08/MaquinaVIrtualRetangulo-730x350.png,"Desenvolver para iOS é uma barreira que diversos desenvolvedores encontram, seja pela necessidade do pagamento anual da conta de desenvolvedor para o deploy do aplicativo ou a obrigatoriedade de utilizar um sistema MacOS para a compilação dele. Visando derrubar uma parte dessa barreira, o TerraLab apresenta neste tutorial como instalar o MacOS Catalina em uma máquina virtual e os programas básicos para o desenvolvimento.

Para o sistema funcionar com fluidez, é necessário um computador com recursos consideráveis. O hardware em que usamos a máquina virtual está listado abaixo:

16GB de Ram DDR4 3000MHz
Processador com 12/6, 19MB Cache e 3.2Ghz

A placa de vídeo é dispensável no caso, visto que o Mac, na máquina virtual, não tem suporte para aceleração de vídeo (então você infelizmente não vai conseguir ativar os efeitos legais de transparência e blur). 

Além disso, o seu computador também deve ser capaz de virtualização e essa opção deve ser ativada na BIOS. A maioria dos computadores hoje em dia suportam. Então, se você tiver o hardware potente o suficiente para rodar uma máquina virtual, provavelmente é atual o suficiente para não ter problema com isso.

Caso esteja em um Notebook, um teclado e mouse USB são necessários, visto que a máquina virtual provavelmente não vai suportar o teclado interno e o touchpad. 

Instalando o VMWare

Vamos utilizar uma versão específica do VMWare Player. A versão utilizada não é a mais atual e não deve ser atualizada. As versões mais novas possuem alguns bugs relacionados a instalação do Mac, que dificultam ou impedem a execução normal do sistema. 

Você pode baixar essa versão AQUI.

Ao longo da instalação pode ser necessário reiniciar o computador uma ou mais vezes.

Quando você finalmente chegar na tela abaixo, quer dizer que o VMWare Player está corretamente instalado. Feche totalmente o programa antes de seguir para a próxima etapa. 

Desbloqueando o VmWare

Normalmente, o VmWare vem bloqueado para instalação do Mac. É necessário desbloquear esse suporte e para isso vamos utilizar o aplicativo Unlocker que pode ser baixado aqui.

Ao extrair o Unlocker em algum lugar, você deve rodar o arquivo win-install.cmd como administrador. A execução do script leva alguns segundos e nesse meio tempo você vai ver a tela abaixo:

Se você não viu essa tela, provavelmente é porque não executou o arquivo como administrador ou não fechou o VMWare Player e o VMWare não foi desbloqueado com sucesso.

Baixando o MacOS e o VMWare ToolsTOOLS

Existem diversas formas de conseguir a imagem de instalação do Mac. Você pode criar sua própria imagem utilizando um dispositivo Mac real ou pode baixar a imagem de algum lugar. Estamos disponibilizando o link abaixo com download do sistema. Foi essa imagem que usamos para desenvolver o aplicativo da Orquestra Ouro Preto.

LINK DA ISO DO MACOS CATALINA

Além da imagem do sistema, também vamos precisar da imagem do VMWare Tools que pode ser encontrada no link abaixo. 

LINK DO VMWARE TOOLS

Vamos usar o VMWare Tools para instalar alguns pacotes no Mac que melhoram a velocidade do sistema, resolução e suporte avançado para dispositivos USB, dentre outras coisas.

Configuração Inicial da Máquina Virtual

Abra o VMWare Player. Você deve seguir os passos abaixo para configurar sua máquina virtual.

Clique em “Create a New Virtual Machine”. 

Nessa tela, selecione a opção para instalar um sistema operacional depois. Clique em Next.

Nesta tela, escolha MacOS e a versão 10.14. Clique em Next. 

Agora, escolha o nome da sua máquina virtual e sua localização. O nome normalmente é uma referência para a função da máquina ou seu sistema operacional. Aqui iremos chamar de Catalina.

É necessário escolher um local com espaço suficiente em disco para caber o HD da máquina virtual, o mínimo recomendado é 40GB mas estamos utilizando 100GB no tutorial. 40GB é muito pouco considerando o tamanho das ferramentas de desenvolvimento e demais subprodutos que serão gerados.

Escolha o tamanho conforme explicamos acima. Qualquer coisa acima de 100GB é válida. Além disso, marque para armazenar o disco em um único arquivo, armazenar em múltiplos arquivos compromete muito a velocidade de transferência dentro da máquina virtual.

Clique em Next.

Agora vamos personalizar o Hardware da máquina virtual. Clique em Customize Hardware quando chegar na tela abaixo.

Para memória, você deve selecionar a metade da memória disponível no seu computador. O mínimo recomendado para o Mac Catalina é 4GB de Ram. Não testamos com um computador contendo essa quantidade de memória, então o desempenho pode eventualmente não ser tão bom.

Em processador, escolha metade do número total de threads que o seu tiver. Marque também a opção “Virtualize Intel or AMD” como marcada na imagem.

Em CD/DVD driver, escolha a opção da imagem e selecione a ISO do MacOS Catalina que você baixou anteriormente.

Em USB Controller, marque a opção apresentada abaixo.

Após realizar essa configuração, clique em Close e em Finish. Sua máquina virtual vai ser criada e vai ficar na tela principal do VMWare Player do seguinte modo:

Nesse momento, feche totalmente o VMWare. Precisamos configurar um arquivo na pasta onde criamos a nossa máquina virtual e para isso é necessário que o VMWare não esteja rodando.

No diretório escolhido para a máquina virtual, você vai ter a seguinte estrutura de arquivos:

Abra o arquivo com extensão VMX em um editor de textos. Na imagem acima seria o arquivo macOS 10.vmx de tamanho 2KB.

Vamos realizar duas modificações. A primeira delas é a seguinte:

A linha

virtualHW.version = ""16""

deve ser alterada para

virtualHW.version = ""10""

A próxima modificação vai depender do seu processador. Vamos adicionar alguns dados ao final desse mesmo arquivo. 

Se seu processador for INTEL, vá até o final do arquivo, crie uma nova linha e digite nela isso:

smc.version = ""0""

Se seu processador for AMD, vá até o final do arquivo, crie uma nova linha e digite nela isso:

smc.version = ""0""
cpuid.0.eax = ""0000:0000:0000:0000:0000:0000:0000:1011""
cpuid.0.ebx = ""0111:0101:0110:1110:0110:0101:0100:0111""
cpuid.0.ecx = ""0110:1100:0110:0101:0111:0100:0110:1110""
cpuid.0.edx = ""0100:1001:0110:0101:0110:1110:0110:1001""
cpuid.1.eax = ""0000:0000:0000:0001:0000:0110:0111:0001""
cpuid.1.ebx = ""0000:0010:0000:0001:0000:1000:0000:0000""
cpuid.1.ecx = ""1000:0010:1001:1000:0010:0010:0000:0011""
cpuid.1.edx = ""0000:1111:1010:1011:1111:1011:1111:1111""
featureCompat.enable = ""FALSE""

Agora você pode salvar o arquivo e abrir novamente o VMWare. Clique com o botão direito na sua máquina virtual e então clique em “Settings”.

Você vai ter uma nova aba aqui, a “Options”. Clique nela para chegar na seguinte tela:

Aqui, você deve alterar o sistema operacional guest para Microsoft Windows, versão Windows 10 x64, ficando desta forma:

Basta clicar em OK. Nenhuma outra configuração é necessária. 

Instalando o MacOS Catalina

Agora que você configurou o VMWare Player, basta dar Play na maquina virtual.

Se tudo tiver dado certo, você vai ver essa tela:

Caso você veja o aviso abaixo, clique em “Remind Me Later”

Se o seu mouse ficar preso na maquina virtual, basta apertar “Ctrl + Alt” no teclado para libertar ele. 

Se seu Mouse ou Teclado não funcionam durante a instalação, você pode usar o menu mostrado abaixo para tentar conectar os seus dispositivos.

O problema ao utilizar a conexão desta forma é que eles ficam totalmente presos no VMWare e não podem mais ser utilizados no Windows. Se você conectar o teclado na máquina virtual dessa forma, possivelmente vai precisar reiniciar para usar o Windows novamente, visto que não vai conseguir desprender os periféricos com Ctrl + Alt.

Quando acabar de carregar (pode levar algum tempo), selecione o idioma e avance para a próxima tela.

Sua máquina virtual pode ficar meio lenta agora, mas isso pode mudar quando instalar o pacote VMWare Tools no fim, então não desista no meio do caminho!

Na tela abaixo, clique em “Utilitário de Disco” e em “Continuar”

Você precisa selecionar o HD demonstrado na figura e então clicar na opção “Apagar” da Barra de Ferramentas Superior. 

Altere somente o nome, como desejar, e clique em “Apagar”

Ao final, clique em “Ok” para confirmar e feche o “utilitário de disco”. Você vai retornar para o menu de instalação. Clique em “Instalar MacOS” e então em “Continuar”. Concorde com os termos, selecione a unidade de disco que você criou e confirme a instalação. É normal demorar bastante na tela abaixo

Eventualmente, depois de MUITO tempo de instalação, você vai acabar chegando nessa tela:

Não se desespere, está tudo certo. Nesse momento você deve fechar o VMWare e abri-lo novamente, mas não inicie a máquina virtual!

Clique com o botão direito na máquina virtual e clique em “Settings”. Em CD/DVD você vai selecionar agora o arquivo Darwin.iso, o VMWare Tools que você baixou junto da imagem do Catalina.

Em options, retorne o sistema operacional para Mac OS e escolha a versão 10.14. Várias janelas de aviso podem aparecer e você deve clicar “Ok” em todas. 

Clique em OK para confirmar as novas configurações e inicie a máquina virtual novamente. Agora a instalação vai continuar normalmente! Na tela abaixo escolha o local do mundo onde você está:

Quando ver essa tela, escolha a opção “Personalizar os Ajustes”. Se não fizer isso, o teclado vai vir configurado errado e algumas teclas estarão fora de lugar. 

Mantenha seu idioma preferido como quiser, porém na tela de teclado você deve clicar no botão de adicionar

Escolha o seu padrão de teclado e adicione. ABNT2 é o mais usado atualmente. De todo modo, você vai poder ver um preview do teclado antes de confirmar.

Clique no teclado antigo e depois  em subtrair para removê-lo.

Continue a instalação deixando o idioma de ditado como o padrão. Na tela abaixo marque para não transferir nenhuma informação.

Na tela que pede uma Apple ID, você deve clicar em “Configurar mais tarde” e em ignorar.

A partir desse ponto você pode confirmar tudo, criar sua conta de usuário e escolher seu tema. Eventualmente, depois de todo esse trabalho, você vai ter logado na sua máquina virtual e seu MacOS Catalina foi instalado com sucesso.

Instalando o VMWare Tools

Se você seguiu o tutorial corretamente até aqui, deve ter um ícone escrito VMWare Tools na sua área de trabalho. Dê um duplo clique nele para abrir a instalação e clique em “Install VMWare Tools”.

Continue confirmando o processo de instalação. Provavelmente, você vai precisar digitar sua senha algumas vezes.

A instalação vai ser bloqueada. Clique em Abrir Preferências de Segurança

Clique no cadeado localizado no canto inferior esquerdo para permitir edição.

Então, clique em “Permitir” e pode fechar a janela. A instalação vai continuar até o momento em que você vai receber a “tela de sucesso” abaixo

Confirme para reiniciar e aguarde.

O VMWare Tools foi instalado com sucesso. Você pode maximizar a janela e alterar a resolução da máquina virtual. Todo o sistema deve rodar consideravelmente mais rápido agora.

Obs: Nas configurações da Máquina Virtual você pode remover a imagem do Darwin para não carregar o instalador do VMWare Tools automaticamente toda vez. 

A partir desse ponto, você pode utilizar e instalar tudo que for necessário. Para começar a desenvolver um aplicativo para iOS, basta abrir a Apple Store e procurar por Xcode.

Para usar alguns recursos como a própria Apple Store pode ser necessário criar uma conta Apple e eventualmente adicionar um cartão de crédito. 

A partir daqui, a utilização fica por conta da necessidade individual de cada desenvolvedor. Sua máquina virtual está pronta para ser usada. Teve alguma dúvida ao longo do tutorial? Quer compartilhar sua experiência programando dessa forma? Deseja trocar uma ideia com a gente sobre algo relacionado? Manda ai um comentário que a gente responde!

Link para a pasta com todos os arquivos utilizados durante o tutorial.

Escrito por Arilton Aguilar. Revisado por Ramon Barros e Camilla Silva."
Lançamento: Conheça o Aplicativo Orquestra Ouro Preto,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/08/blogChamada-730x350.png,"Nesta sexta feira (21/08/2020), às 20h30, a Orquestra Ouro Preto realizará, em uma live, o lançamento do aplicativo Orquestra Ouro Preto. Esse aplicativo foi desenvolvido pelo time TerraLAB em uma parceria com a Orquestra, buscando uma maior integração com o público e o envio de notícias e novidades em primeira mão.

O post de hoje servirá como um tutorial de uso, mas também uma espiada no interior do app e suas funcionalidades. 

A navegação dentro do aplicativo é feita a partir do botão no topo superior esquerdo ou arrastando o dedo da borda esquerda em direção ao centro da tela. Utilize a aba lateral para navegar entre as diversas opções do menu.

Na barra lateral, você também encontra as redes sociais da Orquestra (canto inferior esquerdo). Por lá você tem acesso ao Facebook, Youtube, Instagram, Spotify e ao site, tudo com poucos cliques e sem pesquisar nada. Não deixe de acompanhar a Orquestra em todas elas! 

Notícias: 

Você pode acessar as novidades mais recentes diretamente do app! Na página inicial, veja um carrossel com os destaques do momento. Ao clicar em “Mais notícias”, você poderá ver as novidades da Orquestra sempre, diretamente no seu celular!

Calendário:

Aqui você poderá verificar as datas de próximos eventos da Orquestra! Atualmente, todos os eventos estão suspensos devido à pandemia. Por enquanto, você pode conferir as músicas da Orquestra no Spotify. 

Apresentação:

Nessa página, você pode encontrar informações sobre quem é a Orquestra Ouro Preto, cidades visitadas anteriormente, objetivos, ficha técnica e muito mais! 

Amigos de Ouro:

Entenda melhor sobre o que é um Amigo de Ouro e como se tornar um! Saiba detalhes sobre a atuação da Orquestra, o programa Amigos de Ouro e como redirecionar parte do seu Imposto de Renda para a Orquestra.

TerraLAB:

Saiba mais sobre o TerraLAB e a parceria com a Orquestra. Siga nossas redes para não perder nossas novidades!

O aplicativo também contará com notificações enviadas diretamente pela Orquestra Ouro Preto! Novidades, notícias atualizadas e eventos: tudo direto no celular. 

O lançamento ocorrerá na sexta feira, durante a live da Orquestra, e você poderá testar essas funcionalidades! O aplicativo estará disponível para iOS e Android, diretamente na loja! 

Gostaríamos de agradecer imensamente à Usemobile pela mentoria e todo o apoio prestado durante a criação desse aplicativo. A parceria entre o TerraLAB e a Usemobile foi de valor imensurável para toda a equipe!

Siga o TerraLAB e a Orquestra Ouro Preto nas redes e não perca nenhuma notícia sobre esse lançamento!

Curta a página da Orquestra Ouro Preto no Facebook

Escrito por Arilton Aguilar e Koda Gabriel. Revisado por Ramon Barros, Prof. Tiago Carneiro e Camila Silva."
Concebendo aplicativos que se ajustam à necessidade do cliente,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/08/ImagemPrincipal-Sem-Titulo-730x350.jpg,"Este artigo tem por objetivo explicar como a equipe do TerraLab se organiza e se comporta nos momentos iniciais de um projeto para o desenvolvimento de um aplicativo, quando é necessário ouvir a organização parceira (não temos clientes, formamos parcerias) e precisamos entender suas necessidades. Ele explica como fazemos para  capturar o conceito do aplicativo que nos é encomendado, identificar as funcionalidades que precisam ser oferecidas e, então, projetar o comportamento desejado para se obter cada funcionalidade. Finalmente, o artigo explica como dimensionamos nosso esforço, garantindo que a parceira terá visibilidade do andamento e acessibilidade a todas as informações ao longo do projeto, facilitando a gestão das expectativas da organização parceira com relação à nossa equipe.

Primeiro, é preciso dizer que não nos colocamos como autoridades no assunto e, por isso, não temos a pretensão de fazer deste artigo um tratado sobre “análise e especificação de requisitos de software”, tema em que diversos autores fizeram contribuições que moldaram o estado da arte conhecido pela academia e pela indústria. Não traremos verdades absolutas e imutáveis, pois nosso aprender é contínuo e optamos por mantê-lo sempre em movimento. No máximo, neste artigo compartilharemos algumas lições que aprendemos nas experiências colhidas ao longo dos anos, a partir de projetos executados com diversos parceiros, em diferentes domínios de conhecimento, passando por softwares puramente científicos em parceria com instituições como o INPE e a FIOCRUZ até, mais recentemente,  os aplicativos desenvolvidos em parceria com a Orquestra Ouro Preto e a Ouvidoria Feminina da UFOP.  

O público alvo deste artigo compreende profissionais e estudantes da área de computação que, apesar de saberem programar, ainda têm muitas dúvidas sobre como traduzir as conversas com um cliente potencial na especificação de um produto que atenda as necessidades deste cliente. Pode parecer uma tarefa trivial à primeira a vista, mas prazos ou orçamentos limitados e uma brincadeira de telefone sem fio trazem desafios que precisam ser vencidos para um projeto bem sucedido. Você deve estar se perguntando: – Que brincadeira é essa? O fato é que enviar toda a equipe do projeto para entrevistar o cliente pode ser caro, constrangedor ou coagir o cliente. Então, aquele membro da equipe que conversou com o cliente precisará explicar o produto encomendado ao demais. É nesta ocasião que muita informação é perdida! Além disso, muitas vezes o cliente não tem clareza daquilo que realmente precisa ou tem dificuldade em se expressar, enquanto que o entrevistador teria de ser absurdamente eficaz em compreender tudo que ouviu e transmitir na totalidade esse conhecimento a sua equipe. Consequentemente, a solução para esse desafio passa pela construção de um documento que registre tudo que foi compreendido e precisa ser lembrado. Ele tem diferentes nomes em cada empresas, nós o chamamos de Documento de Especificação de Requisitos (DER). 

 Existe uma dualidade no público alvo do DER. Do lado da equipe de desenvolvimento, seu conteúdo precisa ser formal o suficiente para permitir que todos entendam sem ambiguidades a missão do produto, as funcionalidades que deverão construir e testar ao longo projeto. Do lado do cliente que vai validar o documento, o conteúdo precisa ter uma linguagem simples e direta que para que ele não sofra com termos técnicos que atrapalham a compreensão daquilo que está sendo encomendado. 

A esta altura você pode estar se fazendo um monte de perguntas. É sério, essa turma está mesmo discutindo documentação! Mas e os métodos ágeis? Coisa mais obsoleta! Eu sempre fiz aplicativos e nunca precisei de um documento nestes moldes. Esse povo é doido? Calma…Você não está de todo errado, fique tranquilo! Nem nós, que alívio! Há sempre um ponto de equilíbrio, não é? Quando você tem alta rotatividade na sua equipe, membros entram e saem da equipe por diversas razões, ou quando você quer escalabilidade e manter diversas equipes atendendo diversos clientes, este documento registra o acordo firmado entre você e cada cliente, ele direciona e foca seus esforços e evita que você se repita ao longo do tempo, ao oferecer uma visão holística do produto ao cliente, aos analistas de UX, aos engenheiros de software e aos engenheiros de teste. Além disto, a nosso favor argumentamos que os métodos ágeis jamais condenaram a documentação, eles apenas privilegiam feedbacks rápidos a partir da avaliação contínua dos produtos pelos clientes. Então, o equilíbrio está em encontrar um DER que contenha informações mínimas, suficientes e necessárias para permitir o dimensionamento e esforço pela equipe de desenvolvimento e o acompanhamento do projeto pelo cliente, sem comprometer a agilidade do processo de desenvolvimento. À propósito, nós utilizamos uma versão customizada do processo SCRUM.

Sem mais demora, seguem algumas lições aprendidas nesta caminhada…

Lição 1 – Não se trata de fazer o aplicativo que tenha mais funcionalidades que os concorrentes. Se ele cumpre sua missão com menos funcionalidades…genial!

Atualmente, é muito difícil imaginar um aplicativo para o qual não exista um concorrente pronto. Logo o seu precisará de um diferencial, talvez será a simplicidade ou a praticidade. Não importa qual o diferencial, o fato é que ele precisa fazer tudo que o concorrente faz, certo? Não, na nossa opinião está errado! Uma causa comum de insucesso de projetos de software é, após a análise dos concorrentes, a equipe decidir por dotá-lo de todas as funcionalidades vistas nos concorrentes e torna-lo um monstro, enorme e formado por partes desconexas, difícil de entender, difícil de manter, difícil de evoluir e difícil de usar.

Outro ponto é, será que toda funcionalidade que o cliente pedir deve constar no aplicativo? Na nossa opinião, a resposta novamente é “não”, para a surpresa de muitos leitores! Afinal, o cliente não é o especialista em computação, ele precisa ser orientado sobre os impactos benéficos e maléficos que uma funcionalidade trará. O fato é que no início de um projeto, nem mesmo o cliente tem clareza daquilo que ele realmente necessita e sobre qual é realmente a origem do problema do qual ele padece. Então, tome nota de tudo, mantenha-se atento e ouça com cuidado, mas tenha em mente que você precisará de algum critério para determinar quais funcionalidades são realmente relevantes e, junto com o cliente, priorizá-las estabelecendo a ordem na qual serão desenvolvidas. 

 Na nossa opinião, a melhor estratégia para lidar com essa questão é: O quanto antes, defina a missão do aplicativo! Isto é, descubra qual problema específico do cliente ele tem de resolver. Registre no DER o enunciado deste problema, seja ele um problema de cunho econômico (aumentar custo ou reduzir gasto), um problema de cunho social (democratizar informação), um problema de cunho organizacional (integrar e manter atualizada informação de diversos departamentos) ou um problema operacional (coletar de forma integrada dados que são obtidos de forma separada por diversos dispositivos). Entenda e documente o problema do cliente em um exemplo tão concreto quanto possível. Muita vezes, esta documentação exigirá que você descreva o contexto organizacional do cliente em que a demanda pelo aplicativo surgiu. Como a organização do cliente funciona sem este aplicativo? Assim que entender bem o contexto organizacional e o problema do cliente, enuncie “a missão do meu aplicativo é resolver o seguinte problema específico:….”. Desta maneira, você poderá utilizar este enunciado para, junto com o cliente, avaliar as funcionalidades candidatas. Aqueles que forem de encontro à missão do produto estão dentro! Nós acreditamos que se um aplicativo resolve um mesmo problema com mais simplicidades (o que muita vezes significa menos funcionalidades), então, ele será mais fácil de aprender, usar, manter e evoluir. Portanto, mais propenso ao sucesso.  

Lição 2 – Não se trata de apenas entregar um produto. É preciso gerir as expectativas do cliente ao longo do projeto!

Ao longo de um projeto é comum que a equipe de desenvolvimento mude, com a entrada e saída de membros por diversas razões. Um pouco mais difícil de imaginar é que o cliente também muda. Não é incomum que os clientes de um aplicativo mudem de cargo, de profissão ou emprego, ou tirem férias, licença ou se aposentem. Além disto, quando o projeto de um aplicativo estiver próximo ao final, quando vocês estiverem próximos ao lançamento, os departamentos de marketing do cliente ou da sua própria empresa lhe pedirão informações sobre o aplicativo. Então, à todo momento, você precisará vender o conceito de um aplicativo e, estranhamente, até mesmo para a empresa que o encomendou. Por isso, sugerimos que você registre no DER os benefícios esperados que o aplicativo traga. Para cada categoria de usuário que irá interagir com o aplicativo, tente registrar que tipo de vantagens espera-se que o aplicativo lhe ofereça. Imagine o aplicativo Uber e responda: Quais são os benefícios para o motorista e para os passageiros?

Além dos benefícios, aplicativos sempre têm limitações, especialmente em suas primeiras versões. Como as pessoas em geral são bastante ocupadas e lidam com diferentes projetos/serviços o tempo todo, é comum que elas voltem a solicitar funcionalidades que já foram descartadas e/ou adiadas. Então, é saudável que você registre no DER o escopo negativo do projeto, identificando aquilo que o aplicativo não fará. Desta maneira, será mais fácil gerir as expectativas do cliente com relação ao aplicativo e a sua equipe de desenvolvimento.  

Lição 3 – Não se trata de “como” desenvolver o aplicativo. O desafio é descobrir “o quê” o cliente necessita. Esqueça que você é programador! 

Durante as conversas com organizações parceiras, especialmente quando elas não são de áreas relacionadas com a computação, notamos que no discurso das parceiras há uma mistura de tudo: Problemas que elas enfrentam, desejos de ter um aplicativo legal parecido com um que elas já viram, sugestões de tecnologias que elas gostariam que fossem utilizadas ou das quais já ouviram falar, insatisfações pessoais, divagações sobre a vida. É uma verdadeira mistura como é a vida e como é a mente humana. Então, cabe a você profissional treinado na “análise de requisitos” descobrir aquilo que realmente é relevante para o aplicativo que está sendo encomendado e identificar aquilo que realmente é importante para aquele momento. 

Alguma vezes, é importante até mesmo questionar se o problema colocado pode ser resolvido pela computação. Não adianta apenas oferecer um aplicativo, o importante é oferecer uma solução! A ética e foco em oferecer soluções devem prevalecer. Certa vez, uma pessoa veio a nós solicitando um aplicativo que equilibrasse a “energia” que ela dedicava a seus clientes e que os clientes dedicava ao tratamento que ela oferecia. Ouvimos com atenção e depois de entender o problema em seus detalhes, explicamos que as questões colocadas não poderia ser resolvidas por nenhum sistema computacional, seria preciso uma mudança em sua forma de se organizar e se relacionar com seus clientes. Para a organização de agenda e marketing sugerimos alguns aplicativos já existentes. Tempos depois, algumas parcerias surgiram porque esta pessoa nos recomendou a outras.

No momento de levantar os requisitos de um software, esqueça que você é um programador e que você domina algumas tecnologias! Não se distraia pensando em “como” implementar o aplicativo. Afinal de contas, não adianta aplicar computação de altíssimo nível e oferecer algo que não se ajusta às necessidades do cliente. É importante foco para descobrir “o que” o cliente realmente precisa e evitar contaminar a especificação do aplicativo com questões que podem ser decididas depois, com mais calma e em conjunto com profissionais especializados de sua equipe. 

Um bom caminho para seguir na especificação de um aplicativo é o seguinte. Antes de qualquer coisa, reconheça os tipos de usuários que irão utilizar seu aplicativo. Depois, identifique, para cada tipo de usuário, as funcionalidades que eles precisam. Então, agrupe essas funcionalidades em serviços. Caso haja alguma dúvida sobre uma funcionalidade, pergunte “para quê”  aquele tipo de usuário deseja esta funcionalidade. As respostas a esta pergunta podem esclarecer o critério de aceitação deste usuários, isto é, quais as condições para que estes usuários se dêem por satisfeitos com a implementação de uma funcionalidade.

Finalmente, resta uma informação importante, durante as conversas com o cliente não é o momento de oferecer soluções ou novas funcionalidades. Imagine-se comprando o carro, cujo o prazo de entrega e custo já estejam definidos e à todo momento o vendedor lhe pergunta: Você gostaria deste acessório? Que tal uma pintura metálica? Já viu esse novo equipamento de áudio? Nossa, que tal esse motor muito mais potente! É claro que para toda essas perguntas a sua resposta imediata será: Sim, eu quero! Aliás, preciso!!! Portanto, cuidado! No momento das conversas, limite-se a entender o problema e as necessidades do cliente, volte ao escritório e junto da equipe e de profissionais especializados, projete a solução que resolva com qualidade o problema e que demande menos esforço de toda a equipe. Na próxima reunião, ofereça ao cliente a solução abalizada por sua equipe.     

Lição 4 – Não se trata apenas de bons algoritmos e estruturas de dados. A experiência do usuário é muito importante! 

Imagine que você seja o “Ás da computação” e que seu aplicativo seja “PHoda!”. No que diz respeito à implementação das funcionalidades, ele utiliza os melhores algoritmos e estruturas de dados e que tenha sido construído com as mais altas tecnologias da atualidade.  Nada disso importa para a maioria dos clientes e sabe o porquê? Porque eles não conseguem ver isso, a experiência que eles têm no uso dos aplicativos encomendados em nada depende disso, na maioria das vezes. Então, no momento da concepção de um aplicativo esqueça toda a computação que você sabe e mantenha seu foco em um bom projeto de UX (User eXperience).

Nós temos em mente que, para conceber e projetar qualquer software, é preciso caminhar junto com cliente vindo “de fora para dentro”, isto é, vindo da visão dos usuários até que seja necessário discutir questões relativas à visão dos desenvolvedores. Evitamos a todo custo que o DER ou que as comunicações com o cliente envolvam jargões da computação. É preciso oferecer ao cliente e aos usuários acessibilidade completa a toda informação sobre o projeto, permitindo-o validar com facilidade todos os artefatos que geramos. Nos esforçamos em utilizar os jargões dos próprios clientes e usuários. O nosso objetivo é facilitar ao máximo as comunicações evitando de toda maneira mal entendidos.

Por isso, acreditamos que um dos mais importantes artefatos que utilizamos na concepção de um aplicativo seja o storyboard. O storyboard é um sequência de telas que captura o comportamento que os usuários esperam para o aplicativo. Ele apresenta em cada tela um conjunto campos, menus e botões que informam ao cliente onde e como as informações serão inseridas e onde e como os resultados serão apresentados. Ele também informa como acontecerão as transições delas. Como imagens dizem mais que mil palavras, as pessoas de fora da computação encontram mais facilidade em entender o projeto do aplicativo através dos storyboards e, de maneira muito intuitiva, conseguem perceber falhas de projeto e propor melhorias. Nós começamos com desenhos feitos à mão nos primeiros encontros com o cliente. A cada encontro, evoluímos os storyboards para terem exatamente a mesma aparência esperada para o aplicativo.

De uma forma geral, concebemos e projetamos software à partir da sua interface com o mundo, pois é somente esta interface que será percebida pelo mundo, não importando se esta interface é uma interface gráfica com o usuário final (GUI – Graphical User Interface), se é uma interface de programação (API – Application Programming Interface) que terá programadores como usuários ou se é uma interface de rede ou interface de hardware que será utilizada para comunicações com outros sistemas ou equipamentos.

Lição 5 – Não se trata de fazer o melhor aplicativo possível. Faça o suficiente para oferecer valor no prazo estimado e, depois, evolua!

Apesar de contra intuitivo, mesmo quem nunca desenvolveu um aplicativo, para agradar seu cliente, tenta desenvolver logo na primeira versão o melhor aplicativo que consegue. Este é um erro clássico de quem está inciando a carreira. Por nunca ter feito um carrinho de rolimã e tentar em seu primeiro projeto construir uma Ferrari, o ingênuo desenvolvedor acaba por estourar prazos e custos. Então, ele termina, na maioria das vezes, por fracassar em seu intuito e em obter um aplicativo que satisfaça seu cliente É preciso ter em mente um dito popular – O ótimo é inimigo do bom!

Diante deste cenário, mesmo tendo alguma experiência, nós adotamos e aconselhamos uma atitude mais conservadora. Ao invés de partirmos para o desenvolvimento do melhor aplicativo possível, preferimos fazer o suficiente para entregar valor ao cliente em ciclos curtos de desenvolvimento, para gradativamente o evoluirmos, inserindo novas funcionalidades ou melhorando sua UX. Para isso, ao final de cada ciclo de desenvolvimento, entregamos versões melhoradas dos aplicativos, solicitamos que os usuários as avaliem e colhemos os feedback de  todos. As mudanças solicitadas são incorporadas as próximas versões do aplicativo, de acordo com a prioridade estabelecida pelo cliente. 

Assim, podemos dizer que apesar de termos um DER que define o escopo do projeto e apresenta seu backlog geral, nós estamos sempre abertos a mudança e acreditamos em convergir o aplicativo para as necessidades que o cliente descobre a partir de muita experimentação. Enfim, não seguimos planos cegamente, sem reagir aos acontecimentos do caminho! Nós apenas entendemos que o planejamento inicial é uma linha mestra que nos mantém no rumo e nos permite dimensionar prazo e esforço. O destino é sempre resolver o problema do cliente, entregar valor e cumprir a missão do aplicativo. Então, manter o rumo requer ajustar o plano inicial e o aplicativo idealizado àquilo que o cliente realmente precisa. Fazemos aplicativos sob medida!

Lição 6 –  Não adianta ir rápido sem saber para onde. Colha feedbacks sempre que possível. 

É uma tendência comum que desenvolvedores iniciantes, principalmentes aqueles com fortes habilidades em programação e no uso de ferramentas para produtividade, se municiem das melhores ferramentas, processos e frameworks e comecem logo o desenvolvimento, solicitando agilidade de toda equipe. No entanto, é preciso ter em mente que o cliente irá entender suas próprias necessidades ao longo do projeto e que, no início, a maioria dos requisitos são instáveis e muito propensos a mudar conforme o cliente experimenta versões do aplicativo. Então, não adianta cansar a equipe sem a certeza de produzir os resultados desejados. Esta atitude poderia impactar negativamente a moral da equipe. 

O desafio aqui é não estafar a equipe ao solicitar retrabalho em grandes volumes por caminhar muito tempo na direção errada, simplesmente, por não colher o feedback do cliente. Por isso, nós utilizamos ciclos de desenvolvimento de duração fixa (uma semana), evitamos ficar sem a avaliação do cliente, adiar reuniões de validação e delongar entregas, esperando que as funcionalidades que gostaríamos de mostrar fiquem prontas. Nós sempre apresentamos tudo que pode ou precisa ser validado pelo cliente, mesmo em estágios intermediários de desenvolvimento. Apresentamos os artefatos que produzimos tão cedo quanto possível, para evitar que falhas de comunicação, concepção ou projeto resultem em mais custos e mais retrabalho. Assim, solicitamos que o cliente valide: Missão, escopo, storyboard, backlog, histórias de usuário, cenários de testes, componentes de software e versões do aplicativo. Temos em mente que o feedback do cliente é a bússola que nos mantém no rumo certo, que cumprir a missão do aplicativo é destino certo e que não adianta irmos rápido para o destino errado.

Bem pessoal, nós chegamos ao fim deste artigo e esperamos que as lições que colhemos lhes ajudem a conceber mais facilmente os aplicativos que têm em mente,  a cativar clientes e a construir um portfólio de projetos bem sucedidos. Que tal nos oferecer  um feedback? Ou nos dizer se estamos no caminho certo? Nos informe se estamos conseguindo ser úteis a vocês? Nos sentiríamos honrados com seu feedback e em receber encomenda de novos artigos. Nosso intuito é divulgar aquilo que fazemos e ajudar outros a fazerem também! Para incentivar essa cooperação e sermos mais eficazes em nosso intuito, colocamos aqui um DER completamente preenchido para que o tomem como exemplo e possam concretizar tudo que conversamos acima. Saiba como o projeto Batmóvel foi concebido! 

Exemplo_Especificação_Batmóvel_v1.4Baixar

Escrito por Prof. Tiago Carneiro e revisado por Ramon S. A. Barros e Camilla Silva."
Convertendo um Modelo de Inteligência Artificial para Web com TensorFlow js,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/08/post-tfjs-final-blog-730x350.png,"Introdução

Existem diversas ferramentas e bibliotecas que nos auxiliam na implementação de modelos de inteligência artificial. A linguagem de programação Python é hoje uma das mais completas para esse tipo de aplicação graças à larga disponibilidade de bibliotecas para o aprendizado de máquina e para a análise e visualização de dados que possui. 

Entretanto, quando se trata de soluções para web e mobile, a linguagem mais utilizada é o Javascript. E é aí que surge a dúvida: Como incorporar um modelo de inteligência artificial, construído em Python, em uma aplicação web ou mobile, construída em Javascript?

Nesse post nós vamos aprender:

A criar um ambiente virtual Python para instalação das ferramentas necessárias
Instalar a biblioteca TensorFlow.js que realizará a conversão do modelo
Converter um modelo Python para ser usado em uma aplicação Javascript
Modelo para Triagem de COVID-19 em Exames Raio-X

Para a exemplificação desse post, nós iremos utilizar um modelo de aprendizado profundo (Deep Learning), desenvolvido pela Universidade Federal de Ouro Preto (UFOP), em parceria com a Universidade Federal do Paraná (UFPA). Deep Learning é uma das técnicas mais proeminentes no cenário de aprendizagem de máquina e representando o estado-da-arte para diversas tarefas de visão computacional. 

O objetivo deste modelo é diagnosticar pacientes com COVID-19 através de imagens de raio-x do tórax. 

Para o código do modelo, acesse <https://github.com/ufopcsilab/covid-19>. 

Para mais informações veja os pre-prints em <https://arxiv.org/abs/2004.05717v4> e <https://dx.doi.org/10.21203/rs.3.rs-37908/v1>

Tem-se por meio da web, uma das maneiras de permitir a utilização do modelo pelo público em geral. Onde os usuário podem ter acesso e realizar os diagnósticos através de uma interface amigável.

Entretanto, como pode ser observado no repositório Git, o modelo foi construído em Python, e, como dito anteriormente, para soluções web o JavaScript é a linguagem mais comum. 

Então como fazer com que as duas linguagens conversem?  

TensorFlow 

O TensorFlow é a principal biblioteca de código aberto para o desenvolvimento e criação de modelos de Deep Learning (DL). Ele possui um ecossistema abrangente e flexível de ferramentas, bibliotecas, e uma comunidade grande e ativa. O que permite as mais diversas aplicações de DL. 

Como realizar a conversão de modelos?

Apesar de existir uma versão para JavaScript, a vasta maioria das aplicações que utilizam o TensorFlow é desenvolvida em Python. Felizmente, o TensorFlow.js permite a conversão de um modelo criado em Python para um modelo compatível com o JavaScript.

Para prosseguir neste tutorial é interessante que se tenha o Python e o seu gerenciador de pacotes pip instalados. Não é necessário, mas é recomendado o uso de um ambiente virtual (indicamos o pipenv ou virtualenv). 

Então, bora lá:

   1 . Criação do ambiente virtual (passo opcional, mas recomendável)

Por que devemos usar um ambiente virtual?

No ambiente virtual podemos instalar bibliotecas somente nas dependências do projeto que a utiliza, evitando, assim, conflitos entre as bibliotecas e a instalação global. Além disso, ele permite o controle de versão, em que se pode ter mais de uma versão de uma mesma biblioteca instalada em projetos diferentes, sem divergências.       

Nesse exemplos, nós usaremos o virtualenv, para criar e utilizar um ambiente separado para conversão do modelo. 

Então: 

pip install virtualenv 
virtualenv <nome_do_ambiente>
<nome_do_ambiente>\Scripts\activate

Para usuários linux o comando deve ser:

pip install virtualenv 
virtualenv <nome_do_ambiente>
Source <nome_do_ambiente>\bin\activate
 2 . Instalação da biblioteca TensorFlow.js

A biblioteca TensorFlow.js irá permitir a conversão de modelos criados em Python para modelos que possam ser interpretados pela biblioteca e executado em navegadores, servidores node e aplicativos react-native. 

Atenção: Antes de instalar a biblioteca atente-se a versão do Python que está sendo executada, pode ser que não haja suporte para a última versão lançada ou para uma versão muito antiga. Na data de escrita deste texto foi utilizada a versão 3.7.

Para a instalação basta executar o seguinte comando:

pip install tensorflowjs
  3 . Conversão do modelo

Nós usaremos um modelo criado com o Keras, gerado pelo código utilizado para diagnosticar pacientes com COVID-19, como mencionado na seção “Modelo para Triagem de COVID-19 em Exames Raio-X”.

Para aplicar basta executar o comando tensorflowjs_converter no terminal, como a seguir:

tensorflowjs_converter --input_format=keras --weight_shard_size_bytes=n modeloPy/model_final_smart.hdf5  modeloJs
–input_format  keras: O comando “–input_format” é usado para determinar o tipo de modelo que está sendo convertido. “keras” diz à biblioteca que o modelo é do tipo Keras.
 modeloPy/model_final_smart.hdf5: é o diretório onde está o modelo à ser convertido. OBS: deve obrigatoriamente ser escrito antes do diretório onde será salvo o modelo convertido.
modeloJs: diretório já existente onde será salvo o modelo convertido.
–weight_shard_size_bytes=n : seta o tamanho máximo dos arquivos de peso para “n”. Para web e servidores é opcional, porém para react-native deve ser utilizado para que seja criado apenas um arquivo, para isto basta utilizar um valor alto para n, em bytes.

Ao  converter o modelo será criado um arquivo “model.json”, que contém o grafo referente ao modelo, e arquivos “group1-shard\*of\*”, que contém os pesos da rede neural artificial.

Para outros tipos de modelos a conversão segue a mesma lógica e os formatos disponíveis e outros argumentos opcionais para conversão são descritos na documentação do tensorFlowjs.

   4 . Agora é só usar o modelo

Com o modelo convertido agora basta importar a biblioteca do TensorFlowjs e o modelo em seu código. Em um próximo tutorial ensinaremos, utilizando essa mesma rede, como executar uma aplicação em Nodejs e HTML/CSS.

Vale ressaltar que algumas funcionalidades ainda não foram implementadas para Javascript. Assim, caso você obtenha um erro como o abaixo, uma possível solução pode ser converter o arquivo “keras .h5” para um arquivo “saved_model.pb”.

(node:11476) UnhandledPromiseRejectionWarning: Error: Unknown activation: swish. This may be due to one of the following reasons:
1. The activation is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.
2. The custom activation is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().


Para criar um modelo no formato “saved_model.pb” crie um arquivo .py com o seguinte código:

import tensorflow as tf 
import efficientnet.tfkeras #necessário para este modelo

model = tf.keras.models.load_model('diretorio/modelo.h5')
tf.saved_model.save(model,'diretorio/modelo_pb')

Este erro ocorreu pois o modelo é baseado na rede EfficientNet, que possui camadas customizadas, então ao converter redes personalizadas com TensorFlow atente-se ao formato de origem.

Conclusão

Com o avanço do desenvolvimento de modelos de Inteligência Artificial, se torna cada vez mais necessário soluções que permitem viabilizar sua aplicação em produtos finais. 

Hoje, o Python é a linguagem mais utilizada para a construção de modelos, graças a sua vasta gama de bibliotecas voltadas para esse tipo de aplicação. E uma das principais maneiras de transformar um modelo em um produto a ser consumido é através de aplicações web e mobile, que utilizam principalmente o Javascript. 

Esse exemplo buscou, através de um exemplo de aplicação real, mostrar como realizar a conversão de um modelo Python, para uma aplicação Javascript. E assim, permitir que ainda mais desenvolvedores criem soluções que cheguem até o público em geral.

REFERÊNCIAS

https://tecnoblog.net/263808/o-que-e-inteligencia-artificial/

https://virtualenv.pypa.io/en/latest/user_guide.html

https://www.tensorflow.org/js/tutorials

https://github.com/ufopcsilab/covid-19

https://pythonacademy.com.br/blog/python-e-virtualenv-como-programar-em-ambientes-virtuais

3 Linguagens para Inteligência Artificial"
Como realizar um teste automatizado em aplicativos Web utilizando a IDE do CukeTest?,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/07/Simples-Foto-Selvagem-Post-para-Twitter-1-730x350.png,"Escrito por Ray da Silva Basílio e Vinícius Almeida de Mattos | Revisado por Diego Henrique e Arilton Aguilar

Os testes automatizados são utilizados na verificação das funcionalidades, para encontrar possíveis bugs e garantir a qualidade do software. Pode-se dizer que um software que passa por um detalhado teste de qualidade tem uma maior probabilidade de cumprir as suas especificações técnicas e deixar os clientes mais satisfeitos.

O objetivo deste tutorial é demonstrar o desenvolvimento de testes automatizados nos aplicativos Web implementados em JavaScript e React. Neste tutorial, utilizamos  a IDE do CukeTest que é gratuita e possui uma interface bastante amigável, principalmente para usuários iniciantes na área da automação de testes. Selenium WebdriverIO foi a biblioteca utilizada para inspecionar e interagir com os elementos da tela. Por fim, para o desenvolvimento dos cenários de testes utilizamos a biblioteca Cucumber.

Instalando e configurando o CukeTest

Primeiramente, o tester (nome dado ao profissional que desenvolve os cenários de teste) precisa fazer o download da IDE do CukeTest e configurá-lo para executar o teste “HelloWorld” (teste básico inicial) por meio do passo a passo descrito abaixo:

Depois de realizado o processo de download e instalação, abra a interface do CukeTest e você chegará na seguinte tela:

Clique em “File” e depois em “New Project”:

Vai abrir a seguinte tela:

Digite o nome do seu projeto no campo “Project Name”, selecione “WEB” no campo “Project Template”, selecione a pasta que você deseja salvar esse projeto por meio do campo “Project Path” e clique em “Create”:

O CukeTest abrirá a tela abaixo. No lado esquerdo estão as pastas e os arquivos do seu projeto como: features, step_definitions, support e etc. No centro da tela está localizado o cenário de teste (passo a passo que será executado na realização de um teste) e no lado direito fica o espaço reservado para a programação das funções, comandos e as demais configurações dos testes:

É de extrema importância instalar a pasta node_modules, que contém as dependências e arquivos fundamentais para a execução dos testes. Sem essa instalação os testes não funcionam. Para isso, passe o mouse na barra com o nome do projeto (neste caso, ”TesteWeb”) que está acima da pasta “features”, clique com o botão direito e clique na opção “Show in Cmd Window” para abrir o terminal do CMD na pasta do projeto:

Agora, instalaremos a node_modules. Considerando que o gerenciador de pacotes do “Yarn” esteja instalado em seu computador, basta rodar o comando: “yarn” no terminal e quando a instalação terminar, a tela do CMD deverá ficar aproximadamente da seguinte maneira:

Considerando também que o seu navegador Google Chrome está na versão mais atualizada, digite “yarn add chromedriver@latest” no terminal do CMD e aperte “Enter”. Caso contrário, você pode instalá-lo seguindo as recomendações do site da documentação do Yarn.

Agora, é possível visualizar a pasta “node_modules” na raiz do projeto: 

Executando um Cenário de Teste

O cenário de teste apresentado abaixo foi desenvolvido para navegar em uma determinada página Web (neste caso na página do Bing). 

Para executá-lo, basta clicar no botão de Play que exibe a mensagem “run this scenario” quando o usuário passa o mouse sobre ele. Depois disso, vai aparecer a janela do Google Chrome no site do Bing:

O navegador abrirá e fechará após o teste, por causa do comando “return driver.quit()” localizado no arquivo “hooks.js”, dentro da pasta “support”, que fecha a janela do Google Chrome no site do Bing. 

Feita essa configuração de teste inicial, pode-se desenvolver um código de teste mais elaborado, explorando mais funções e comandos visando atender diferentes tipos de funcionalidades dos diversos projetos de aplicativos Web. A seguir, apresentamos um exemplo básico de teste, que aborda essa estrutura de programação.

Exemplo básico de teste automatizado 

A seguir será implementado um cenário de teste genérico para navegar, clicar em botões, verificar a exibição de componentes (textos, nomes de usuários e etc…) e preencher campos de texto nas páginas Web. Assim, a implementação demonstrada nas imagens abaixo pode ser utilizada para testar diversas funcionalidades em qualquer sistema de aplicações Web.

Criando a feature do teste inicial 

 Para criar uma nova feature clique em “File” e logo em seguida em “New Feature”:

Para renomear o arquivo da feature, basta clicar duas vezes no seu nome e digitar o nome desejado, como por exemplo: “feature1”.

Logo em seguida dê um click duplo em “<Feature Name>” e coloque o nome desejado. Aqui nesta demonstração será colocado o nome: “Teste Padrão”. Da mesma forma será criada uma descrição para a Feature em “feature description” e por fim clique em “Add new scenario”:

Feito isso, a tela do CukeTest ficará assim:

Agora, clique em “Save” para salvar o código e coloque o nome desejado na feature, como, por exemplo, “Teste_Padrao”:

Agora é necessário adicionar os passos do cenário de teste da seguinte maneira. Para isso, clique nos passos e adapte-os para a página e os componentes presentes do seu projeto:

O primeiro comando do cenário de teste foi feito para acessar o site de um determinado aplicativo Web, navegando com o browser até o site do mesmo:

Depois disso, é possível clicar em um botão da página Web do aplicativo com propriedade ID genérica:

E quando a página ou qualquer componente desejado for carregado:

Então, o campo que tiver um ID será preenchido com o texto a ser inserido:

OBS: É importante, ao implementar os testes, utilizar o mesmo padrão na escrita dos comandos 1 a 4 listados acima, pois dessa forma você poderá reutilizar essas etapas facilmente, necessitando apenas de: trocar os termos entre aspas (identificadores dos botões, textos a serem inseridos nos campos, url de sites e etc…); e implementar esse comando padrão no arquivo “definitions.js”. Assim, você pode testar várias funcionalidades e todas elas conseguirão acessar esse mesmo comando apenas trocando o campo dos termos variáveis. Exemplo: com a implementação padrão de um comando para navegar em páginas Web no arquivo “definitions.js”, você poderá utilizar essa etapa no cenário de teste para navegar em qualquer site desejado. O primeiro comando do cenário de teste abaixo, é utilizado para navegar no site do Bing e o segundo no site do Google.

Para a implementação do código dos 4 comandos discutidos acima, desenvolveu-se no CukeTest os seguintes comandos no arquivo “definitions” da pasta step_definitions:

Na parte da implementação do arquivo “definitions.js” temos dois argumentos “arg1 e arg2” que correspondem respectivamente às strings entre aspas da parte do “Scenario”, podendo representar o ID do botão, o nome e o ID do campo de texto, e o texto que indica o nome ou componente da página. Esses args são associados às strings do cenário de teste pela posição, de acordo com a ordem em que eles foram inicializados no escopo da função dos comandos. Como o cenário de teste segue a metodologia do BDD, são necessários três comandos básicos, que são o Given (Dado), When (Quando) e Then (Então). O comando “await” é utilizado para fazer com que o teste aguarde até que uma determinada ação aconteça.

Explicando o código do arquivo “definitions.js”:

drive.get(url): faz com que o CukeTest abra um navegador e acesse a página com o url digitado no cenário de teste;
driver.findElement({id: arg1}).click(): procura um elemento na página do navegador com a propriedade ID denominada de  “arg1” e clica no mesmo”;
driver.findElement({id: arg1}): procura um elemento na página do navegador com a propriedade ID denominada de  “arg1”;
drive.sleep (long_time ou short_time): faz o navegador aguardar um determinado tempo em milisegundos;
driver.findElement({id: arg1}).sendKeys(arg2): procura um elemento na página do navegador com a propriedade ID denominada de  “arg1” e preenche nele o texto de “arg2”.

Cada funcionalidade (Login, cadastro de usuários e etc) deve ser desenvolvida um arquivo “.feature” diferente para promover a organização do código e facilitar a localização dos erros encontrados na realização dos testes.

Mesmo que os comandos dos cenários de teste estejam em features diferentes, a implementação de tais comandos deve ser desenvolvida em um único arquivo “definitions.js”, pois todas as definições das etapas dos testes ficam nele.

Feito isso, você poderá realizar diversos tipos de testes em aplicativos Web!!!

Considerações finais

Visando facilitar a verificação de funcionalidades dos aplicativos web, as soluções de testes automatizados se apresentam como abordagens bastante práticas, viáveis e econômicas. Elas são capazes de possibilitar correções preventivas nos defeitos de software, reduzir os esforços necessários corrigir as falhas, e consequentemente, minimizar os custos de manutenção nos aplicativos, além de serem reutilizáveis, fazendo com que sejam aplicáveis em diversos tipos de projetos. 

Este artigo apresentou um passo a passo de como utilizar a IDE do CukeTest para a realização de testes em aplicativos Web e um exemplo genérico de implementação de teste automatizado para navegar, clicar em botões, verificar se algum componente (mensagem de texto, nome da página) foi carregado e preencher campos de texto em uma página Web.

Assim, é possível aumentar a eficiência das suas aplicações, agregar mais valor, qualidade e confiabilidade aos produtos finais, fazendo com que os clientes fiquem bastante satisfeitos com as suas soluções de software desenvolvidas.

Este post te ajudou a compreender o funcionamento e a implementação de um teste automatizado em aplicativos Web? Você já desenvolveu um teste automatizado? Conte pra gente quais são os desafios e experiências que você tem com a automação de testes Web. 

Referências

CUKETEST. Visão Geral do CukeTest.Disponivel em: http://cuketest.com/zh-cn/bdd/.Acesso em: 04 jul. 2020.
SELENIUM. Getting started. Disponível em: https://www.selenium.dev/documentation/en/getting_started/. Acesso em: 15 jul. 2020.
WEBDRIVER.IO. API Docs. Disponível em: https://webdriver.io/docs/api.html. Acesso em: 15 jul. 2020.
YARN. Instalação. Disponível em: https://classic.yarnpkg.com/pt-BR/docs/install/#windows-stable. Acesso em: 04 jul. 2020."
Atenção ao escolher um framework de estilos para seu projeto WEB,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/07/Atenção-ao-escolher-um-framework-de-estilos-para-seu-projeto-WEB-1-730x350.png,"Escrito por Diego Matos | Revisado por Koda Faria

Um framework é um facilitador no desenvolvimento de diversas aplicações, e sem dúvida, sua utilização poupa tempo e custos para quem o utiliza. De forma mais básica, são padrões de artefatos e código que provêm diversas funções que podem ser reutilizadas em quaisquer novos projetos, conferindo produtividade à equipe e qualidade aos resultados. Porém, a utilização indiscriminada de frameworks pode gerar problemas. Devemos portanto seguir alguns passos para evitar esses problemas e, caso sejam inevitáveis, buscar fórmulas já testadas para resolvê-los. Este artigo trata especificamente de como o TerraLAB pensa seu framework para desenvolvimento de Frontends WEB. 

O que são estilos e estilização?

Um site construído apenas com HTML com certeza causaria um grande estranhamento devido sua ausência de estilização. É importante para qualquer aplicação WEB que haja estilos. Além de deixar a aplicação mais bonita, melhora a experiência do usuário. 

Geralmente os arquivos de estilização são construídos utilizando CSS (Cascading Style Sheets) onde são utilizadas tags para identificar elementos HTML e aplicar cada estilo a sua tag correta.

Todas as tags que representam elementos HTML existem dentro de uma estrutura de dados chamada Árvore DOM (Document Object Model). Essa estrutura permite algumas facilidades com uso do JavaScript, mas que não serão abordadas neste post. A árvore DOM permite uma busca por qualquer elemento ser intuitiva. Ela é construída por tags que podem ser pai ou filho de outras tags. Se o elemento <p> está dentro do elemento <div> então <div> é pai de <p> e <p> é filho de <div>.

Como escolher um framework de estilos

No TerraLAB, julgamos ser importante escolher apenas um framework de estilização (bootstrap, material ui, material io, materializecss, etc…) para um dado projeto. Solicitamos nossos colaboradores que dêem preferência ao framework que mais converge com os estilos determinados pelo cliente e pelo protótipo do produto. Um importante critério é escolher um que seja completo, o que evitará a procura de componentes presentes em outros frameworks. 

O principal fator para escolher apenas um framework é que ele modifica internamente quase todo seu projeto CSS. Se instalado conjuntamente com outro framework, a chance desses estilos criarem conflitos e sobreposições é muito grande, gerando componentes quebrados e esquisitos.

Mesmo assim, caso você decida pegar um componente presente em outro framework, há uma forma de evitar erros. Ao instalar esse novo componente você deve identificá-lo na árvore DOM de seu navegador e o examinar o mais “profundamente” possível. Assim você poderá pegar informações importantes como quais estilos estão se sobrepondo e quais os parâmetros como “className, id …” do componente.

Como a reutilização de estilos é feita na prática

Utilizaremos figuras para ilustrar essa situação:

Imagem 1- A linha azul escuro, na base da figura, destaca o objeto desejado identificado na árvore DOM. Também é possível ver, no canto superior direito, a área destacada em azul claro, onde o objeto (componentes estilizado) deveria ter aparecido na interface gráfica.

Imagem 2 – Aparência correta e esperada do componente com estilização


Fica claro observando as duas imagens que algo não está certo. O que está acontecendo é que a biblioteca principal do projeto (Materializecss) está sobrepondo o estilo desse componente de outra biblioteca (Bootstrap).

Imagem 3 – O componente na árvore DOM  aparece grifado em vermelho e a sua direita os estilos aplicados a ele.


Ao identificar o elemento devemos observar quais estilos estão sendo aplicados a ele. À direita é possível encontrar a tag que está sobrescrevendo seu estilo. É possível desativar esse estilo dentro do navegador e ver ele se realmente se aplicava ao componente. Ao desativá-lo, pode-se observar na imagem que o elemento fica em seu estado esperado.

Ótimo, agora encontramos o erro e é só resolvê-lo. Para tal devemos “diferenciá-lo”.

Primeiramente, vamos criar um “className” ou “Id” para o componente.

Imagem 4 (Componente agora tem um ClassName=“star-rating”)


Com isso feito vamos usar o “CSS selector” para selecionar exatamente a estilização que queremos alterar. Vimos na imagem 3 que o que queremos mudar é o estilo da tag “Label” e que agora está dentro do className “star-rating”.

Agora vamos no nosso arquivo CSS, de preferência o “GLOBAL.css” e selecionamos o estilo a ser alterado.

Imagem 5 (Arquivo .css selecionando o campo a ser estilizado)


Usando o atributo “unset”, retiramos qualquer estilo anteriormente utilizado para esses elemento. Sendo assim o componente deixa de ser sobreposto e o estilo dele volta a ser o esperado.

Imagem 6 (Estado final do componente)


Seu componente agora está pronto para o uso e sem riscos de quebrar algo em outra parte do código.

Considerações finais

A fim de evitar problemas é preferível utilizar apenas um framework de estilos devido a padronização dos elementos presentes fazendo tudo se “encaixar” bem. Se utilizarmos da solução apresentada nesse post evitaremos vários desses problemas de conflitos."
Propriedade Industrial Patente: O que é? E por que é importante?,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/07/15-07-Propriedade-Industrial-Patente-Redes-Sociais-730x350.png,"Escrito por Camilla Silva

No dia 15 de maio de 1997 entrou em vigor a lei nº 9279, que diz a respeito da propriedade industrial, sendo comumente conhecida como a lei das patentes. Essa regula direitos e deveres referidos à propriedade industrial, sendo esses a concessão e utilização não somente das patentes (invenção ou modelo de utilidade ou melhorias), mas como também de desenhos industriais, marcas e indicações geográficas.  Contudo, neste post o enfoque se embasará somente nas patentes.

Invenção ou modelo de utilidade?

De acordo com essa legislação apenas a invenção ou o modelo de utilidade são suscetíveis de patenteabilidade. Os três requisitos básicos para a patenteabilidade são: (1) a novidade, ou seja, deve ser algo novo, em outras palavras a invenção ou modelo de utilidade não deverá estar em uso público ou em conhecimento por pessoas; (2) utilidade e aplicação industrial, a grosso modo dizendo deve ser útil, a invenção deverá realizar exatamente o que foi designado no pedido de patente e as funções especificadas no modelo de utilidade devem atingir resultados benéficos; (3) e a não obviedade ou atividade inventiva, sendo esse o termo que explica que a invenção ou modelo de utilidade não deve ser óbvia para qualquer indivíduo de conhecimento médio científico ou técnico consiga reproduzir e alcançar os mesmos resultados.

A invenção configura-se como algo novo, do zero. Já o modelo de utilidade pode ser explicado como algo novo dentro de algo que já existe, um exemplo que pode ser citado para melhor entender é a invenção de aparelhos telefônicos, sua primeira patente deu-se em 1876 por Alexander Graham Bell, ele “inventou” um aparelho por transmissão de voz. Já como modelo de utilidade temos a proteção das melhorias funcionais no uso  ou fabricação de aparelhos telefônicos. A invenção ou modelo de utilidade patenteada garante o direito de comercialização que o que foi patenteado ou que receba royalties caso alguém usufrute do mesmo, porém somente por um período de tempo.

A duração da patente por invenção é de 20 anos contados a partir do depósito, independente se a aprovação ocorreu meses depois, e a de modelo de utilidade de 15 anos. A patenteabilidade deve atender aos requisitos de novidade, atividade inventiva e aplicação industrial, cumulativamente e não alternativamente.

Atividade inventiva:

Art. 13. A invenção é dotada de atividade inventiva sempre que, para um técnico no assunto, não decorra de maneira evidente ou óbvia do estado da técnica.

Art. 14. O modelo de utilidade é dotado de ato inventivo sempre que, para um técnico no assunto, não decorra de maneira comum ou vulgar do estado da técnica.

Industriabilidade:

Art. 15. A invenção e o modelo de utilidade são considerados suscetíveis de aplicação industrial quando possam ser utilizados ou produzidos em qualquer tipo de indústria

Depósito do pedido de patente

De acordo com o artigo 19 da lei de propriedade industrial, para  depósito do pedido da patente, deve-se constar no pedido: (I) requerimento; (II) relatório descritivo; (III) reivindicações; (IV) desenhos, se for o caso; (V) resumo; e (VI) comprovante do pagamento da retribuição relativa ao depósito.

Após esse pedido, será submetido a um exame que se aprovado será protocolizado. A data do depósito é a da apresentação do pedido. Pedidos que não atenderem a todos os requisitos previstos no art 19, mas que contiverem dados relativos ao objeto, ao depositante e ao inventor, poderão ser entregues mediante o recibo datado, logo serão dispostas exigências a serem cumpridas num prazo de 30 dias, podendo ocorrer a devolução ou arquivamento da documentação.

E por que é importante?

De acordo com Waldo Fazzio, o uso, a venda, a exposição à venda, quanto ao processo patenteado, sem a comprovação do possuidor ou proprietário mediante determinação judicial, se caracteriza como violação. É assegurado ao detentor da patente o recebimento de indenização. Em situação da não aprovação da patente, não se comprova ao autor como dono da invenção ou modelo de utilidade, podendo ocasionar a cópias do mesmo. 

Para saber mais 

 Nos dias 17 e 24 de julho, o Laboratório XR4Good da UFOP promoverá, em parceria com o NITE/UFOP e INPI um minicurso acerca dos aspectos técnicos sobre busca e redação de patentes em dois módulos. Para se inscrever basta acessar a página de instagram do laboratório (XRLGOODLAB) e acessar o link da bio. Não fique de fora, vale a pena conferir!

Bibliografia

http://www.planalto.gov.br/ccivil_03/leis/l9279.htm

https://www.youtube.com/results?search_query=lei+9279%2F96+resumo

https://jus.com.br/artigos/618/lei-9279-96-nova-lei-de-propriedade-industrial

https://www.direitocom.com/lei-da-propriedade-industrial-comentada/titulo-i-das-patentes-art-06-a-93/capitulo-i-da-titularidade-art-06-a-07

Fazzio, Waldo. Manual de direito comercial.12º edição. Editora Atlas. São Paulo, 2011."
Integração contínua e a entrega contínua (CI/CD) no GitLab – O caso de um Frontend Web,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/07/08-07-Integração-Contnua-e-Entrega-Contínua-Frontend-Redes-Sociais.png,"Escrito por Guilherme Carolino e Arilton Aguilar

Este tutorial busca demonstrar como o TerraLAB utiliza o GitLAB para realizar a integração contínua e a entrega contínua de seus produtos. Este é o segundo tutorial do total de três, sobre o tema CI/CD, desta vez iremos cobrir o caso de um frontend web.  

Alguns dos conceitos apresentados já foram explicados no primeiro tutorial e não serão explicados em detalhes novamente. Qualquer dúvida, caso ainda não tenha visto, você pode ler o  primeiro tutorial desta série aqui no blog! Ele apresenta os conceitos básicos do tema e explica como fazemos CI/CD em backends..

Este  tutorial oferece uma visão dos passos de um pipeline CI/CD completo para projetos de frontends web desenvolvidos em React e finaliza com o deploy no serviço de nuvem AWS. Daremos uma atenção maior aos scripts utilizados no pipeline do que nas ferramentas e dependências ao redor deles, algo já tratado na primeira parte da série de CI/CD.

PREPARANDO O GITLAB

Primeiramente, começaremos preparando nosso ambiente no arquivo .gitlab-ci.yml

Caso esse arquivo não exista no seu repositório, basta criar um novo arquivo com esse nome. 

Utilizaremos a imagem docker node:10 como padrão para nosso script e colocaremos quatro stages para nosso script percorrer. A imagem do node pode variar de acordo com o seu projeto ou você pode usar node:latest para ter sempre a última versão.

ESTÁGIO 1: testBuild

O primeiro stage do projeto é o stage testBuild, no qual é feito o build do projeto para teste e preparação dos arquivos do próprio teste.

Nesse script, começamos utilizando a palavra reservada *only-default ,para o estágio rodar apenas para a brach master, para merge requests ou tags. 

Logo abaixo, definimos que esse stage é o stage de testBuild e, mais abaixo, é possível utilizar uma outra palavra reservada “tags:”, para definirmos um runner específico onde o pipeline irá rodar. Nesse CI/CD utilizaremos os runners disponíveis no próprio GitLAB e por conta disso nós comentamos a linha contendo “tags”.

Depois é utilizado outra palavra reservada que é o “before_script:” onde definimos tarefas que vão ser executadas no nosso projeto antes do script principal. No caso do nosso before_script, estamos instalando o yarn, o gerenciador de pacotes que utilizamos.

Depois de definido o before_script, definimos o próprio script. Como apresentado na imagem acima, utilizamos o comando yarn para fazer o download das dependências do projeto e em seguida fazemos o seu build. Navegamos então para a pasta onde nossos testes feitos em selenium estão localizados e, com outro yarn, realizamos o download das dependências dos testes. 

A última coisa que fazemos é especificar como artefatos de saída desse estágio do pipeline as pastas build e node_modules dos testes. É necessário especificar esses artefatos pois eles serão utilizados em outro estágio do pipeline. Caso não os especifiquemos, essas pastas se perdem ao fim da execução desse estágio. 

Todo estágio do pipeline que possui artefatos vai expor esses artefatos para download no final de toda execução. Esses artefatos ocupam espaço de armazenamento no seu git e, dependendo no número de vezes que esse CI/CD for executado, este armazenamento pode rapidamente se esgotar. Para evitar problemas, especificamos então um tempo de existência para os artefatos. No nosso caso, os artefatos irão existir por uma hora e, ao fim desse tempo, serão eliminados da memória. 

ESTÁGIO 2: testDeploy

O segundo estágio é o testDeploy, no qual o deploy do site é feito em nosso servidor de teste na Firebase. Optamos por hospedar o site de testes na Firebase pois, em diversos casos, ter uma interface visual para os desenvolvedores pode ajudar na identificação de eventuais problemas. 

Diversos detalhes são comuns em todos os scripts então iremos nos concentrar em explicar o que cada um tem de único. 

Aqui, nosso before_script faz a instalação do firebase-tools globalmente. É necessário para podermos fazer o deploy do frontend web na Firebase.

O script executa o comando para o deploy utilizando um FIREBASE_TOKEN previamente cadastrado como variavel de ambiente no gitlab. 

Uma característica desse estágio é que ele é diretamente dependente do estágio anterior, isto é, ele precisa de arquivos produzidos no estágio anterior. Especificamos então em “dependencies” que precisamos utilizar os artefatos produzidos pelo stage testBuild.

Ao final da execução deste estágio do pipeline, nosso site já estará hospedado no servidor de testes.

ESTÁGIO 3: testRun

O terceiro estágio é o testRun, onde iremos rodar o teste automatizado.

Aqui, nós estamos novamente produzindo artefatos, desta vez com o intuito de manter os resultados dos testes contidos na pasta reports salvos por sete dias no gitlab. 

O before_script navega para a pasta de testes onde realizamos a instalação do yarn, chrome, java, cucumber, chromedriver e as dependências para os arquivos Debian. Algumas dessas ferramentas são obtidas pelo próprio node utilizando o yarn, outras precisam ser baixadas externamente e instaladas. 

Ao final da instalação de todas as dependências, podemos finalmente executar o nosso script de testes que consiste em um simples yarn cucumber. 

Lembrando que já tínhamos feito o build do teste no primeiro estágio do pipeline, então é necessário especificar este estágio como dependência. 

ESTÁGIO 4: productionDeploy

O quarto e último estágio é o productionDeploy, que só é feito caso tenha um merge request para nossa branch Master e o teste executado no stage anterior tenha sido executado com sucesso. 

A forma como fazemos o deploy para AWS é semelhante ao backend, apresentado na primeira parte desta série de tutoriais. 

A única diferença aqui está no updateAndRestart.sh onde damos o build do projeto dentro da AWS e em seguida rodamos essa build. 

Esperamos que este tutorial lhe ajude a implementar seu próprio pipeline! Nesse segundo tutorial nós apresentamos como lidamos com o CI/CD do front-end web, mostramos exemplos de como cada um dos estágios (stages) do pipeline devem ser customizados.. Existem diversas formas de realizar esse procedimento e esta é uma delas. E aí, você conhece outras? Quer conversar sobre elas? Teve alguma dúvida sobre nossa explicação? Comentá aí para podermos conversar sobre!"
Licenças e direitos autorais: Por que são importantes para os projetos de software?,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/06/24-06-Licencas-e-Direitos-autorais-Blog.png,"Escrito por Igor Muzetti

Este artigo busca desmistificar o assunto, mostrando os principais aspectos a respeito de direitos autorais e licenças de software. Este é um artigo de suma importância para o TerraLAB que, desde 2002, 18 anos atrás, desenvolve e distribui gratuitamente sob a licença LGPL o simulador de nome TerraME (www.terrame.org). Este desenvolvido em parceria com o Instituto Nacional de Pesquisas Espaciais (INPE) permite ao Brasil entender melhor os processos de ocupação do solo na região da Amazônia Legal (luccme.ccst.inpe.br), estimar com precisão suas emissões anuais de gases de efeito estufa (inpe-em.ccst.inpe.br) e, apoiado sobre bases científicas, apoiar a definição de políticas públicas voltadas para estas questões. 

Qual a relação entre direito autoral e licença de distribuição de software? 

Direito Autoral é um ramo da propriedade intelectual que incide, dentre outras obras intelectuais, sobre expressões literais, e que proíbe que terceiros as copiem, alterem, redistribuam ou vendam sem a autorização do Titular do Direito dos ativos intangíveis criados. O termo copyright é comumente utilizado para expressar tal direito. 

Ao contrário do que algumas pessoas pensam, o copyright é um direito que acompanha o software durante a sua vida, desde que tais direitos não sejam revogados por lei ou abdicados pelos autores. Assim, mesmo no caso de um software livre ou open source, quando alguém reaproveitar um código, os nomes dos autores originais não podem ser removidos. Caso você seja autor de um software e receba  contribuição de terceiros, em princípio, você e os demais autores “primários” deverão ser citados em copyright, pois possuem mérito intelectual. No entanto, poderá haver exceções tratadas pela licença do software.

Licenças de Software definem o que alguém pode ou não fazer com um determinado software. Isso envolve o conceito de direitos e deveres dos usuários e, muitas vezes, do próprio autor. As licenças se aplicam ao software na sua forma binária e, caso esteja disponível, ao seu código fonte. Todo software, para ser útil, deve possuir uma licença. A licença deve ser clara nas suas permissões e proibições. Não é permitido fazer algo que não esteja expressamente declarado. 

Leis para proteção à propriedade intelectual 

A Lei do Direito Autoral, Lei Nº 9.610, de 19 de fevereiro de 1998, define, no seu art. 7º, quais as obras intelectuais protegidas por meio da criação de espírito. No inciso XII deste artigo são inseridos também os programas de computador, como obra protegida. O § 1º rege que o programa de computador é objeto de legislação específica. 

A proteção à propriedade intelectual dos programas de computador é estabelecida pela Lei Nº 9.609, de 19 de fevereiro de 1998. Esta lei é conhecida popularmente como Lei de Software. A Lei de Software e a Lei de Propriedade Industrial (LPI), Nº 9.279 de 14 de maio de 1996, oferecem diferentes modos de proteção. A proteção dada pela primeira abrange apenas as expressões contidas no código utilizado, não os procedimentos ou métodos. Estes podem ser protegidos pela LPI, considerada uma proteção mais abrangente. 

Vale ressaltar que a proteção aos direitos relativos ao programa de computador independe de registro. No entanto, registrar o programa no Instituto Nacional de Propriedade Industrial (INPI) garante maior segurança jurídica ao seu detentor, caso haja, por exemplo, demanda judicial para comprovar a autoria ou titularidade do programa. Além disso, a proteção não é territorial como no caso das patentes, de outra forma, sua abrangência é internacional, compreendendo todos os 176 países signatários da Convenção de Berna (1886). No Estado brasileiro, o registro de software é feito no INPI e os direitos do titular são assegurados internacionalmente. O processo de registro é feito totalmente on-line, sem burocracia e com decisão totalmente automatizada. Maiores informações sobre o processo de registro você encontra no Portal do INPI.

Para que servem as Licenças de Software e como funcionam? 

Um software sem licença não pode ser utilizado por alguém ou empacotado por uma distribuição. Com certeza, você já notou que softwares proprietários de grandes organizações exibem na tela, durante a instalação, a sua licença. Há também licenças de documentação. Similarmente às leis, licenças dependem de interpretação, podendo gerar litígios e ações jurídicas. 

No entanto, licenças não são leis e não estão acima delas. Licenças são acordos entre partes. Outro conhecimento importante é o conceito de trabalho derivado. Trabalhos derivados são trabalhos que são feitos a partir de um outro já existente, dando continuidade a ele ou aproveitando partes dele para um novo trabalho. 

Ainda há o conceito de licença viral. A viralização ocorre quando uma licença faz com que qualquer software ou trabalho derivado tenha que usar a mesma licença de origem (Exemplo: General Public License – GPL).

Existe também o domínio público. No domínio público, uma obra (software, música, livro) pode ser utilizada livremente, sem a permissão ou pagamento de direitos ao autor. Cada país possui uma legislação específica, muitas vezes diferentes para livros, softwares e indústria. Não há um padrão internacional. Enfim, existem várias condições que definem se uma obra estará em domínio público ou não. 

O termo copyleft é usado por alguns, incluindo a Free Software Foundation (FSF), para exigir que um software (ou outra obra) tenham todas as suas versões diretas ou derivadas livres. A palavra é um trocadilho para copyright, mas não suspende os direitos cobertos por este. Geralmente, o copyleft gera viralização de licença. Por exemplo, se você criar um trabalho derivado, este trabalho também tem que ser distribuído como GPL. Compilar ou usar componentes licenciados como GPL pode ser considerado como trabalho derivado. Combinar dois módulos em um software significa conectá-los de maneira que eles agora formam um único programa maior. Se qualquer das partes for coberta pela GPL, toda a combinação também precisa ser disponibilizada sob a GPL. Se você não puder ou não quiser fazer isso, então você não pode combiná-las. O princípio geral é: se você usar software livre, licenciado com copyleft, em parte ou todo, para criar outro software, e pretender distribuí-lo, mesmo gratuitamente, terá que fazer sob algum tipo de licença copyleft, disponibilizando o código fonte e os avisos das licenças.

Tipos de Licença de Software 

Existem várias classificações de licenças. As licenças proprietárias impõem condições que cerceiam liberdades de alguém que tenha contato com o software. Exemplos: licença do Microsoft Windows e do Dropbox. Não fornecer o código fonte já é um cerceamento de liberdade. Licenças livres permissivas dão ampla liberdade para se fazer o que quiser com o código-fonte e o binário por ele gerado. Pode-se até fechar o código e torná-lo comercial (Ex: Windows e Mac OS usaram código FreeBSD com licença BSD). Exemplos de permissivas: BSD de 2 ou 3 cláusulas, a MIT e a Public Domain. As licenças livres restritivas não permitem que determinadas operações ocorram, com o fim de manter a eterna liberdade do software e dos seus descendentes. Um exemplo de licença livre restritiva é a GPL, que requer que trabalhos derivados também sejam GPL. As licenças restritivas, muitas vezes são conhecidas como copyleft. Há a possibilidade de multilicenciamento (Ex: Um software sob GPL 2 e/ou BSD de 3 cláusulas tem que seguir as duas). Nesse caso não poderá haver conflitos entre as licenças envolvidas.

Algumas licenças conflitam entre si e não podem ser utilizadas juntas em um mesmo projeto. Exemplo: a GPL 2 não é compatível com a licença Apache 2.0. A GPL 3 é compatível com a licença Apache-2.0. A GPL 2 e GPL 3 não são compatíveis com a BSD de 4 cláusulas. As GPL 2 e GPL 3 são compatíveis com a BSD de 2 cláusulas e MIT.

Alguns softwares proprietários e de código fechado possuem versões gratuitas. Por exemplo, o Avast Free Antivirus, o AVG Gree Antivirus e o CCleaner Free. Porém está escrito na licenças destes softwares que nenhuma solução do cliente é fornecida ou licenciada para uso por: (i) pessoa física para fins comerciais; ou (ii) empresa, sociedade, entidade governamental, organização não governamental ou outra entidade sem fins lucrativos ou instituição educacional.

No mundo Free and Open Source Software (FOSS), geralmente, as licenças são aplicadas a cada versão do software em questão. O autor do software pode, na maioria dos casos, modificar o licenciamento de uma nova versão. Mas se for BSD ou MIT? Pode! E se for GPL? Para mudar o licenciamento, todos os autores envolvidos devem estar de acordo, pois há copyright. O assunto é complexo. Pode haver mais de uma interpretação para uma mesma licença. As licenças são estipuladas por versão de software e os autores podem fazer modificações

As mais comuns de licenças para Software de Código Aberto

A Figura abaixo apresenta o resultado de uma pesquisa da WhiteSource sobre as licenças de código aberto mais populares de 2019.

Figura : Principais licenças de código aberto em 2019

A licença do MIT permanece no topo de popularização de licenças para código-aberto desde 2018 com 27% de uso. Isso não é uma surpresa, pois tem sido uma tendência no GitHub desde 2015. Ben Balter, advogado, desenvolvedor de open source e gerente de produto sênior do GitHub, disse que os desenvolvedores escolhem a licença do MIT porque “Ela é curta e objetiva. Ela informa aos usuários a jusante o que eles não podem fazer, inclui um aviso de direitos autorais e se isenta de garantias implícitas. É claramente uma licença otimizada para desenvolvedores. Você não precisa de um diploma em direito para entendê-la, e a implementação é simples.” De acordo com o site choosealicense.com do GitHub, a licença do MIT “permite que as pessoas façam o que quiserem com o seu código, desde que elas lhe devolvam a atribuição e não se responsabilizem”. Há dois anos, o Facebook substituiu publicamente a controversa licença do React por uma licença do MIT. Outros projetos populares de código aberto licenciados pelo MIT são Angular.js, Rails e .NET Core.

Seguem as descrições dos mais comuns  tipos de licença:

GPL (GNU General Public License): Ela reforça as quatro liberdades, e impede que o código-fonte (ou suas derivações) sejam apoderados por terceiros, virando software proprietário. É a que garante realmente as quatro liberdades, porém, restringe o software derivado através do copyleft.
LGPL (GNU Lesser General Public License): Apesar de ser baseada na GPL, permite o uso do software, neste caso bibliotecas de código, com software proprietário, desde que seja mantido o aviso de copyright. “Copyleft fraco”.
BSD (Berkeley Software Distribution): É utilizada pelo Unix derivado da universidade de Berlkeley, e é muito permissiva, não impondo o princípio de copyleft.
Apache (Apache Software Foundation): É utilizada pelos produtos da ASF, e é permissiva, não impondo o princípio de copyleft.
Mozilla (Licença pública Mozilla): É utilizada por vários produtos Mozilla, como o navegador Firefox. Permite exceções ao copyleft. “Copyleft fraco”.
MIT: Criada pelo Instituto de Tecnologia de Massachusetts (MIT). Ela é uma licença permissiva utilizada tanto em software livre quanto em software proprietário.

Considerações Finais

Este artigo teve como objetivo esclarecer as questões relativas à propriedades intelectual e ao licenciamento de produtos de software. Enumeramos as leis que regem o tema, os discutimos principais tipos de licença em uso na atualidade e descrevemos brevemente o funcionamento de algumas licenças mais comuns. Desta maneira, esperamos ter lhe ajudado a fazer a melhor escolha para sua equipe e parceiros. E como será que funcionam as licenças e direitos para o modelo de computação nas nuvens como o SaaS (Software as a Service)? Este assunto será tema de um post futuro aqui no blog!

Referências

http://www.planalto.gov.br/ccivil_03/leis/L9609.htm
http://www.planalto.gov.br/ccivil_03/leis/L9610.htm
https://www.youtube.com/watch?v=iySaBWeQt3E&t=3689s
https://people.debian.org/~bap/dfsg-faq.html
https://www.copyrightlaws.com/what-is-the-public-domain/
https://www.conjur.com.br/1999-abr-14/protecao_legal_programas_computador
https://www.gnu.org/licenses/"
Como contribuir para um software livre ou de código aberto pode impulsionar sua carreira?,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/06/10-06-Carreira-DEV-Blog-scaled.jpg,"Escrito por Igor Muzetti

No texto a seguir veremos que investir seu tempo em um software livre ou de código aberto pode alavancar sua carreira.

Contribua para projetos de software livre ou de código aberto e ganhe novas habilidades.

Ganhe novas habilidades

Adquirir habilidades técnicas e pessoais (hard e soft skills), obter oportunidades profissionais e concluir seu curso, podem ser considerados desejos de uma grande parte dos estudantes em computação durante seus cursos de graduação e pós. Alguns alunos conseguem experiências de pesquisa e/ou de desenvolvimento de software participando de projetos de pesquisa, laboratórios, projetos de extensão, empresas júniores, estágios, entre outras oportunidades que as Universidades propiciam. Outros alunos podem ter mais dificuldade em obter oportunidades como essas. Muitos fatores induzem essa dificuldade como: concorrência, localização geográfica dos cursos, estrutura das universidades, problemas sociais, entre outros. Contudo, é comum o mercado de trabalho exigir até dos recém-formados, experiências em diferentes tipos de tecnologias. Através de entrevistas e análise de currículos, organizações selecionam candidatos buscando perfis técnicos e pessoais que mais se adéquem com suas culturas e planos.

Neste contexto, projetos de software livre e código aberto podem ser considerados como um complemento e diferencial para sua próxima oportunidade profissional. O diploma é importante, mas ser capaz de demonstrar trabalhos realizados em paralelo ao período de graduação (ou pós) através da experiência de trabalhos em equipe e mostrar vontade de colaborar com a sociedade utilizando suas hard e soft skills, também é importante. Várias tecnologias de software livre e código aberto oferecem ensejos de experiência, como: comunidades de distribuições de sistemas operacionais, de bibliotecas, de ferramentas e de linguagens de programação. Contribuidores dessas comunidades normalmente apresentam e desenvolvem diversas habilidades técnicas e pessoais e podem se destacar aos olhos de futuros empregadores.

Formas de contribuir

Comunidades de software livre e código aberto sempre querem contribuidores. Qualquer pessoa pode contribuir para diversos projetos desta natureza, independentemente do seu nível de conhecimento. É comum existirem contribuições anônimas ou registradas nas bases de dados dos projetos. Existem diferentes maneiras de contribuição nesses projetos, são elas:

Empacotamento de software. Um pacote é uma coleção de arquivos que permite aplicativos ou bibliotecas serem distribuídos através do sistema de gerenciamento de pacotes. O objetivo de criar pacotes é permitir a automação da instalação, atualização, configuração e remoção de programas para os projetos de forma consistente e precisa. Um pacote normalmente consiste em um componente fonte e um ou mais componentes binários.
Instale e use o sistema operacional ou ferramenta. Ao instalar e usar, você terá testado o instalador, aumentará a base de usuários e divulgação, além de ficar familiarizado com os sistemas.
Reportar falhas. Ao encontrar uma falha nos sistemas, envie um bug report (relatório de erros) para que os Desenvolvedores fiquem sabendo e possam corrigi-lo. É possível acompanhar o ciclo das falhas reportadas.
Enviar correções. Se você sabe como resolver um bug, você pode enviar uma requisição de conserto. Os desenvolvedores irão analisar e se tudo estiver correto, eles aplicarão a sua solução.
Documentação. Você pode produzir documentação escrevendo manuais, tutoriais, HOWTOs, FAQs. Diferentes níveis de documentos como iniciante, intermediário ou avançado. Publicação escrita ou em vídeo.
Suporte a outros usuários. Ajude a tirar dúvidas de outros usuários. Existem muitos locais que você pode participar como listas de e-mails, canais no IRC, fóruns e grupos do Facebook, Telegram.
Divulgação. Ajudar a divulgar os sistemas em escolas, universidades, congressos, encontros, redes sociais, etc. Comprar produtos das lojas oficiais que ajudam na manutenção dos projetos.
Publicidade. Os projetos possuem um time de publicidade que elabora textos de notícias e mantém alguns sites e perfis de redes sociais. Você pode contribuir com o time de publicidade ajudando a escrever as notícias ou a revisar os textos. 
Organização de eventos. Esses eventos podem ser desde desde um encontro em um bar ou restaurante até um evento com palestras e oficinas relacionadas aos projetos. O importante é promover encontros da comunidade local para celebrar o projeto.
Produção de material gráfico. Você pode produzir materiais gráficos e disponibilizá-los para que outras pessoas utilizem livremente.
Tradução. Traduzir páginas dos sites, instaladores, descrição de pacotes, documentação, notícias, alertas de segurança, entre outros. Tornar os sistemas usáveis por mais pessoas também tem o objetivo de atrair mais contribuidores para o projeto. Quando você traduz do Inglês para o Português por exemplo algum artefato, acredite, essa é uma excelente maneira de melhorar seus idiomas.

Participar de projetos de software livre e código aberto possibilita interagir com um público grande, sem exigências de renda. Sistemas oriundos desses projetos oferecem o requisito não-funcional da portabilidade. Este requisito trata da capacidade de trocar informações ou executar o software entre diferentes plataformas. Exemplo: Até poucos anos, o IRPF só podia ser declarado em computadores que rodavam Microsoft Windows. Hoje, pode ser declarado em quase todos os tipos de equipamento.  

“Software Livre” e “Software de Código Aberto” são coisas diferentes

Vamos apresentar então brevemente a história de alguns elementos essenciais para entendermos melhor os conceitos de software livre e código aberto (open source). Em 1965 Ken Thompson e Dennis Ritcie (Bell labs, MIT, At&T) começaram a desenvolver o sistema operacional Multics (algo como múltiplos). Mas a junção dessas empresas não deu muito certo. Cada uma possuía interesses específicos e contraditórios. Em 1969 começaram a escrever um outro sistema operacional chamado Unics e depois renomeado para Unix. Escreveram usando a linguagem Assembly. Em 1973 reescreveram o Unix em linguagem C. Ken Thompson criou a linguagem B, que antecedeu a linguagem C. Para se ter uma ideia do quanto atual ele permanece, ele é um dos criadores da linguagem Go.

GNU é um sistema operacional tipo Unix criado por Richard Stallman, cujo objetivo desde sua concepção é oferecer um sistema operacional completo e totalmente composto por software livre. O nome “GNU” é um acrônimo recursivo para “GNU’s Not Unix!” (em português, é traduzido como “GNU Não é Unix!”). “GNU” é pronunciado como “menu”, com “g” em vez de “me”, e como o animal de origem africana “gnu”. O desenvolvimento do GNU, que começou em Janeiro de 1984, é conhecido como Projeto GNU. Muitos dos programas em GNU são lançados sob responsabilidade do Projeto GNU (os pacotes GNU). Stallman também criou um mecanismo legal de garantia para que todos pudessem desfrutar dos direitos de copiar, redistribuir e modificar software, o que deu origem a General Public License (GPL). E para institucionalizar o Projeto GNU, Stallman fundou a Free Software Fundation (FSF). O software em um sistema semelhante ao Unix que aloca recursos da máquina e conversa com o hardware é chamado “kernel”. Linus Torvalds em 1991 começou o desenvolvimento do kernel Linux. GNU é tipicamente usado com um kernel Linux. Essa combinação é o Sistema Operacional GNU/Linux. 

Software livre (free software) e código aberto (open source) são dois termos que geram muita confusão. Software livre é um movimento social, criado pela FSF, que visa a manutenção de quatro liberdades básicas de software para as pessoas. Trata-se de um filosofia. Um software é dito livre se os usuários possuem as quatro liberdades essenciais:

A liberdade de executar o programa, para qualquer propósito (liberdade 0).
A liberdade de estudar como o programa funciona, e adaptá-lo às suas necessidades (liberdade 1). Para tanto, acesso ao código-fonte é um pré-requisito.
A liberdade de redistribuir cópias de modo que você possa ajudar ao próximo (liberdade 2).
A liberdade de distribuir cópias de suas versões modificadas a outros (liberdade 3). Desta forma, você pode dar a toda comunidade a chance de beneficiar de suas mudanças. Para tanto, acesso ao código-fonte é um pré-requisito.

Assim sendo, software livre é uma questão de liberdade, não de preço. Para entender o conceito, pense em “liberdade de expressão”, não em “cerveja grátis”.

Open source é uma metodologia de desenvolvimento, criada pela Open Source Initiative (OSI), que respeita os mesmos princípios do software livre. Trata-se de uma visão mais técnica do que filosófica. De uma maneira geral, os dois termos descrevem quase a mesma categoria de software, porém eles referem-se a visões baseadas em valores fundamentalmente diferentes. Mas a visão da FSF e da OSI são diferentes em relação a algumas licenças. Assim sendo, há divergências. Muitas vezes trata-se de interpretação de texto. O conceito de open source veio diretamente da Debian Free Sofwtare Guidelines (DFSG). Open source são programas cujo código fonte é disponibilizado pelo Desenvolvedor. Mas isto não quer dizer que sejam considerados como software livre. Existe o termo FOSS: Free and Open Source Software. Um programa open source pode não ser FOSS. Um programa FOSS pode não ser software livre.  Em suma, para ser software livre, as quatro liberdades essenciais devem ser possíveis. 

Freeware é um software disponibilizado para uso, sem a exigência de pagamento de licença de uso, ou com licença opcional. Não significa que o código fonte estará disponível, e, se estiver, não significa que você tenha as liberdades de alterá-lo e redistribuí-lo. Licenciamento de software sempre foi um tema polêmico ou nebuloso para muitos. A licença de uso é quem diz se é um software livre ou não. Software livre, geralmente, usa uma licença do tipo CopyLeft. Este tipo é uma maneira de usar a legislação de proteção dos direitos autorais com o objetivo de retirar barreiras à utilização, difusão e modificação de uma obra criativa devido à aplicação clássica das normas de propriedade intelectual, exigindo que as mesmas liberdades sejam preservadas em versões modificadas. 

Software para o bem de todos

O TerraLAB desde sua concepção utiliza ideias, conceitos e soluções amplamente distribuídas pela comunidade de software livre. Geotecnologias como o sistema de informações geográficas Spring, o visualizador de dados espaciais TerraView, a biblioteca C ++ GIS de classes e funções TerraLib  são exemplos brasileiros, todos desenvolvidos pelo Instituto Nacional de Pesquisas Espaciais (INPE), o Spring com mais de quatro décadas de existência.

Há 18 anos, desde 2002, o TerraLAB em parceria com o INPE desenvolve e distribui o software livre e de código aberto chamado TerraME – Terra Modeling Environment (terrame.org).

Atualmente, o TerraME serve como base tecnológica para projetos nacionais desenvolvidos pelo Centro de Ciência do Sistema Terrestre (CCST) do INPE. O projeto LuccME (luccme.ccst.inpe.br) busca entender os processos de ocupação do solo na região da Amazônia Legal e no Cerrado brasileiro. O projeto INPE-EM (inpe-em.ccst.inpe.br) permite ao Brasil estimar sua emissões anuais de gases de efeito estufa devido às queimadas.

Para seu funcionamento, o TerraME estende a linguagem de programação LUA, um dos mais bem sucedidos projetos de software livre brasileiro, desenvolvido pelo Pontifícia Universidade Católica (PUC-RJ), para fornecer a pesquisadores uma linguagem de programação destinada à modelagem e à simulação das dinâmicas espaciais dos sistema sociais e naturais .

Além destes projetos de software livre, diversas ferramentas que fazem parte do pipeline de implantação DevOps do TerraLAB são softwares livres e de código aberto. Este assunto inclusive será abordado com maiores detalhes em um post futuro aqui no blog!

Referências

https://www.fsf.org/
https://opensource.com/article/17/11/open-source-or-free-software
https://www.gnu.org/
https://www.debian.org/social_contract.pt.html#guidelines
Carneiro, T.G.S., Lima, T.F., Faria, S.D, TerraLAB – Using Free Software for Earth System Research and Free Software Development, Workshop de Software Livre – WSL 2009, Fórum Internacional Software Livre, 2009, Porto Alegre, RS."
Integração contínua e a entrega contínua (CI/CD) no GitLAB – O caso de um Backend NodeJS,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/06/10-06-Integração-Contnua-e-Entrega-Contínua-Blog.png,"Escrito por Guilherme Carolino e Arilton Nunes

No TerraLAB, a ferramenta de versionamento  oferecida pelo GitLab é essencial e desempenha um papel muito importante na produção de software. Utilizamos seu versionamento de código e também fazemos uso intensivo da sua estrutura de issues, como foi tratado no último post. Nesse post, iremos mostrar outra ferramenta de grande importância do GitLab: o CI/CD. 

Este artigo busca ensinar como o TerraLAB utiliza o GitLAB para realizar a integração contínua e a entrega contínua de seus produtos de software. Trata-se do primeiro artigo da série CI/CD, um assunto extenso e rico em detalhes, que somente muita experimentação e pesquisa nos permitiu tratar. O artigo oferece uma visão geral dos conceitos e ferramentas utilizado no pipeline CI/CD, depois, ilustra seu uso a partir de um estudo de caso que ensina a realizar o CI/CD de um backend em NodeJS.

O CI/CD se define em integração contínua e entrega contínua. Com a integração contínua, é mais fácil e ágil testar uma nova feature em um ambiente semelhante ao de produção, e só fazer o deploy depois de todos os testes passarem, fazendo com que a qualidade e a velocidade de entrega de softwares seja maior no ambiente. 

Os testes regressivos garantem uma maior qualidade no produto e uma certeza maior de que novos bugs ou falhas não estão sendo inseridos na versão principal do software.

O “caminho” que o software vai passar ao longo do CI/CD é chamado pipeline e esse pipeline precisa ser executado em algum lugar. É aí que entra o conceito de Runner. Um Runner é uma máquina onde o CI/CD vai ser rodado. Ele pode estar localizado em alguma nuvem, como AWS, algum servidor ou até mesmo local no seu computador. 

Um pipeline de integração contínua pode ser feito de diversas formas, dependendo muito das ferramentas que o projeto necessita. Nesse caso, trabalharemos utilizando o Docker. Não importando onde você for rodar o Runner, nesse mesmo ambiente é necessário ter o docker instalado.

No link abaixo é possível baixar e instalar o docker para o seu sistema operacional:

https://www.docker.com/products/docker-desktop

A instalação é bem simples e não vai ser abordada nesse post. Não é necessário criar nenhum docker manualmente, o próprio Runner se encarrega disso. 

Um tutorial completo de instalação pode ser encontrado no site oficial:

https://docs.docker.com/engine/install/

É importante notar que, enquanto é possível utilizar um docker Linux em cima de um sistema operacional Windows, algumas funcionalidades são limitadas, por exemplo, a virtualização que é necessária para rodar emuladores como o de Android dentro do docker. Caso o seu CI/CD precise de um emulador em algum momento, seus sistema operacional base deve ser Linux. 

Abaixo tem-se um exemplo de tela do Gitlab. 

O primeiro passo para começar um pipeline de integração contínua é colocar um arquivo em seu repositório do chamado “.gitlab-ci.yml”. Esse arquivo será responsável pelo controle de como o nosso pipeline vai funcionar. 

O pipeline do CI possui três estágios padrões, porém não obrigatórios. Estes se definem em:

Build
Test
Deploy

Na imagem acima podemos ver um exemplo de arquivo de integração contínua que apenas faz o estágio de build (Utilizamos um script sem funcionamento para ilustrar um “stage” da pipeline). Cada stage do pipeline deve ser explicitamente declarado abaixo da tag “stages:” exatamente como o build foi. 

É importante notar que o Gitlab reconhece a indentação como parte da linguagem, então qualquer erro de indentação vai fazer o seu arquivo não funcionar. 

Como dito antes, o GitLab  nos permite rodar nossos pipelines em Runners. Ao executar o pipeline, esses Runners recebem automaticamente o conteúdo do seu repositório e executam tarefas que você designou conforme seu arquivo YML. Por padrão os Runners utilizados são Runners do GitLab, que além de possuirem cotas de utilização possuem certas limitações pois não são ambientes totalmente controlados pelo usuário e pode ocorrer de nos colocarmos em situações onde não temos a permissão necessária para alguma execução. Para tornarmos nosso pipeline mais interessante e controlável,  configuramos nossos próprios Runners.

A figura acima é um exemplo dos “Runners” oferecidos pelo GitLab (Podem ser encontrados nas configurações do seu projeto,na camada escrita CI/CD  e posteriormente em Runners).

Supondo que já temos o Docker instalado em nossa máquina vamos registrar nosso Runner. Existem diferentes formas diferentes de configurar nosso Runner. Para esse CI/CD, o Runner ficará no próprio servidor e utilizará o Docker para executar as tarefas.

Para configurarmos o Runner temos dois passos, o primeiro é baixar o GitLab Runner e instalá-lo em seu computador e o segundo passo é pegar o token disponível no site para atrelar o Runner ao seu projeto.

Podemos conferir o passo a passo para instalar o Runner de acordo com seu sistema operacional nesse link: https://docs.gitlab.com/Runner/install/. Para rodar o Runner, você deve ter o Git instalado e configurado no seu computador.

Após a instalação do Runner, é necessário que seja feito o registro do mesmo. Alguns dados sobre o seu projeto serão requisitados durante o processo. Eles podem ser encontrados na aba CI/CD do repositório, em “Set up a specific Runner manually”. 

Caso utilize um sistema operacional diferente do Ubuntu para o qual peguei de exemplo siga o passo a passo da documentação do GitLab https://docs.gitlab.com/Runner/register/ que não enfrentará problemas. O registro é extremamente semelhante em todos os sistemas operacionais.

Começaremos dando o comando para registrar o Runner: 

sudo gitlab-Runner register 

logo após executarmos esse comando é pedido uma URL do GitLab e entraremos com a padrão:

https://gitlab.com

Logo depois de colocarmos a URL ele irá pedir o token do seu projeto. É usando o token que ele vai vincular o Runner criado ao projeto de origem. O token pode ser encontrado na aba CI/CD do seu projeto, como demonstrado na imagem mais acima.

Após isso,é pedido uma descrição para o seu “Runner”, e fica de forma arbitrária o padrão de descrição a se seguir. É uma boa prática colocar uma descrição que detalhe um pouco sobre o funcionamento deste Runner. Logo depois uma Label é pedida. É importante colocar uma Label pois na hora de criar seu arquivo YML é utilizando as tags que você vai escolher em qual Runner o pipeline vai ser executado. 

Exemplos de tags: “Runner-produção”, ”Runner-homologação”, ”Runner-teste”.

Após a label, nos é perguntado o executor do Runner, e colocaremos “Docker”, seguindo o modelo abaixo:

Please enter the executor: ssh, docker+machine, docker-ssh+machine, kubernetes, docker, parallels, virtualbox, docker-ssh, shell:

docker

Como escolhemos o Docker como executor, seremos perguntados sobre uma imagem padrão para usarmos em nossos projetos, esta parte varia muito de acordo com o seu projeto e das ferramentas dele, dependendo da linguagem, banco de dados e suas dependências. Como faremos para uma aplicação em Node.Js utilizaremos a imagem “node:latest”. Essa imagem garante que possamos utilizar no docker funções atribuídas a última versão do Node.Js. 

Acabando o registro, basta inicializar o Runner com o comando:

.\gitlab-runner.exe start

Pronto , nosso Runner está atrelado a nosso projeto e vai aparecer como ativo na página de Runners.

Ficaremos com o seguinte resultado no site do GitLab (nesse caso existem dois Runners rodando), não se preocupe se a “circunferência verde” demorar um pouco para aparecer, demora um certo tempo até o GitLab reconhecer que o Runner está online.

Agora que instalamos tudo que precisávamos para nosso CI/CD acontecer vamos a parte mais importante que é a configuração de nosso arquivo YML. O arquivo YML deve ser feito de forma adaptada para nosso projeto, então seus serviços devem estar alinhadas com as dependências do projeto.

CI/CD para o Backend de um projeto Node.Js

Nesse arquivo YML acima está sendo executado dois estágios, o estágio de build e o de teste. Colocamos no início de nosso arquivo a imagem node:10 como a imagem a ser utilizada no Runner, e utilizamos outra palavra reservada “stages”, que define quais etapas serão executadas em seu arquivo. Utilizamos na linha 15 outra palavra reservada “cache” que serve para o compartilhamento de arquivos entre estágios, porque a cada estágio que é executado o Runner não salva por padrão o resultado deles, apenas os logs. Nesse caso, salvamos o “node_modules/”, que será importante tanto no estágio de build, quanto no estágio de teste. Vale ressaltar também que nosso script de teste utiliza de um banco de dados, e nesse caso é o MONGODB, então deixamos como serviço na linha 3 uma imagem do mongodb.

No estágio de build (linha 20) , definidos o stage em que estamos. Escrevemos build na linha 20 por convenção, porém esse nome não está vinculado ao estágio realizado. Poderíamos, por exemplo, chamar de build:android.  

Na linha 22 descrevemos as tags utilizadas. É aqui que especificamos em qual Runner o estágio co CI/CD vai rodar. No nosso caso, queremos rodar no “Runner-deploy-aws”, que é o nome que dei para meu Runner.

Logo após na linha 25, temos a palavra script. É aqui que o script de build é executado. No caso do nosso backend, é possível realizar o build simplesmente executando npm install. Caso tudo ocorra de maneira positiva, a próxima tarefa será executada, o teste. 

No teste também definimos nosso stage, nossa tag para selecionarmos o Runner e também utilizamos outra palavra reservada chamada artifacts, que serve para algo similar ao cache, porém ela salva o que você seleciona e deixa disponível para download depois que a tarefa é concluída. Por último o script de teste é chamado. Nosso teste é feito em Jest e pode ser executado com o comando “npm test”. 

A parte dos script é extremamente semelhante a execução local do backend, com exceção da necessidade de salvar em cache algumas coisas. 

Desta forma concluímos um pipeline de Ci,que incluia duas tarefas uma de build e outra de test.Os estágios do pipeline podem ser vizualizados em tempo real quando acessamos essa aba,e podemos vizualizar os logs de tarefa por tarefa,facilitando assim a depuração de erros e soluções para eles.

Exemplo de LOG do um stage de teste do script.

O ultimo stage é o de deploy. Esse estágio pode ser estruturado de diversas maneiras diferentes e depende muito da sua aplicação e onde ela vai ser hospedada. Essas condições irão modelar o arquivo de diferentes maneiras. No exemplo abaixo a aplicação está hospedada na nuvem da AWS e iremos acessar a instância via SSH. Quando realizada a conexão iremos rodar um Shell Script para colocarmos nossa aplicação no ar.

Nesse caso estamos utilizando o mesmo Runner que é o “Runner-deploy-aws” para fazer o deploy. Como já citado anteriormente existem diversas formas de executar o deploy e uma delas é deixando um Runner específico apenas para isso. O script para deploy dessa aplicação começa fazendo uma verificação do openssh-client e verifica se o Runner já possui ele instalado, se não ele roda um comando de update e vai instalar o openssh-client. Depois ele da permissão para o arquivo disableHostKeyChecking.sh rodar e por último chama um outro arquivo chamado deploy.sh.

Ambos os arquivos disableHostKeyChecking.sh e deploy.sh estão na raiz do nosso Git. 

O arquivo deploy.sh configura nossa private_key para acessar a aws através de uma variável global no Git. Em seguida ele roda o arquivo ./disableHostKeyChecking.sh e então faz a conexão com a AWS passando para execução o script no arquivo updateAndRestar.sh

As variáveis de ambiente $PRIVATE_KEY e $DEPLOY_SERVERS foram criadas no Gitlab na página abaixo. Deploy_servers contem o endereço da nossa máquina na AWS, no caso, toda a parte depois do @. Private_key contém o conteúdo do nosso arquivo “.pem” de acesso. Essas variáveis são secretas e não ficam expostas durante a execução do pipeline. 

Por último, abaixo está nosso arquivo updateAndRestart.sh. Inicialmente ele verifica se já existe uma aplicação na porta onde nossa aplicação ficará ouvindo, caso positivo, ele pega o PID e mata a aplicação. Logo depois ele atualiza o diretório através do git pull e roda os scripts npm install e npm start que são responsáveis por colocar a aplicação no ar.

Com isso, o CI/CD está quase completo. Para garantir a segurança do nosso projeto, realizamos outra configuração no Git. 

Em Settings > General, marcamos dentro de “Merge Requests” a opção “Pipeline must succeed”. Isso garante que um merge request só pode ser aprovado se o CI/CD for bem sucedido. 

Além disso, fazemos uma última alteração no nosso arquivo YML. Quando alguém cria uma branch a partir da master que contém o YML, o arquivo é levado junto. Você não quer que sua aplicação fique fazendo deploy toda vez que um commit for feito. 

Na linha 15 criamos o comportamento possível para os estágios do pipeline. Nas linhas 22 e 30 especificamos que aqueles estágios serão executados sempre que forem executados a partir das especificações da linha 15. Na linha 41 acrescentamos a palavra reservada “only” indicando que aquele estágio só deve ser executado caso ocorra um merge request."
"Estudo do INPE com apoio do TerraLAB/UFOP orienta contenção da COVID-19 nos municípios do Vale do Paraíba e Litoral Norte, SP",http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/06/INPE_Blog.png,"Escrito por Tiago Carneiro

No dia 01 de maio de 2020, um grupo de cientistas de diferentes instituições de pesquisa e ensino publicaram uma Nota Técnica sobre a situação da COVID-19 na RMVPLN – Região Metropolitana do Vale do Paraíba e Litoral Norte. Esta região integra 39 municípios entre as duas regiões metropolitanas mais importantes do país: São Paulo e Rio de Janeiro .  O mapa a seguir mostra a região e sua estrutura de conectividade.

A nota técnica liderada pelo Laboratório de Investigação de Sistemas Socioambientais (Liss) da Coordenação Geral de Observação da Terra (CGOBT), do Instituto Nacional de Pesquisas Espaciais (INPE), é assinada conjuntamente por pesquisadores do laboratório TerraLAB da UFOP, do Grupo de Métodos Analíticos em Vigilância Epidemiológica (MAVE) que reúne pesquisadores da FIOCRUZ e Escola de Matemática Aplicada da Fundação Getúlio Vargas – RJ, e do Centro Nacional de Monitoramento e Alerta de Desastres Naturais (CEMADEN). 

O TerraLAB tem importante papel na definição e Implementação de modelos computacionais espacialmente-explícitos que permitem simular a Dinâmica Regional da COVID-19 para analisar cenários onde são avaliadas diferentes estratégias de contenção da doença. Este modelos partem da hipótese de que, conhecendo-se a mobilidade regional, é possível simular e entender as possibilidades de espalhamento da epidemia no espaço metropolitano, além de entender o papel relativo de cada município na rede regional, em função de seu grau de conectividade. 

Neste sentido, a nota técnica destaca a  situação de 4 municípios. O município de Areias por apresentar a condição de vulnerabilidade mais alta entre os 39 que formam a RMVPLN. Os municípios de Ubatuba e São Luiz do Paraitinga por apresentarem condição de vulnerabilidade intermediária. E, finalmente, Taubaté por, apesar de apresentar condição de vulnerabilidade baixa, ser altamente conectado à municípios que apresentam epidemia instalada e com quantidades significativas de casos e óbitos confirmados, como São José dos Campos, São Paulo e Campinas.

 Assim, os pesquisadores explicam porque nenhum município sozinho e isoladamente pode ser efetivo no enfrentamento  à  pandemia de COVID-19 e concluem a nota técnica recomendando a imediata criação de um Comitê Técnico-Científico para o Enfrentamento da COVID-19 na RMVPLN.  Eles reiteram que esse comitê deve ser independente, formado por profissionais de notório saber no campo da saúde pública e do planejamento territorial. O comitê deve ter como foco a coordenação das ações para que se possa produzir as melhores informações, baseadas em evidências, para auxiliar a tomada de decisões em situação de incertezas acumuladas."
Entendendo o Funcionamento do Issue Boards no GitLab,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/IssueBoard-Blog-I.png,"Escrito por Arilton Junior e Tiago Carneiro

No TerraLAB, nós utilizamos outros artefatos na nossa linha de produção que são geridos de forma integrada através do serviço de versionamento de software oferecido pelo GitLAB. Além de utilizamos sua capacidade de controle de versão e armazenamento compartilhado de código fonte, fazemos uso intensivo da sua estrutura de issues (“demanda”, numa tradução livre para o Português) organizadas em boards (“painéis”, no Português).

Ao abrir o grupo do projeto, essa é a primeira tela que o usuário tem contato.

Nela podemos visualizar todos os subgrupos que fazem parte de um projeto específico. No exemplo acima, o projeto é o Geospotted. Os subgrupos utilizados no lab frequentemente são divididos em API, Frontend Mobile e Frontend Web.

Ao abrir um subgrupo, chega-se à tela abaixo:

Nesse momento, toda a barra lateral esquerda faz referência somente ao subprojeto ativo. No caso, GeoSpottedAndroid. Nosso interesse aqui está nas Issues. Passando o mouse sobre o menu Issues, é possível acessar um submenu com várias opções, dentre elas Boards. É em torno das Boards que nosso processo de produção se estrutura.

A figura acima mostra o Issue Board para o subprojeto GeoSpottedReactWeb do projeto Geospotted. No início, qualquer nova issue é inserida no board mais à equerda e, ao longo do projeto, vai avançando até chegar ao board mais à direita, indicando que a issue foi realizada com sucesso, aprovada pelo cliente do projeto e pela equipe por meio de testes regressivos automatizados.  No TerraLAB, os board têm a seguinte sequência não-natural! Rsrsrs

Open > Backlog > Sprint Backlog > Doing > Waiting Acceptance > Done > Automating Tests > Deploy > Waiting Validation (externally) > Closed

Você pode estar se perguntando porque “não-natural”!  É assim porque, naturalmente, as soluções desenvolvidas para a issues falham e, então, as issues que falharam retrocedem nos boards, lamentavelmente!

Para ilustrar, imagine que qualquer ideia nova é inserida como uma issue no board “open”. Ela ainda é só uma ideia, não faz parte do trabalho a ser realizado. Somente o Product Owner pode movê-la para o board “backlog” indicando que a ideia inicial já foi reescrita na forma de uma issue de acordo com um formato padronizado (em conteúdo e formato).  Agora sim, a issue é parte do trabalho que será feito para cumprir a missão do projeto. Então, quando um Product Owner move uma issue para o “Sprint Backlog” ele está, na verdade, solicitando que essa issue seja resolvida na próxima sprint. Quando qualquer membro da equipe de projeto fica ocioso, ele vem até esse board, move uma das issues para o board “Doing” e começa a resolvê-la. Quando ele acha que a resolveu, ele a move para o próximo board “Waiting Acceptance” e assim por diante. A figura abaixo mostra o significado de cada Board:

Observação de funcionamento: É possível arrastar uma Issue de um board para outra utilizando o mouse.

Agora, reparem que existem balões coloridos nas issues de um board. Esses balões são labels. Labels são marcações que a equipe de projeto adiciona às issues para indicar a sua natureza.  Da natureza de uma issue dependem a prioridade e importância que a ela é dada e o rigor técnico que é exigido na sua especificação (em forma e conteúdo). A figura abaixo apresenta as labels atualmente utilizados no TerraLAB.

A seguir, são descritos com detalhes cada um dos boards que estruturam o processo de desenvolvimento de software do TerraLAB:

 – Open: Aqui encontramos atividades abertas que não entram ou ainda não entraram no backlog, mas surgiram nas discussões da equipe de projeto com clientes e parceiros. Por exemplo: partes do projeto que os desenvolvedores não sabem como realizar e será necessário mais pesquisas entram aqui com ao label “research”. Sugestões de melhoria no projeto são criadas com a label “enhancement”, etc.

– Backlog: Aqui ficam todas as funcionalidades do backlog do produto. O backlog que foi levantado no “Documento de Especificação de Software” é inserido no git na forma de issues. Cada funcionalidade se torna uma issue no board “Backlog”. Outros itens eventualmente acabam parando no board “Backlog”, tais como sugestões de “enhancement” aprovadas pelo Product Owner. Os “bugs” relatados são tratados com máxima prioridade e, quando relatados, entram direto no board “Sprint Backlog”!

– Sprint Backlog: Aqui ficam as issues que serão trabalhadas na sprint atual ou, no máximo, na próxima. Antes de arrastar uma issue do “Backlog” para o “Sprint Backlog”, o Product Owner do projeto vai completar a especificação da issue com, pelo menos, uma “História de Usuário”. No GitLAB, é possível visualizar uma janela contendo mais detalhes de uma issue. Muitos destes detalhes podem ser editados. Ao especificar “Histórias de Usuários” e o “Cenário de Testes” como na figura abaixo, o Product Owner e o Tester Engineer, respectivamente, acrescentam os labels “Storie” e “Scenario” indicando que estes artefatos foram elaborados, revisados e aprovados.

No canto superior desta Janela, é possível atribuir uma issue a um ou mais membros da equipe responsabilizando-os por sua resolução. No TerraLAB, nos esforçamos para atribuir uma issue a apenas um profissional. Assim, em geral, após a criação de uma issue, após a revisão de sua solução, é natural de uma issue seja atribuída a um profissional específico. Ao longo das sprints, as issues são atribuídas a diferentes membros da equipe, sejam para solicitar que eles resolvam parte da issue ou para solicitar que eles revisem a solução desenvolvida por outro membro.

– Doing: São as issues que o desenvolvedor está trabalhando no exato momento.

– Waiting Acceptance: São issues que foram implementadas pelo desenvolvedor e estão passando por testes feitos pelos testers.

– Done: São issues que passaram pelos testes (previamente validados pelo Product Owner) e cuja implementação é considerada concluída.

– Automating Tests e Deploy: São boards especiais referentes as partes de CI – Continuous Integration e CD – Continuous Deployment do projeto. Quando uma issue é movida para“Done”, geralmente, isto significa que uma nova release do projeto pode ser liberada incluindo esta nova funcionalidade. Para isso, o projeto vai passar por uma série de testes automatizados, o CI, e durante esse tempo as issues que estão entrando nesse novo release irão ficar no board “Automating Tests”. Ao passar por todos os testes de integração, caso as soluções destas issues não injetem falhas no produto, estas soluções são transferidas para o board “Deploy” onde ocorre o CD.

– Waiting Validation (externally): Uma vez realizado o CI e o CD, um novo release do produto foi gerado no ambiente de teste e é, então, submetido à validação dos clientes e/ou usuários reais do produto, isto é, pessoas externas equipe de desenvolvimento e que, muitas vezes, são os encomendantes do projeto. Caso nenhuma falha ou melhoria seja relatada, essa issue finalmente é movida para o próximo board.

Done: Issues que passaram por todo processo de produção e foram validadas pelos clientes dos produtos e já podem ser implantadas no ambiente de produção dos clientes do produto.

Detalhes sobre a utilização do Issue Board

O Product Owner é o principal responsável por gerenciar o Issue Board. É ele quem define as “histórias de usuário” e é quem valida os “cenários de teste” especificados pelo Tester. Ele é também o principal responsável por coordenar as as atribuições de issues aos membros da equipe de um projeto, em consonância com seus perfis profissionais, ocupação e habilidades.

Sempre que um bug for encontrado, ele deve ser relatado imeditamente ao Product Owner que vai criar uma nova Issue com a label Bug.

Caso alguém (equipe ou cliente) tenha uma ideia de melhoria para o projeto, ele vai relatar ao Product Owner que irá criar uma Issue com a label “enhancement”.

Caso um membro da equipe tenha dificuldade ao implementar algo e precise pesquisar sobre o assunto, ele deve inserir uma nova issue com a label “research” no board “Open”. Ela será atribuída para um membro de equipe pelo Product Owner de acordo com a prioridade que este último estabelecer em comum acordo com quem relatou a necessidade de pesquisa. Enquanto uma pesquisa estiver em andamento, ela deverá permanecer com ela no board “Doing”. Assim, não é dificil imagina que um membro de equipe normalmente possui várias issues a ele atribuídas.

Existem outras visualizações para as issues além da “Board”.

Como a figura acima mostra, List apresenta todas as issues em formato de lista. Pode ser interessante trabalhar com a lista de issues quando é necessário ordenar as issues de forma específica, por data de criação por exemplo.

Também é possível visualizar as issues de um projeto inteiro. Nas figuras anteriores, estávamos visualizando as issues especificas de um subprojeto, porque é uma organização mais fácil de ser entendida e gerenciada. Contudo, quando necessário ver de forma geral todas as Issues, basta navegar para a raiz do projeto e utilizar novamente as opções disponíveis para visualização de issues.

O print acima mostra todas as Issues para o projeto GeoSpotted, reunindo issues dos subprojetos: API, Frontend Mobile e Frontend Web.

CRIANDO OS BOARDS DO SEU PROJETO

Criando as Labels

A primeira coisa a se fazer é abrir a raiz do seu projeto. A pasta que contém todos os subprojetos, como demonstrado abaixo. Em seguida, basta ir em issues e labels. Vamos criar as labels utilizadas.

Os projetos começam sem nenhuma label e todas devem ser criadas de acordo com a tabela de labels do TerraLAB. Essa tabela é encontrada aqui. Devem seguindo o padrão de cores e a descrição.

Criando os Boards

Após criarem as labels gerais para todo o projeto, entrem no subprojeto específico que querem criar os boards e acessem o issue board dele. Vocês verão uma tela igual a de baixo:

Caso já exista algum board, basta utilizar a opção de excluir e deixar igual a tela acima. A opção de excluir fica ao lado do título, no canto direito do board. Os boards “Open” e “Closed” não podem ser excluídos.

Conforme a figura abaixo, cliquem no botão “Add List” e escolham a label para gerar o board.

Os boards devem ser gerados na seguinte ordem:

Backlog > Sprint Backlog > Doing > Waiting Acceptance > Done > Automating Tests > Deploy > Waiting Validation (externally)

Após gerados, o Issue Board vai ficar assim:

Criando as Issues

A primeira coisa a ser feita depois de criar os boards é criar as issues. Todo o desenvolvimento começa no primeiro board, o Backlog. Aqui, pegaremos diretamente o backlog estabelecido no “Documento de Especificação de Software” e iremos passar tudo para o git.

Ao clicar em “Add Issues” como circulado na figura acima, a tela de issues vai se abrir e então clicamos em “New Issue”.

A tela abaixo vai se abrir. Em Title colocamos o título da issue. Em Description, uma descrição dela. Em Labels, escolhemos em qual coluna ela vai ser criada.

Todo o resto pode ficar da forma que está. Ao fim, basta clicar em “Submit Issue”.

Quando todas as funcionalidades do backlog estiverem sido acrescentadas como issues, o board de Backlog vai ficar preenchido, como na imagem abaixo:

Trabalhando o backlog

Como o projeto que vocês estão assumindo já estava em andamento, é necessário trabalhar esse backlog para refletir o estado atual do produto.

Primeiro, vocês devem acrescentar as histórias de usuário e cenários de teste para todas as issues que já tiverem esses dados especificados. Toda issue que já foi levada para o sprint backlog deve ter, no mínimo, uma história de usuário.

Quando preencher a issue com esses dados, caso os testes já tenham sido implementados e a issue passou nos testes, ela vai ser movida para “Waiting Acceptance”.

Se a issue que estiver aguardando aceitação for aprovada pelo cliente interno, ela vai ser movida para “Done”. Isso quer dizer que o cliente interno concorda com o teste implementado e confirma que tudo está certo.

Se nenhum teste estiver sido implementado ou a issue ainda não estiver passando nos testes, ela deve ser movida para a coluna “Doing”."
10 Apps Úteis no Home Office durante o Isolamento Social,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/22-05-DEZ-APPS-HOME-OFFICE-Blog-min-scaled.jpg,"Escrito por Fabrício Pereira | Revisado por Camila Silva

Devido ao cenário atual, o home office deixou de ser uma alternativa e passou a ser a melhor opção. Uma vez que há a possibilidade da equipe de trabalho concentrar suas atividades sem se expor ao risco de um possível contágio pelo Coronavírus. Com o objetivo de potencializar a produtividade das tarefas em home office, listamos abaixo 10 ferramentas que auxiliam tanto no gerenciamento do trabalho quanto a manter o foco nas atividades.

Google Meet

Como destaque na categoria de aplicações de videoconferências, cita-se o Google Meet disponibilizado gratuitamente até o dia 1º de julho. A ferramenta se diferencia por proporcionar sessões com até 250 pessoas e  se integra ao ecossistema de aplicativos da Google. O usuário pode agendar suas reuniões com a Google Agenda e compartilhar arquivos do Google Drive de maneira mais nativa e simplificada.

Forest

O celular pode ser uma poderosa ferramenta no gerenciamento de tarefas e na execução de planos de trabalho. O problema passa a aparecer quando as notificações de redes sociais e outros aplicativos, roubam a atenção e como consequência ocupam o tempo que deveria ser investido em trabalho. O Forest foi pensando justamente para ser um aliado nessas horas. A proposta do app é trazer elementos inspirado em games com uma lógica que obriga o usuário a se concentrar na atividade que realmente importa e se manter longe de distrações e do aparelho eletrônico. Como isso seria possível? Dentro da aplicação o usuário cultiva plantas e quanto mais se distanciar do celular, mais as plantações irão crescer e lhe render pontos. Caso contrário, as plantas irão morrer pela falta de foco do usuário ou até mesmo o app faz com que o smartphone toque músicas constrangedoras em altos volumes.

Pomodoro

A técnica Pomodoro consiste em um método de estudos/trabalho famoso pela sua efetividade em proporcionar foco. Criada nos anos 1980, a estratégia consiste em separar as atividades em intervalos de 25 minutos com pausas de 5 minutos e outra longa de 20 a 30 minutos. Várias são as ferramentas que utilizam o Pomodoro, destacamos aqui o site TomatoTimer. Sua interface é simples e intuitiva, permite o usuário controlar o timer tanto atráves de botões na tela como por teclas de atalho, receber notificações mesmo enquanto navega em outras abas ou softwares além de administrar as pausas e resetar o relógio caso deseje.

Evernote

Essencial para o cotidiano do profissional que lida com muitas tarefas e ideias ao mesmo tempo, o Evernote funciona como um bloco de notas capaz de manter na nuvem tudo o que o usuário deseja anotar e salvar. Ele pode, através de tags, planejar seu dia de trabalho com antecedência, agendar lembretes com clientes, ter informações sobre eles e montar o briefing de seus pedidos além de compartilhar tudo com colegas de trabalho.

Notion

Com uma interface mais minimalista, o Notion, similarmente se configura como uma ótima solução para se organizar através de anotações. Sua particularidade centra-se na capacidade de transformar rapidamente uma simples nota de texto a uma página de um caderno digital. Dessa forma o usuário é livre para organizar suas ideias e tarefas em esquemas que se aproximam de um quadro de avisos ou uma grande mesa de trabalho.

Pocket

Se o que você deseja é simplesmente salvar links de páginas da WEB para ler em outra hora, o Pocket pode se tornar o seu mais novo app favorito. Além de armazenar links, o Pocket disponibiliza as páginas que você salvou para ler mesmo sem uma conexão de internet. Além dessa funcionalidade, segmenta suas preferências e lhe recomenda conteúdos de acordo com elas e possibilita “seguir” colegas de trabalho a fim de ter acesso ao que eles leem. Com isso, sua equipe de trabalho pode ficar em sintonia compartilhando das mesmas ideias e inspirações para realizar as tarefas com maior empenho e motivação.

Microsoft To Do

O aplicativo Microsoft To Do é uma eficiente ferramenta no gerenciamento de tarefas diárias, semanais e mensais. Com a função “Meu dia”, o usuário pode planejar antecipadamente seus afazeres diários de maneira a tornar seu dia mais produtivo e organizado ou se planejar a médio e longo prazo com a função “Planejado”. O uso da ferramenta não se limita ao planejamento de tarefas de trabalho já que é possível diferenciá-las das pessoais, por exemplo, por meio da criação e gestão de listas.

Trello

Sente falta de um quadro de avisos que confere dinamismo e mantém todo o time consciente de tarefas e prazos? Inspirado em metodologias ágeis de desenvolvimento, o Trello se sobressai ao permitir a criação de inúmeros quadros para um mesmo time. Dentro de cada quadro, o membro do time pode criar listas de tarefas, separá-las por etapa de desenvolvimento (se estão pendentes, em andamento ou concluídas), e determinar prazos e quem será o responsável por cada tarefa. 

Github

Sinônimo de gerenciamento de projetos e versões de códigos, o GitHub também funciona como uma plataforma de rede social para desenvolvedores. Nela, você pode trabalhar de modo colaborativo com seus colegas ao mesmo tempo que acompanha  o andamento de seus projetos através do versionamento de seus softwares ou até se inspirar em produções desenvolvidas por inúmeros usuários do mundo todo.

Calm

Foco no trabalho e emocional abalado não são aliados a uma boa produtividade. Especialmente durante a atual crise provocada pelo Coronavírus é essencial valorizar o bem estar e a saúde mental. Por isso, recomendamos o Calm para momentos de estresse a fim de proporcionar horas de relaxamento. A partir da aplicação. é possível selecionar tempos de dois a 30 minutos, conforme sua disponibilidade e objetivos: dormir, relaxar ou simplesmente curtir o momento. 

Conhece uma ferramenta que merecia estar listada acima? Compartilhe com a gente nos comentários. "
Processo Seletivo TerraLAB 2020.1: Encerramento,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/13-05-Encerramento-Processo-Trainee-Blog.png,"Escrito por Fabrício Pereira | Revisado por Palloma Brito

Após 8 semanas de muito comprometimento e aprendizado, o Processo Seletivo TerraLAB 2020.1 se encerrou na semana passada. Mesmo com o cenário mundial em crise pelo Coronavírus, os 29 aprovados persistiram nos seus trabalhos através de home office com motivação de sobra.

Divididos em 5 equipes (squads), os trainees vivenciaram uma experiência semelhante ao mercado de trabalho, desenvolvendo 5 aplicativos WEB e Mobile ao todo. Por meio de desafios semanais, aprenderam a utilizar tecnologias como Angular JS, React e NodeJS através de cursos online e mentoria oferecidos por empresas de destaque no setor tecnológico da região como: Cachaça Gestor, Gerencianet, Stilingue e Usemobile. Conheça a seguir um pouco de como foi essa experiência e dos resultados:

Squad Amarelo

“O processo foi bem engrandecedor já que tive que aprender algumas tecnologias que ainda não havia tido contato onde consegui passar por quase todas as funções do squad (exceto Mobile), lidar com adversidades também foi bom para pensar em soluções e se utilizar do companheirismo do squad.” destaca Diego Henrique Marques Matos do squad amarelo. A equipe desenvolveu o app GeoLarica no qual o usuário pode avaliar estabelecimentos alimentícios da região, recomendando ou não os serviços. Confira algumas telas do projeto:

GeoLarica - Tela de Login
« ‹
1 de 5
› »
Squad Branco

O squad branco desenvolveu o app WindSun com o qual o usuário pode conferir a localização das estações meteorológicas brasileiras. Além disso, são disponibilizados dados de cada localidade como a velocidade média do vento. Sobre o processo seletivo, Larissa Viana da Silva relata: “Entrei no processo sem entender direito do que se tratava. Já no workshop, tive a sensação de que valeria a pena. Ouvir todos os palestrantes, principalmente os ex alunos da UFOP contando a experiência deles, me motivou a seguir em frente e encarar o desafio. Foram semanas intensas, de muito trabalho e esforço, mas também de muito aprendizado. Tive contato com as ferramentas de desenvolvimento e pude compreender na prática como funciona a construção de uma aplicação WEB e Mobile. A equipe do TerraLab sempre esteve a disposição para tirar nossas dúvidas, nos auxiliando durante todo o processo seletivo. Assim, entendo como é fazer parte do time e que pertencer ao laboratório poderia me agregar tanto profissionalmente, quanto pessoalmente.”. Veja a seguir algumas telas da ferramenta:

WindSun - Tela de Início
« ‹
1 de 2
› »
Squad Laranja

Responsável por desenvolver uma aplicação que auxilie a população local em meio a crise provocada pelo Coronavírus, o time laranja produziu o Covid Zone. O aplicativo permite ao usuário uma rápida avaliação de sua saúde via preenchimento de seus sintomas; dispõe de gráficos com os casos confirmados, recuperados e as mortes causados pela COVID-19; mostra ao usuário a localização dos casos confirmados dentro de sua região além de disponibilizar dicas de prevenção da doença. Sobre a experiência vivenciada junto ao seu grupo, Diego Henrique Marques Matos declara: “Esse squad em particular me deixou bastante animado com o projeto pois há muitos problemas que ainda não vimos sendo implementados e ter um viés social é animador.” Abaixo, as telas do app:

Covid Zone - Tela Inicial
« ‹
1 de 6
› »
Squad Verde

Encarregados de desenvolver uma aplicação para a Orquestra Ouro Preto, o squad verde reuniu notícias e curiosidades sobre o grupo musical, sua agenda cultural, vídeos e a possibilidade de se entrar em contato com a orquestra. O usuário também pode adicionar os eventos da Orquestra a uma agenda própria dentro do sistema. Ailton Sávio Sacramento Júnior, membro do squad, nos conta como foi a sua experiência dentro do nosso processo seletivo: “Foi prazerosa, deu pra aprender e melhorar em muita coisa, tanto na programação de software em si quanto também (e acho que principalmente) nas famosas “soft skills”. Meu squad conseguiu desenvolver um software de alta qualidade na minha opinião, que possa fazer parte do meu portfólio e quem sabe um dia me ajudar em alguma entrevista de emprego ou algo do tipo.”. Logo abaixo, você confere as telas do aplicativo:

Orquestra Ouro Preto - Tela Inicial
« ‹
1 de 5
› »
Squad Voluta

A empresa júnior do curso de Ciência da Computação, Voluta SD, também esteve presente no processo seletivo. Motivado em diminuir o crescente número de casos de violência contra mulheres durante o isolamento causado pelo Coronavírus, o time desenvolveu uma poderosa ferramenta. Dentro da aplicação a usuária pode registrar onde ocorreu a agressão e contatar os órgãos locais responsáveis pela sua segurança como a Ouvidoria Feminina da Universidade Federal de Ouro Preto (UFOP). Esses registros são disponibilizados em um mapa para outras usuárias da plataforma no intuito de instituir e fortalecer uma rede de proteção à saúde da mulher. Sobre a imersão promovida pelo TerraLAB durante essas semanas, Marcus Vinicius Souza Fernandes ressalta: “Está sendo muito benéfico adquirir este aprendizado para compartilhar com os demais membros da Voluta, para que assim, possamos adaptar alguns processos e trocar nosso framework. Todos os membros estão ganhando com isso, não só os que estão participando.”. Seguem as telas do projeto:

Aplicativo de Segurança da Mulher - Tela de Login
« ‹
1 de 4
› »

Parabenizamos e agradecemos cada participante que fez parte do nosso time durante essas semanas. A troca de conhecimentos e experiências promovida foi enriquecedora para cada um de nós. Aos que não foram aprovados, desejamos sucesso em suas trajetórias. O TerraLAB terá imensa alegria em recebê-los nas próximas edições de nosso processo seletivo.

Você ficou curioso sobre um dos projetos? Não deixe de acompanhar nosso blog e redes sociais para saber mais sobre cada um!"
Como Funciona o Versionamento de Software e como criar Tags no Git,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/06-05-Versionamento-e-Git-3.png,"Escrito por Koda Faria | Revisado por Fabrício Pereira

Quando estamos desenvolvendo um software grande ou uma API é preciso ter em mente que outras pessoas utilizarão o produto e isso explica o quão importante é manter compatibilidade de software. Entretanto, nem sempre conseguimos manter as novas versões com compatibilidade. Por isso é importante fazer o versionamento!

As palavras must, must not, should, should not e may fazem parte de um guia de boas práticas utilizado ao se falar de softwares e seus pré-requisitos, seja pessoalmente, em documentações ou pela internet:

MUST (DEVE): significa que algo é essencialmente necessário para a especificação ou projeto.
MUST NOT (NÃO DEVE): significa que algo é essencialmente proibido para a especificação ou projeto.
SHOULD (DEVERIA): significa que algo pode ser ignorado em situações específicas de uso, mas as consequências devem ser medidas e avaliadas antes de seguir um curso diferente do proposto.
SHOULD NOT (NÃO DEVERIA): significa que algo pode ser aceitável ou útil em situações específicas de uso, mas as consequências devem ser medidas e avaliadas antes de seguir um curso diferente do proposto.
MAY (TALVEZ): significa que um item é essencialmente opcional.
Regras para Uso da Semântica de Versionamento
Um software que utiliza a semântica de versionamento DEVE declarar uma API pública. Essa API pode ser declarada no próprio código ou existir em uma documentação. Independente da forma, ela DEVE ser precisa e de fácil compreensão.
Um número de versão padrão DEVE ter o formato X.Y.Z onde X, Y e Z são números inteiros não negativos e não devem conter zeros à esquerda.
X indica a versão major (principal)
Y indica a versão minor (secundária)
Z indica a versão de patch (correção de bugs)
Todos os elementos DEVEM sempre aumentar.
Exemplo: 1.9.0 → 1.10.0 → 1.11.0
Ao se lançar um pacote versionado o conteúdo desse pacote NÃO DEVE ser modificado. Todas as modificações DEVEM ser lançadas como uma nova versão.
A versão principal zero (0.y.z) é para desenvolvimento inicial. Tudo pode ser modificado a qualquer momento. A API pública dessa versão NÃO DEVERIA ser considerada estável.
A versão 1.0.0 define a API pública. A forma como o versionamento é modificado após esse lançamento depende inteiramente dessa API e de como ela é modificada.
O número de patch Z (x.y.Z | x > 0) DEVE ser incrementado apenas se bugs antigos e retrocompatíveis forem corrigidos. Uma correção de bug é definida como uma modificação de código que corrige comportamento inadequado.
O número minor (x.Y.z | x > 0) DEVE ser incrementado se novas funcionalidades retrocompatíveis forem inseridas na API pública. DEVE ser incrementado se qualquer funcionalidade da API pública for decretada como deprecated. TALVEZ seja incrementado se funcionalidades substanciais ou grandes melhorias são feitas em código privado. TALVEZ possua modificações a nível de patch. O número de patch DEVE começar em 0 sempre que um número minor for incrementado.
O número major (X.y.z | x > 0) DEVE ser incrementado se qualquer modificação NÃO retrocompatível for inserida na API pública. TALVEZ possua modificações a nível de número minor e patch. Os números minor e de patch DEVEM começar em 0 sempre que um número major for incrementado.
Versões pré-lançamento TALVEZ possuam um apêndice alfanumérico imediatamente após o número de patch. Pode conter apenas caracteres ASCII alfanuméricos e hífen.
Exemplo: 1.0.0-alpha
O que são Tags do Git?

As tags no git funcionam como marcadores para commits específicos. Sâo como branches, exceto que tags não são deletadas. São constantemente usadas para demarcar versões de software.

Existem dois tipos de tag: as lightweight tags e as annotated tags.

Lightweight Tag (tag leve)
Uma lightweight tag serve como um indicador de commit. É basicamente um indicador de branch que nunca muda. É apenas um ponteiro que indica qual commit representa essa tag. Contém também um arquivo checksum associado.
Geralmente é utilizada internamente pela equipe para dar nomes e indicadores para commits importantes internamente, mas que não farão parte de releases oficiais.
Annotated tag (tag com notação)
Annotated tags são salvas como objetos completos no banco de dados do git. Elas contém um arquivo checksum, o nome do criador da tag, email do criador e a data de criação. Possuem também uma mensagem associada.
São utilizadas em lançamentos públicos do software e impactam diretamente no usuário do software. Podem ou não vir acompanhadas de mudanças na API pública.
Como Criar uma Tag no Git? (Versão Linha de Comando)
Lightweight Tag
Para criar uma lightweight tag execute o seguinte comando:
git tag my_lightweight_tag_name
Annotated tag
Para criar uma annotated tag execute o seguinte comando:
git tag -a number_of_version -m ‘This is the number_of_version version’
-a indica a anotação da tag
-m indica a mensagem associada a tag

Os dois tipos de tag são criados por padrão na sua branch e commits atuais. Para que as tags criadas localmente sejam sincronizadas com o repositório online, execute o seguinte comando.

git push origin –tags

Para criar tags para commits antigos é necessário saber o código hash do commit. Cada vez que um commit é feito, um código hash associado é criado. Você pode encontrá-los digitando o seguinte código na linha de comando

git log –pretty=oneline

Para criar tags associadas a um desses commits antigos, copie o código hash e utilize o seguinte comando

git tag -a number_of_version hash_of_commit_here

COMO CRIAR UMA TAG NO GIT? (VERSÃO SITE DO GITLAB)

Para criar tags no site do GitLab, acesse primeiramente a página do projeto desejado. Clique em +, e em seguida em New Tag.

Na próxima página, digite o nome desejado para a tag e selecione a branch desejada.

Lightweight Tag
Não preencha o campo message. Release notes são opcionais.
Annotated tag
Preencha obrigatoriamente o campo message. Release notes são opcionais, mas fortemente recomendadas.

Clique em create tag e pronto!

As tags do projeto podem ser visualizadas ao clicar em tags na página principal do projeto.

Referências:

Semântica de versionamento

Como se tornar um mestre das tag Git

Git Basics – Tagging (Documentação oficial)

Key words for use in RFCs to Indicate Requirement Levels

Já passou por uma situação em que o versionamento seria de extrema valia para solucionar um problema de implementação? Nos conte nos comentários! Tem dúvidas sobre boas práticas de uso de Git e Github? Não deixe de nos acompanhar nas nossas redes sociais e nos mandar uma mensagem!"
Como Publicar um Aplicativo feito em React Native na Play Store,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/01-05-React-Native-Play-Store-5.jpg,"Escrito por Koda Faria | Revisado por Fabrício Pereira

Para publicar um aplicativo na Google Play Store é preciso que ele seja assinado com uma release key que precisa ser utilizada para todas as versões futuras do aplicativo.

Desde 2017 é possível que a própria Google Play gerencie essas chaves graças a funcionalidade de Assinatura de Apps da Google Play. Entretanto, antes de fazer upload do app é necessário assiná-lo com uma chave de upload.

Para ver o processo por completo e em detalhes, você pode acessar a página oficial “assinando seus aplicativos” na documentação do Android Developers. Esse guia é uma descrição simples do passo a passo necessário.

Observação: todas as localizações descritas são relativas à pasta principal do projeto, exceto quando outro caminho for indicado.

Gerando uma Chave de Upload

Você pode gerar uma chave privada utilizando o keytool. No Windows, você precisa executar o keytool a partir da pasta “C:\Program Files\Java\jdkx.x.x_x\bin” com o Prompt de Comando em modo de administrador.

Para gerar a chave, execute o seguinte código:

keytool -genkeypair -v -keystore my-upload-key.keystore -alias my-key-alias -keyalg RSA -keysize 2048 -validity 10000

Substitua conforme o projeto:

my-upload-key: nome da sua key;
my-key-alias: apelido da sua key;
-validity 10000: quantidade de dias da validade da key.

Esse comando te pedirá várias informações, conforme imagem abaixo. Preencha-as de acordo com o contexto de lançamento do seu aplicativo.

Nota: lembre-se que sua key store é privada. Caso a key seja comprometida de alguma forma, ou você a perca, siga essas instruções.
Configurando as Variáveis no Gradle
Mova o seu arquivo “minha-chave-upload.keystore” para a pasta “android/app”;
Edite o arquivo “~/.gradle/gradle.properties” ou “android/gradle.properties” e adicione as seguintes linhas, substituindo as informações pelas cadastradas no passo anterior:
 MYAPP_UPLOAD_STORE_FILE=my-upload-key.keystore

 MYAPP_UPLOAD_KEY_ALIAS=my-key-alias

 MYAPP_UPLOAD_STORE_PASSWORD=*****

 MYAPP_UPLOAD_KEY_PASSWORD=*****

Essas se tornarão variáveis globais no Gradle e poderão ser utilizadas posteriormente para configurar a assinatura do aplicativo.

Nota de segurança: Se você não gosta de manter sua senha em texto simples e está em um OSX você pode armazenhar suas senhas em um app de Keychain Access

Adicionando Configurações de Assinatura à Configuração do Gradle do seu APP

Edite o arquivo “android/app/build.gradle” e adicione a configuração de assinatura:

...

android {

   ...

   defaultConfig { ... }

   signingConfigs {

       release {

           if (project.hasProperty('MYAPP_UPLOAD_STORE_FILE')) {

               storeFile file(MYAPP_UPLOAD_STORE_FILE)

               storePassword MYAPP_UPLOAD_STORE_PASSWORD

               keyAlias MYAPP_UPLOAD_KEY_ALIAS

               keyPassword MYAPP_UPLOAD_KEY_PASSWORD

           }

       }

   }

   buildTypes {

       release {

           ...

           signingConfig signingConfigs.release

       }

   }

}

...
Gerando o AAB de Lançamento

Execute o código em um terminal:

cd android && gradlew clean && gradlew bundleRelease

A função bundleRelease criará um bundle de todos os javascripts necessários para rodar sua aplicação em um AAB (Android App Bundle).

O arquivo AAB gerado pode ser encontrado na pasta “android/app/build/outputs/bundle/release/app.aab” e agora está pronto para ser enviado para a Google Play.

Nota: para que o Google Play aceite o formato AAB o App Signing by Google Play precisa previamente ser configurado para sua aplicação no Google Play Console. Se você for atualizar um aplicativo já existente que não utiliza ainda o App Signing by Google Play cheque a seção de migração para mais informações.

Para cada AAB gerado que for ser enviado como uma nova versão é necessário que se modifiquem as variáveis “versionCode” (some +1 a cada novo aab) e “versionNumber” no arquivo “android/app/build.gradle” (suba conforme versionamento. Leia sobre versionamento aqui.) conforme mostrado abaixo:

android {

   defaultConfig {

       versionCode 1

       versionName “1.0.0”

       {…}

   }

   {…}

}

Mais informações:

Publishing to Google Play Store

Atualizando o número de versão de um app React Native

Ainda lhe resta alguma dúvida? Conhece alguma dica valiosa que deveria ser mencionada aqui? Não deixe de deixar nos comentários e nos seguir nas redes sociais!"
COVID-19: Como Projetos de Pesquisa Auxiliam no Combate a Doença?,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/04/Projetos-de-Pesquisa-COVID19-1.png,"Escrito por Fabrício Pereira | Revisado por Palloma Brito

Ao lado de outros dois pilares de uma instituição pública de ensino superior, os projetos de pesquisa representam significativo papel científico dentro da sociedade. São esses trabalhos que contribuem para avanços e criação de tecnologias que impactam diretamente o cotidiano e o bem estar dos indivíduos.

Em um momento de pandemia global como o atual, cabe a pesquisa desenvolver meios e estudos para gerir e erradicar o Coronavírus Sars-Cov-2, além de fornecer auxílio a população contra a patologia provocada por ele, a COVID-19.

A Universidade Federal de Ouro Preto (UFOP) não fica de fora desse movimento. O “CuidaIdoso” tem o objetivo de disponibilizar gratuitamente plataformas computacionais para suporte e orientação da população durante a pandemia. Através do site, Instagram e Facebook o grupo divulga diversas informações ligadas aos cuidados com os idosos, especialmente durante a atual pandemia. O conteúdo criado destaca-se pelo espaço dedicado não só ao monitoramento físico e psicológico do idoso mas também por orientações on-line e a criação de uma rede de colaboração. A ideia é que esses dados sejam disponibilizados em tempo real e usados para a determinação de regiões de risco, para que apoiem decisões do sistema de saúde sobre políticas de apoio à população.

A parceria entre integrantes do Departamento de Computação (DECOM) e Departamento de Análises Clínicas (DEACL) pretende explorar novos canais além do site e redes sociais. O interesse da equipe é abranger o conteúdo também em forma de um aplicativo que permitirá a conexão entre as pessoas e o apoio ao monitoramento de seu estado de saúde.

Com o objetivo de analisar a subnotificação de casos de COVID-19 no Brasil, pesquisadores da UFOP e da Universidade Federal de Minas Gerais (UFMG) publicaram um artigo com os resultados obtidos. O estudo que tomou como base o número de síndromes respiratórias registradas pelo Sistema Público de Saúde (SUS) ao longo dos últimos dez anos teve como resultado a proporção 7,7:1, ou seja, o número de casos reais no país deve ser de pelo menos sete vezes o número de casos divulgados. O cálculo serve como um alarme para a curva crescente de contágios da doença e a importância da prevenção por parte de toda a população.

Ainda no território mineiro, a Universidade Federal de Viçosa (UFV) através de seus seis laboratórios investe na detectação do coronavírus. O teste realizado é denominado RT-PCR e é feito em tempo real. Trata-se da coleta de uma amostra de secreção nasal e da garganta do paciente que é levada ao laboratório para uma busca pelo material genético do vírus. A expectativa dos coordenadores do projeto autorizado pela Secretária de Estado de Saúde de Minas Gerais é de 200 testes realizados ao dia. Tal iniciativa é de suma importância para o desafogamento do sistema de saúde local.

A Universidade também se destaca pelo projeto focado na produção de equipamentos de proteção individual. Em falta, os chamados EPIs são utilizados pelos profissionais de saúde no acolhimento de pacientes que apresentam sintomas da COVID-19. Diante disso, os pesquisadores da UFV se empenham em produzir EPIs do tipo máscara de proteção feitas por impressoras 3D. Com a estimativa de 25 EPIs produzidas por dia, o grupo planeja atender toda a região e aceita doações recebidas por formulário.

Mobilização semelhante a esta é a que acontece em quatro universidades paulistas. Os mais de 140 pesquisadores das universidades de São Paulo (USP), Estadual de Campinas (Unicamp), Estadual Paulista (Unesp) e Federal de São Paulo (Unifesp) estão divididos em projetos desde a produção de ventiladores pulmonares de baixo custo, testes de diagnósticos da patologia a pesquisa sobre a qualidade do ar atmosférico e sobre os impactos na economia.

Acontece também na USP um teste de medicamentos contra a COVID-19. Em parceria com a Eurofarma, o grupo avalia a atividade antiviral de compostos em células infectadas com o SARS-CoV-2, em testes in vitro. As análises automatizadas, acontecem de maneira simultânea e usam dezenas de milhares de fármacos toda semana. Os pesquisadores envolvidos estimam que mais de 2.500 compostos fiquem prontos em apenas algumas semanas.

O Ministério da Educação (MEC) se mostra ativo neste movimento ao promover dois editais pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES). O edital nº 11/2020 é voltado exclusivamente ao combate à pandemia, com foco no estudo de fármacos, vacinas, produtos imunológicos e temas correlatos. Já o edital nº 12/2020 refere-se ao desenvolvimento de estudos, procedimentos e inovações tecnológicas em telemedicina e análise de dados médicos não só para o combate ao coronavírus mas também para temas afins.

É também de autoria do MEC uma plataforma de monitoramento das instituições de ensino. A ferramenta dispõe de listas das ações de enfrentamento desenvolvidas pelos centros acadêmicos reunindo números de instituições por ação; o número de ações realizadas; dos médicos, enfermeiros, farmacêuticos e fisioterapeutas dedicados a elas. São encontradas também um conjunto rico de informações acerca dos inúmeros projetos, como os acolhimentos psicológicos virtuais e o Tele Coronavírus, uma rede de atendimento mantida pela Universidade Federal da Bahia (UFBA) destinada a orientar a população sobre o coronavírus. A plataforma também dá ao visitante a possibilidade de saber sobre o funcionamento dos institutos e universidades federais durante a pandemia. As datas da suspensão das aulas de cada instituição e universidade, assim como as que oferecem aulas remotamente, são disponibilizadas no sistema.

Em resumo, a ciência deve ser valorizada especialmente em momentos como este. Por meio dela que avanços tecnológicos na área da saúde são possíveis e permite, ao lado dos profissionais de cada setor, progredir rumo ao combate à crise.

Conhece algum projeto que merece ser reconhecido pelo seu valor no cenário contemporâneo? Nos conte abaixo nos comentários! Não deixe também de acompanhar nossas publicações aqui no blog e nas nossas redes sociais."
Tecnologia Vs Coronavírus: Conheça 5 Tecnologias que Agregam no Combate ao Vírus,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/03/COVID-Blog.png,"Escrito por Fabrício Pereira | Revisado por Palloma Brito

Diversas são as maneiras para evitar e combater o novo coronavírus. Numerosas também são as possibilidades da Computação contribuir para a causa. Conheça abaixo cinco dessas tecnologias:


O Auxílio de Drones para a Conscientização Social

Instituições governamentais europeias e asiáticas aplicaram medidas para a conscientização populacional sobre o perigo da aglomeração durante a atual pandemia. Pode se citar como exemplo a Espanha, local em que foi promulgado um decreto que ordena o isolamento por 15 dias. Lá a polícia informa, por meio de drones, os que insistem em desobedecer o decreto. Através de um aviso sonoro, esses drones ajudam na contenção dos casos no país.

Esses veículos aéreos transportam amostras médicas entre o centros de controle da epidemia na China. O primeiro país a sofrer com os impactos da doença se beneficia desse movimento para a contenção de contágios entre seus habitantes.

O Suporte da Inteligência Artificial

A China também usa a inteligência artificial desenvolvida pela Infervision no diagnóstico do COVID-19. A plataforma escaneia imagens de tomografia dos pulmões dos pacientes e facilita que os sinais da patologia sejam facilmente observados pelos médicos.

Um projeto apoiado pela Microsoft em Seattle – EUA também se utiliza dos benefícios da inteligência artificial em testes da doença. Os usuários fazem a avaliação em casa e recebem o resultado em até dois dias. Em caso afirmativo, deve ser respondida uma série de perguntas sobre os locais onde a pessoa esteve e os entes que teve contato.

A Biometria em favor da Detecção de Sintomas

A chinesa ZKTeco distribuiu terminais capazes de rastrear sintomas do COVID-19 através do reconhecimento facial em suas fábricas. O equipamento leva frações de segundo para determinar a temperatura de uma pessoa. Esta aferição é feita pela palma da mão à distância, a fim de se evitar que o aparelho fique contaminado e contagie outras pessoas.

A Geolocalização a Serviço da Telemedicina

Quando se trata de visualização de ocorrências em territórios, a geolocalização de dispositivos é uma poderosa aliada à telemedicina. Por intermédio dela a startup paranaense SIGA disponibiliza, a partir da coleta dos dados fornecidos pelo Ministério da Saúde, o mapa da evolução da pandemia no Brasil. Nele, é possível visualizar os casos confirmados em círculos vermelhos e os de morte em círculos pretos junto dos números em cada estado brasileiro.

Ainda em nível nacional, se destaca o CovidZero. A plataforma é desenvolvida por profissionais tanto do setor de tecnologia da informação quanto de comunicação e acumula mais de 1200 voluntários. Através dela o usuário se informa sobre os números dos casos confirmados e de óbitos em cada um dos estados; filtra esses casos por cidades e, por meio da geolocalização de seu dispositivo, do local onde estiver situado. São disponibilizados também inúmeros artigos sobre a origem da patologia, medidas de proteção e demais informações relevantes sobre o tema. 

Projetos de instituições de ensino fornecem também potentes recursos na situação atual. O dashboard desenvolvido por Johnnatan Messias, ex aluno de Ciência da Computação pela Universidade Federal de Ouro Preto (UFOP), cobre a pandemia em nível global. Atualizado em tempo real, o site dispõe tabelas e gráficos com os casos confirmados, recuperados e de óbitos dos países mais afetados pelo coronavírus. Destaque também para o dashboard implementado pela Universidade da Virgínia que contempla tanto os itens citados como os organiza em uma linha do tempo pelo globo terrestre.

O Investimento do Governo Brasileiro em Telemedicina

Promovido pelo Ministério da Saúde, o aplicativo Coronavírus-SUS reúne informações e orientações acerca da doença. Mediante um formulário de autoexame, o usuário elenca os seus sintomas, dessa forma o app verifica a possível contaminação da patologia. Disponível para Android e iOS, o app já conta com mais de 500 mil downloads.

O que você tem feito para evitar a contaminação deste vírus? Compartilhe com a gente nos comentários e não se esqueça de acompanhar nossas redes sociais."
Processo Trainee TerraLAB 2020.1: Update,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/03/ProcessoSeletivo-Blog-1.png,"Escrito por Fabrício Pereira

O primeiro evento do Processo Trainee TerraLAB 2020.1 aconteceu no dia 13 de março – o Workshop TerraLAB 2020.1. Nossas parceiras de sempre – Stilingue, Usemobile, Cachaça Gestor e Gerencianet – nos brindaram com a sua presença em palestras proveitosas sobre arquitetura de softwares, desenvolvimento para Android, uso do AngularJS e NodeJS em aplicações WEB, gestão de pessoas, processos ágeis, etc. Lotando o auditório do Departamento de Computação (DECOM) de 08h até às 18h, o evento também contou com a participação ilustre da equipe de Quality Assurance do iFood compartilhando conosco poderosas dicas de como, através de testes automatizados, é possível se garantir a qualidade de um produto ou serviço dentro do mercado. O Dr. Igor Soares, ex engenheiro de software da Google por 10 anos, nos prestigiou com uma bela palestra sobre sua enorme vivência nessa indústria.

Auditório do Departamento de Computação (DECOM) durante o Workshop TerraLAB 2020.1
« ‹
1 de 9
› »

O Processo Trainee recebeu ao todo 39 inscrições de graduandos e pós-graduandos em Ciência da Computação, Engenharia de Controle e Automação, Engenharia Mecânica, Engenharia de Minas, Arquitetura e Urbanismo, Direito, Engenharia de Materiais, Jornalismo e Estatística até sexta-feira, dia 13 de março. O TerraLAB iniciou o processo seletivo na segunda-feira seguinte ao workshop. Mesmo com a ameaça do Coronavírus, o grupo se manteve ativo via home office, sendo dividido nas seguintes equipes: Análise de Negócios, Análise de Geoprocessamento, Jurídico, Infraestrutura, Design e Marketing, Gerência de Projeto, Engenharia de Software Backend, Engenharia de Software Frontend WEB e Mobile; e Engenharia de Testes WEB e Mobile. Cada uma dessas equipes ou capítulos vem recebendo a mentoria de profissionais reconhecidos pelo mercado e liderados por integrantes mais experientes do laboratório.


Objetivando a troca de conhecimentos e experiências adquiridas tanto na trajetória acadêmica e profissional de cada um, como também por meio de cursos online oferecidos pelas empresas parceiras do TerraLAB, os inscritos são convidados a cumprir desafios semanais que os levará ao desenvolvimento de seis projetos de software que resultarão em aplicativos WEB e Mobile inovadores.


Inicialmente, o processo trainee teria a duração de três semanas. Com a atual situação, a data para seu término permanece em aberto. Toda atualização será postada aqui no blog. Por isso, continue acompanhando este blog e nossas redes sociais!"
Workshop TerraLAB 2020.1,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/03/WhatsApp-Image-2020-03-07-at-5.52.53-PM-2.jpeg,"No dia 13 de março de 2020, sexta feira, o TerraLAB irá realizar um Workshop com várias palestras voltadas para o desenvolvimento de software e como melhorar suas habilidades.

O Workshop é também a primeira atividade obrigatória para os inscritos no Processo Seletivo Trainee 2020/1.

Confira nossa programação:

8h – Apresentação Programa Trainee (Professor Tiago Carneiro)
Nesta palestra, o professor Tiago Carneiro apresentará o formato e cronograma do processo seletivo do TerraLAB. Ele também apresentará as empresas parceiras desta iniciativa e falará sobre as perspectivas futuras da construcão de um “Programa de Residência em Software” e um “Programa de Mestrado Profissional em Computação Aplicada” no DECOM .

9h – Processos ágeis, gestão de pessoas, pipeline de produtos (Stilingue)
Nesta palestra a Stilingue nos apresenta uma filosofia que acaba por incentivar o maior trabalho em equipe, a auto-organização, a comunicação frequente, o foco no cliente e a entrega de valor.

10h – Noções básicas de desenvolvimento de aplicativos Android (Usemobile)
Nessa palestra será apresentado pela Usemobile o conceito básico de desenvolvimento mobile e as principais tecnologias que envolvem a construção de uma aplicação Android.

11h – Noções básicas de uma aplicação web utilizando AngularJS e Node (CRUD Soluções)
Nessa palestra serão apresentadas pela CRUD Soluções duas das principais tecnologias para o desenvolvimento de serviços WEB: AngularJS para front-end e Node para back-end.

13h – Arquitetura de Software na Gerencianet (Gerencianet)
Nesta Palestra, Fracisco (Gerencianet) falará sobre motivações e benefícios dos modelos arquiteturais de software utilizados na Gerencianet.

14h – Garantindo a qualidade do seu produto/serviço com testes automatizados (iFood)
Nesta palestra de Igor Resende (iFood) você irá ouvir sobre conceitos básicos a arte na automação de testes para garantir a qualidade de aplicações front e backend

15h – Vivências na indústria de software e sua carreira (Dr. Igor Prata)
Informação: de Gargamel a surfista <Shift F6>. Calma! Eu sei que não parece, mas é isso mesmo. Sabe esse espanto/desconforto que você sentiu? Pois é, a palestra é sobre ele 🙂

16h – O que consigo fazer no Kubernetes? (Stilingue)
Nesta palestra a Stilingue irá falar sobre o uso da tecnologia Kubernetes na orquestração de containeres na Google Cloud: Deploy Automatizado de Aplicações, Gestão e Escalabiliade.

17h – Transformação digital na Indústria 4.0 (Vallourec)
Nesta palestra, o engenheiro Júlio César e time irão compartilhar as experiências da equipe no desenvolvimento da estratégia de implantação de soluções inteligentes para otimização de processos industriais e no desenvolvimento de produtos digitais da VSB.

Venha prestigiar conosco esse evento!"
Aberto o Processo Trainee TerraLAB 2020/1,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/03/POSTER-ATUALIZADO.png," 

Inscrições de 02 a 13 de março de 2020

Aprenda as tecnologias mais pedidas no mercado e desenvolva suas habilidades trabalhando com profissionais experientes e qualificados. Você desenvolverá aplicativos mobiles e web, ambos integrados a serviços na nuvem! Dentre outras tecnologias, você vai ter contato com React, React Native, Node JS, Jest, Cucumber e Selenium, Apium, AWS e mais.

Graduandos e pós-graduandos da UFOP: Aumente as suas chances de conseguir uma vaga de emprego e domine as tecnologias utilizadas nas indústrias de software.

Nosso objetivo é aumentar sua empregabilidade e, ao mesmo tempo, reduzir o custo e tempo da seleção e treinamento de recursos humanos por parte da indústria.

Os estudantes serão capacitados em um ambiente profissionalizante, similar ao de uma fábrica de software, onde o estudante pode vivenciar os papéis existentes no ecossistema dessa indústria.

Nominalmente, os estudantes podem vivenciar os seguintes papeis: Analista de Negócio, Gerente de Projeto, Analista de Sistema, Engenheiro de Teste, Engenheiro de Software, Designer Gráfico, Analista de Geoprocessamento, Analista de TI, Análise de marketing, Analista contábil e Analista jurídico.

Coordenaçao: O laboratório TerraLAB é coordenada pelo professor Dr. Tiago Garcia de Senna Carneiro, do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP).

Mentoria: Os estudantes selecionados receberão mentoria de profissionais experientes que atuam em empresas parceiras. Estas empresas, de antemão, expressaram interesse em contratar, em médio prazo, os estudantes que apresentarem bom desempenho na realização das atividades propostas nesta inciativa.

Processo seletivo: Após o período de inscrição, no dia 13 de março, sexta, faremos um workshop onde as empresas parceiras apresentarão seus cases e ambientes de desenvolvimento aos estudantes inscritos. Depois, os estudantes terão aproximadamente 3 semanas para realizarem auto-treinamento por meio dos cursos online oferecidos pelas empresas parceira sobre as atuais tecnologias por elas utilizadas para o desenvolvimento de frontends (web e mobile), backends (em nuvem) e testes (web, mobilie e backend). Durante esse tempo, serão propostos desafios para os estudantes, que devem utilizar os conhecimentos adquiridos para resolvê-los. Um deles será o desenvolvimento de um aplicativo, utilizando-se das tecnologias descritas acima.

Pré-requisitos: É essencial que os candidatos sejam pessoas automotivadas e comprometidas com o sucesso dos projetos com os quais se envolvem. Apesar dos projetos pedirem demandas técnicas específicas, os candidatos receberão treinamento e mentoria para desenvolvimento de suas tarefas. No ambiente de fábrica de software, é clara a necessidade de profissionais que atuam para apoiar o processo de produção de software e que não são programadores, este é o caso do Analista de TI que mantém a rede de comunicação e servidores linux em funcionamento, além do Designer Graficos que cuida das peças (imagens, ícones, telas e textos) de comunicação associadas aos produtos e o Gerente de Projetos que tem como incumbência administrar os resultados, prazos, custos e recursos dos projetos. Então, se você ainda não é um programador no paradigma orientado por objetos, pense em nos ajudar desempenhando um desses papéis.

Inscrição: De 02 a 13 de março de 2020. Os interessados devem enviar currículo LATTES e fornecer todas as informações solicitadas no seguinte formulário .

Bolsa: Neste semestre, os estudantes deverão se voluntariar e receberão como contrapartida a mentoria e o treinamento nas tecnologias supracitadas. Após esse período de tempo, os estudantes com bom desempenho poderão receber bolsas para a continuidade dos projetos que iniciaram neste semestre.

Carga Horária: 20h/semanais

Cooperação voluntária: Todos os estudantes inscritos no processo seletivo estão desde já convidados a ajudar na preparação do espaço físico, de hardware e de software do TerraLAB.

Número de vagas : Não há um número máximo de vagas em aberto, todos os estudantes que tiverem bom desempenho no processo seletivo serão convidados a tomar parte nesta iniciativa. No passado, o laboratório TerraLAB já teve 36 colaboradores atuando simultaneamente em diversos projetos.

As inscrições para o processo seletivo podem ser feitas do dia 2 ao dia 13 de março de 2020.

A proposta está aberta aos estudantes de todos os cursos de graduação e pós-graduação da UFOP.

A primeira atividade obrigatória do processo seletivo é a presença durante todo o Workshop TerraLAB que irá ocorrer no dia 13 de março de 2020, sexta feira. 

Os horários e temas do evento podem ser encontrados no fim dessa postagem.

Ajude-nos a divulgar essa iniciativa!

 "
O que as Empresas Parceiras Pensam sobre a Atuação do TerraLAB em 2019,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/01/ae5c77_772dd6465cdc40b98dfa441b3d3e6346_mv2.jpeg,"Em razão da necessidade de simular um ambiente empresarial e familiarizar os alunos do curso de Ciência da Computação com as principais tecnologias exigidas pelo mercado de tecnologia, o TerraLAB começa o ano de 2020 estruturando o processo seletivo do seu  Programa de Trainee 2020/1 e  apresentando o feedback das empresas parceiras que ofereceram mentoria aos estudantes que fizeram parte de todo o processo de desenvolvimento de software proposto para no Programa de Trainees 2019/2.

 

Julio Cesar Ferrerira – Gerente de Datascience da Vallourec
Pedro Gomide – Engenheiro de Soluções Digitais da Vallourec

“A percepção dos nossos times de Datascience e de Soluções Digitais para a Industria 4.0 é de que os trainees do TerraLAB entenderam muito claramente o timming e o pragmatismo com os quais os trabalhos são desenvolvidos nas empresas.

Nós acreditamos que o Programa de Trainee do TerraLAB irá gerar excelentes resultados sobre a formação dos alunos.

A interação entre trainees e profissionais vem acontecendo de forma muito espontânea e os trainees vem assimilando muitos bem as questões tratadas nestas interações, devido à leveza na qual essas questões vêm sendo tratadas, apesar no alto nível técnico das exigências.

O ambiente do TerraLAB é certamente um ambiente de cooperação.

Assim, a perspectiva é de que os trainees se graduem com um entendimento muito aprimorado de como as soluções devem ser desenvolvidas nas empresas.

Por essa razões, eles irão começar a vida profissional vários passos à frente dos estudantes que não passam por uma experiência como essa.”

————

Conrado Carneiro,  CEO na Usemobile – Ex-aluno do DECOM e do TerraLAB

“Projeto como esse liderado pelo TerraLab é fundamental para fomentar dentro da universidade um ambiente empresarial, proporcionando aos alunos experiências de como desenvolver produtos.Senti falta dessa oportunidade durante meu período de estudo na graduação. A maioria das iniciativas eram voltadas para pesquisas e muito distante do mercado de trabalho.

Durante a apresentação dos resultados fiquei surpreso ao perceber que os produtos desenvolvidos (GeoSpotted e Batcaverna) em menos de 3 meses, utilizaram tecnologias e processos que gastamos mais de 1 ano para criar na Usemobile. Desde a qualidade de software e testes até a aparência e a experiência proporcionada aos usuários. Um produto em sua primeira versão, digno de lançamento ao mercado.

Acredito que em breve colheremos bons frutos preparando essa turma para atender as exigências do mercado de trabalho.

Parabéns contem com a Usemobile para dar continuidade nesse projeto.”

————-

 

Filipe Mata,  Analista de Sistemas na Gerencianet Pagamentos do Brasil 

“Achei bem interessante pois todos os alunos demonstraram imensa gratidão ao Tiago e ao laboratório, uma vez que todos afirmaram terem ganhado conhecimento e experiência que jamais teriam ganhado somente no curso de Ciência da Computação.

Por menor que fosse o tempo da apresentação, pude perceber o como os alunos cresceram profissionalmente e tecnicamente falando.

Fiquei muito feliz ao ver que estes alunos tiveram a oportunidade de aprender um acervo de tecnologias demandadas pelo mercado, e que se empenharam em desenvolver algo com o que aprenderam. Segundo o próprio Tiago, foram 6 semanas de trabalho duro e aprendizado.

Eu particularmente me impressionei com a qualidade do produto que eles fizeram em tão pouco tempo e usando tantas tecnologias que usamos aqui e que outras empresas, como a Usemobile, utilizam.

Mais do que capacidade técnica, pude enxergar nos alunos um forte espirito de equipe, já que durante o processo cerca de 17 alunos desistiram do projeto, mas aqueles que restaram realmente souberam interagir entre si, trabalhar em equipe e lidar com as pressões de atender a um prazo de entrega.

O que me deixou mais abismado é que os alunos mantiveram a empolgação mesmo diante das adversidades.”

————

 

Itálo Milagres – Engenheiro de Software na Cachaça Gestor – Ex-aluno da UFOP e do TerraLAB

“O programa de Trainee que acontece no TerraLab, idealizado pelo professor Tiago, é uma “ferramenta” importante na formação dos alunos graduandos em Computação, visto que o mesmo abrange áreas que o currículo acadêmico não alcança.

Sendo graduado também pela UFOP no ano de 2015, eu e vários colegas, sentimos, e pelo o que eu pude observar os alunos atuais sentem também, um certo medo quando nós chegamos perto de concluir o curso por não nos acharmos preparados para o mercado de trabalho, visto que tivemos muito pouco – ou nenhum – contato com a maioria dos requisitos solicitados em vagas de empregos da nossa área, durante a nossa graduação. Por esse “medo” eu entrei no mestrado de para-quedas e fiz um período até eu abandonar e estudar por conta própria as tecnologias do mercado.

No pouco tempo de estruturação e execução da primeira turma do projeto de trainee já tivemos resultados satisfatórios tanto para os mentores quanto para os alunos. Ao fim de três semanas de projeto os alunos conseguiram desenvolver aplicações tanto web quanto android, com a implementação de testes, gerência de projetos, cooperação entre as equipes e muito mais. Coisa que o mercado necessita e muito!

Com certeza os alunos que participarem e concluírem este treinamento sairão muito mais preparados para o mercado do que aqueles que apenas frequentaram as aulas e tiraram notas boas nas disciplinas.

Vejo que tal projeto, além de preparar melhor os alunos para o mercado, também conseguirá diminuir a evasão escolar, que é tão grande no nosso curso, pois 1) os alunos irão trabalhar efetivamente em um projeto real, 2) irão aprender tecnologias que são amplamente utilizadas pelas grandes empresas, 3) a sinergia entre alunos e empresas reais irá de fato acontecer, e com isso os mesmos ficarão mais motivados para continuar no curso e na carreira.

Quem dera se eu pudesse ter passado por algo similar durante a minha graduação.

Hoje fico muito feliz em poder ajudar os novos alunos a verem um outro mundo da computação dentro da Universidade.”

————

 

Novamente, o nosso muito obrigado às empresas parceiras ! 

Siga nossa redes sociais para mais informações de como abaixar a versão Beta do Geospotted e ficar por dentro das novidades do processo seletivo Trainee TerraLAB 2020/1."
Visualização de modelos espaciais no Repast,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/10/googleSIGHabitar.png,"Este é o primeiro blog de uma série que abordará a visualização de modelos espaciais em algumas das principais ferramentas de modelagem. Ele será dividido em duas partes, iniciamos dando uma visão geral da plataforma de modelagem e a finalizamos com um pequeno tutorial sobre o desenvolvimento de modelos espaciais.

Repast

O simulador Repast foi criado no laboratório de pesquisa computacional em Ciências Sociais da Universidade de Chicago e atualmente é gerenciado pelo Repast Organisation for Architecture and Development (ROAD) [Crooks, 2007]. Ele é uma plataforma de modelagem baseada em agentes (ABM) desenvolvida em Java, de código livre e atualmente esta na versão Simphony 2.0[1]. O nome é um acrônimo para Recursive Porous Agent Simulator Toolkit.

O Repast foi concebido como uma biblioteca Java para trabalhar em conjunto com o simulador Swarm[2] e facilitar seu uso. No entanto, devido ao potencial vislumbrado pelos pesquisadores tornou-se um projeto independente [Collier, 2002].

Tutorial – Criando um modelo em Repast

Nesse tutorial, criaremos um modelo de drenagem, onde a chuva ocorre nas áreas mais altas (altitudes superiores a 200 metros) e escorre para as áreas mais baixas. No resultado final, as linhas de drenagem emergem e mostram o percurso feito pela água, como apresentado na Figura 1. Como ocorre com qualquer ambiente de modelagem é importante que o usuário tenha algum conhecimento de lógica de programação. No Repast, também é necessário um conhecimento básico da linguagem Java e por isso não entraremos em muitos detalhes acerca do uso desta linguagem e sua interface de programação.

Figura 1- Resultado final do modelo de drenagem

 

O simulador pode ser encontrado na página http://repast.sourceforge.net/download.html. Nesse tutorial utilizaremos a versão Simphony 2.0 para Window 7. Informações sobre a instalação em outras plataformas estão disponíveis no endereço acima.

Após o download e a instalação, abra o Repast Simphony. Essa versão do simulador esta integrada ao ambiente Eclipse 3.7.1.

Modelos em Repast podem ser criados utilizando fluxogramas, linguagem Groovy e linguagem Java. Fluxogramas facilitam o desenvolvimento de modelos por meio do uso de componentes gráficos que representam passos da simulação (Figura 2). A linguagem Groovy[3] é uma linguagem de programação ágil e dinâmica para a plataforma Java e inspirada em outras linguagens como Python e Ruby (Figura 3). E, a linguagem Java foco desse tutorial.

Figura 2 – Modelagem via fluxogramas (Fonte: FlowZombies disponível junto com o simulador)

Figura 3 – Código de inicialização do modelo na linguagem Groovy (Fonte: FlowZombies disponível junto com o simulador)

 

Para iniciar, vamos criar um projeto Repast Simphony:

Clique no menu File > New > Other…

Surgirá uma janela como apresentado na Figura 4.

Selecione a opção “Repast Simphony Project” e depois clique no botão Next.

Na próxima tela, dê o nome “drenagem” ao projeto e depois clique em Finish.

 

 

Figura 4 – Assistente de criação de projetos do Repast

 

O ambiente vem, por padrão, com a perspectiva ReLogo ativada, para alterar para a Java basta clicar no botão “Java” na parte superior direita da aplicação (Figura 5). Para projeto em Java, é necessário ativar o autobuilding que por padrão vem desativado. Para isso, vá ao menu Project e clique em Build Automatically.

Figura 5 – Perspectiva Java ativada

 

Neste momento, o Repast deverá apresentar uma tela parecida com a Figura 6. Em destaque nessa figura são arquivos gerados automaticamente pela plataforma e não serão usados. Logo, podem ser apagados.

Figura 6 – Estado do Repast Simphony após a criação do projeto

 

Após apagar os arquivos “ModelInitializer.groovy” e ModelInitializer.agent”, iremos criar a classe que será responsável pelo ambiente de execução do modelo. Então, clique no menu File > New > Class, será exibido uma tela similar à Figura 6. Insira o nome “Drenagem” para classe e clique no botão Finish.

Figura 7 – Assistente de criação de classes

 

Nesse momento a classe “Drenagem” já esta criada mas ela ainda é somente uma simples classe Java.

A estrutura padrão de um modelo no Repast é baseada em dois objetos Context e Projection. Context é um container de objetos e representa uma população abstrata de um modelo porém, sem o conceito de espaço ou de relações. Internamente, ele mantém informações sobre o ambiente em que os objetos interagem, não estando diretamente relacionado a um espaço físico [Allan, 2010] [Howe et al 2006]. Projections são estruturas que definem algum tipo de relação entre os agentes pertencentes a um Context possibilitando a interação entre eles [Allan, 2010]. Um objeto Context pode relacionar com diversos objetos Projection porém, um Projection relaciona apenas com um objeto Context.

Para que possamos utilizá-la no Repast para manter o ambiente de simulação, é necessário que ela implemente a interface Java Context e sobrecarregue o método build(). Para isso, altere o código da classe para este apresentado no Código 1. Nós também declaramos algumas variáveis e constantes que serão usadas durante a simulação.

package drenagem;

 

import java.util.ArrayList;

import repast.simphony.context.Context;

import repast.simphony.dataLoader.ContextBuilder;

 

publicclass Drenagem implements ContextBuilder<Object>

{

         // Constantes

        final static int TAMANHO = 100;        // tamanho do espaço (área modelada)

        final static double INFILTRACAO = 0.0;         // coeficiente de infiltração

        final static double CHUVA_POR_TEMPO = 10.0;    // quantidade de chuva por tempo

        final static double ALTITUDE_MIN_CHUVA = 200.0; // altitude mínima onde ocorrerá chuva

        // Variáveis

        private double[] arquivoAltimetria;

        public static ArrayList<Object> celulas;

       

        // Método de construção do contexto da simulação

        public Context<Object> build(Context<Object> context)

        {

                // Define uma identificação para o contexto

                context.setId(“drenagem”);

                return context;

        }

}

Código 1 – Classe Drenagem com a implementação da interface Context, declaração de algumas constantes e variáveis

Neste ponto, temos nossa classe “Drenagem” interagindo com o Repast e mantendo o ambiente de simulação.

Leitura do arquivo de altimetria

Modelaremos uma área de 100 m² que será fragmentada em células que ocupam áreas de 1 m² e possuem altitude definida pelo arquivo “altimetria.csv“. Iniciaremos carregando este arquivo para a memória e em seguida, associaremos o valor encontrado em cada linha do arquivo a uma célula.

Para codificar o método de leitura, faça download do arquivo (altimetria.csv) e salve no diretório definido como workspace do Repast Simphony (Diretório padrão é “C:\RepastSimphony-2.0\workspace”). Copie o método leArquivoAltimetria() do Código 2 e insira após o método build().

  // Método de leitura do arquivo de altimetria

  private void leArquivoAltimetria(String nome) throws IOException

  {

        arquivoAltimetria = newdouble[TAMANHO * TAMANHO];

        try

        {

                // Abre o arquivo altimetria.csv

                BufferedReader in = new BufferedReader(new FileReader(nome));

                int contaLinha = 0;

                String linha;

                // Lê cada linha e armazena o valor no array arquivoAltimetria

                while ((linha = in.readLine()) != null )

                {

                        arquivoAltimetria[contaLinha] = Double.parseDouble(linha);

                        contaLinha++;

                }

        }

        catch (FileNotFoundException e)

        {

                e.printStackTrace();

        }

}

Código 2- Método leArquivoAltimetria. Iniciamos o array arquivoAltimetria para armazenar o conteúdo do arquivo de altimetria

Criando o agente Célula

Como já comentado, o espaço é fragmentado em células e cada célula ocupará uma posição no espaço.

Dando continuidade ao tutorial, agora criaremos o objeto “Celula”. Para começar, vá ao menu File > New > Class… Na tela que surge, no campo Name, digite “Celula” e clique em Finish.

Copie o Código 3 e sobrescreva o aquele gerado pela automaticamente pela plataforma.

package drenagem;

 

import java.util.List;

import repast.simphony.query.space.grid.GridCell;

import repast.simphony.query.space.grid.GridCellNgh;

import repast.simphony.space.grid.GridPoint;

publicclass Celula {

        // Variáveis

        public double altitude, qtdAgua, fluxo;

        public int x, y;

 

        // Construtor

        public Celula(int x, int y, double altitude){

                this.x = x;

                this.y = y;

                this.altitude = altitude;

                qtdAgua = 0.0;

                fluxo = 0.0;

        }

 

        // Método que será executado pelo escalonador

        publicvoid execute() {

                // Recupera a localização no espaço

                GridPoint pt = Drenagem.grid.getLocation(this);

                // Cria uma grade de celulas vizinhas

                GridCellNgh<Celula> gradeViz = new  GridCellNgh<Celula>(Drenagem.grid, pt, Celula.class, 1, 1);

               

                // Cria uma lista de grade de celulas vizinhas

                List<GridCell<Celula> > resultado = gradeViz.getNeighborhood(true);

                int contVizinhos = 0;

                // Calcula a quantidade de água

                qtdAgua = qtdAgua – Drenagem.INFILTRACAO * qtdAgua;

 

                // Conta o número de vizinhos mais baixos

                for (GridCell<Celula> gradeCel : resultado){

                        Iterable<Celula> iteradorItens = gradeCel.items();

                        for (Celula vizinho : iteradorItens){

                               if ( (this != vizinho) && ( altitude >= vizinho.altitude) )

                                       contVizinhos++;

                        }

                }

                if (contVizinhos > 0){

                        // Divide a quantidade de agua igualmente entre os vizinhos

                        double fluxo = qtdAgua / contVizinhos;

                        // Envia agua para os vizinhos mais baixos

                        for (GridCell<Celula> gradeCel : resultado){

                               Iterable<Celula> iteradorItens = gradeCel.items();

                               for (Celula vizinho : iteradorItens){

                                       if ( (this != vizinho)

                                          && ( altitude >= vizinho.altitude) ){

                                               vizinho.fluxo = vizinho.fluxo + fluxo;

                                       }

                               }

                        }

                }

        }

}

Código 3 – Código completo do objeto Celula

 

Criando o agente Chuva

Esse agente simulará a chuva que apenas ocorre nas células mais altas.

Para continuarmos, crie uma nova classe chamada “Chuva” e sobrescreva o conteúdo pelo apresentado no Código 4.

package drenagem;

 

publicclass Chuva {

 

        public Chuva()

        {

        }

       

        // Método chuva

        publicvoid execute()

        {

                // Percorre todas as células

                for(Celula c : Drenagem.celulas)

                {                      

                        // A chuva ocorre somente nas células com

                        // altitude superior a 200 metros

                        if (c.altitude > Drenagem.ALTITUDE_MIN_CHUVA)

                               c.qtdAgua = c.qtdAgua + Drenagem.CHUVA_POR_TEMPO;

                }

        }

}

Código 4 – Código do objeto “Chuva”

Criando o agente Fluxo

O agente fluxo permitirá a transferência entre a quantidade de água das células mais altas para aquelas mais baixas.

Crie uma nova classe com o nome “Fluxo” e sobrescreva o conteúdo pelo Código 5.

package drenagem;

 

public class Fluxo {

        public Fluxo()

        {              

        }                                              

        // Método fluxo superficial

        public void execute()

        {

                // Percorre todas as células e a quantidade de água

                // é, a cada execução, sobrescrita pela água recebida

                // de células mais altas

                for (Celula c : Drenagem.celulas)

                {

                        c.qtdAgua = c.fluxo;

                        c.fluxo = 0.0;

                }

        }

}

Código 5 – Código do objeto “Fluxo”

Agora, voltemos para o código do objeto “Drenagem” para descrevermos o código de criação dos objetos “Celula”. Então, vá ao Package Explorer e dê um duplo clique nele, como apresentado na Figura 1 e insira o Código 6 no método build().

Figura 8 – Localização do Package Explorer e o arquivo Drenagem.java

 

// Método de construção do contexto da simulação

public Context<Object> build(Context<Object> context)

{

        // Define uma identificação para o contexto

        context.setId(“drenagem”);

 

        // Inicializa o container de celulas

        celulas = new ArrayList<Celula>();

       

        // Adiciona a chuva ao contexto da simulação

        context.add(new Chuva());

 

        // Le o arquivo de altimetrias

        try {

                leArquivoAltimetria(

                        “C:/RepastSimphony2.0/workspace/altimetria.csv”);

                }

        catch (IOException e) {

                e.printStackTrace();

        }

               

        // Laço de repetição que cria as células e define

        // sua altitude de acordo com o valor definido no

        // arquivo e armazenado no array arquivoAltimetria

        int cont = 0;

        for (int i = 0; i < TAMANHO; i++)

        {

                for (int j = 0; j < TAMANHO; j++)

                {

                        // Cria um objeto celula na coordernada (i, j)

                        Celula c = new Celula(i, j, arquivoAltimetria[cont]);

                                  

                        // Adiciona ao container de celulas

                        celulas.add(c);

 

                        // Adiciona a celula no contexto da simulação

                        context.add(c);

                        cont++;

                }

        }

 

        // Adiciona no contexto da simulação

        context.add(new Fluxo());             

       

        return context;

}

Código 6 – Código de leitura da altimetria, criação dos objetos “Celula” e adição ao contexto e ao container de células. Adição do objetos “Chuva” de “Fluxo” ao contexto.

Definindo a projeção espacial

As relações são definidas por projeções. O Repast possibilita a descrição de modelos que usam várias projeções espaciais: projeção espacial contínua é um espaço onde os agentes presentes nele possuem localização representada por coordenadas de ponto flutuante; projeção discreta é um espaço representado por coordenadas inteiras em que os agentes são dispostos em uma grade; projeção geográfica são projeções em que os agentes estão associados a uma geometria espacial (por exemplo: polígono e ponto); projeção em rede permite representar relações abstratas entre agentes. Sendo assim, para criarmos uma relação espacial entre as células do espaço, definiremos uma projeção espacial discreta através do objeto “Grid” do pacote “repast.simphony.space.grid”. Para isso, vá ao método build() do objeto “Drenagem” e insira as linhas apresentadas no Código 7. O método createGrid() não possui valores padrões logo, é necessário passar alguns parâmetros.

        // Cria uma fábrica de construção de objetos grid

        GridFactory gridFactory = GridFactoryFinder.createGridFactory(null);

        // Especifica propriedades para um objeto grid

        GridBuilderParameters<Object> gridBuildParam = new GridBuilderParameters<Object>(

                        new WrapAroundBorders(), new SimpleGridAdder<Object>(),

                        true, TAMANHO, TAMANHO);

        // Inicializa o objeto Grid

        grid = gridFactory.createGrid(“grid”, context, gridBuildParam);

Código 7 – Inicialização do objeto Grid

Código 6 contém a lista de classes de arquivos de importação necessários para que as definições dos objetos sejam encontradas.

import repast.simphony.space.grid.GridBuilderParameters;

import repast.simphony.space.grid.SimpleGridAdder;

import repast.simphony.space.grid.WrapAroundBorders;

import repast.simphony.context.space.grid.GridFactory;

import repast.simphony.context.space.grid.GridFactoryFinder;

Código 8 – Lista de arquivos de importação

O objeto “Grid” como apresentado no Código 7 irá acarretar em erro pois ele ainda não foi declarado. Sendo assim, insira a linha em destaque no Código 9 (inicio do objeto “Drenagem”) a declaração desse objeto.

        // Variáveis

        private double[] arquivoAltimetria;

        public staticArrayList<Celula> celulas;

        public static Grid<Object> grid;

Código 9 – Declaração do objeto “Grid”

Ainda no método build(), é necessário inserir o objeto “Celula” na projeção então, insira a linha em negrito no Código 10 imediatamente após a linha “contex.add(c)” dentro do laço de repetição.

 

// Laço de repetição que cria as células e define

// sua altitude de acordo com o valor definido no

// arquivo e armazenado no array arquivoAltimetria

int cont = 0;

for (int i = 0; i < TAMANHO; i++)

{

        for (int j = 0; j < TAMANHO; j++)

        {

                // Cria um objeto celula na coordernada (i, j)

                Celula c = new Celula(i, j, arquivoAltimetria[cont]);

       

                // Adiciona a celula no contexto da simulação

                context.add(c);

               

                // Insere o objeto c na coordenada (i, -j)

                // O valor de negativo para j é necessário para o posicionamento

                // correto das células no espaço pois o Repast considera o canto

                // inferior esquerdo como ponto (0, 0)

                grid.moveTo(c, i, -j);

                cont++;

        }

}

Código 10 – Adição da célula na projeção espacial

O Repast utiliza arquivos XML para a criação de interface gráfica do usuário (GUI) e de outros componentes em tempo de execução. Para que possamos rodar nosso modelo, é necessário adicionar qual o tipo e o nome da projeção que será usada na simulação. Para tanto, localize o arquivo “context.xml” (Figura 9), insira a linha em negrito no Código 11 e salve o arquivo.

Figura 9 – Arquivo context.xml

 

<context id=“drenagem”

        xmlns:xsi=“http://www.w3.org/2001/XMLSchema-instance”


        xsi:noNamespaceSchemaLocation=“http://repast.org/scenario/context”>

        <projection type=“grid” id=“grid”></projection>

</context>

Código 11 – Definição do tipo e do nome da projeção utilizada

 

Configurando a GUI do modelo de drenagem

Agora, execute o modelo. Vá à barra de ferramentas e clique no botão Run As… no menu que se abre, clique em drenagem Model (Figura 10).

Figura 10 – Botão “Run As…” e a opção de executar o modelo drenagem

 

Irá surgir uma tela similar a esta apresenta na Figura 11. Em azul, destacamos a barra de ferramentas do ambiente de simulação e em vermelho, na listagem à esquerda tem a opção “Data Loaders”. Clicando sobre ela com o botão direito do mouse será exibido um menu de contexto, clique em Set Data Loaders, em seguida abrirá o assistente de configuração, selecione a opção “Custom ContextBuilder Implementation” e em seguida clique em Next. Na tela seguinte, aparecerá na caixa de combinação o nome “drenagem.Drenagem” selecionado, clique em Next e depois em Finish.

Para efetivar as alterações, clique no botão Save Model na barra de ferramentas no ambiente de simulação.

A criação da interface gráfica é feita clicando na opção “Displays”, logo abaixo de “Data Loaders”, com o botão direito e depois em Add Display abrirá o assistente apresentando em Figura 12. No assistente, clique em “grid”, depois na seta para a direita e em seguida, em Next.

Figura 11 – Ambiente de simulação do Repast Simphony

 

Figura 12 – Assistente de configuração do Display – Seleção da projeção

Figura 13 – Assistente de configuração do Display – Seleção do agente

 

A tela do assistente será similar à apresentada em Figura 13. Selecione “Celula” na lista à esquerda , clique na seta para a direita e depois, em Next. Nas próximas telas, manteremos as configurações pré-definidas. Então, clique em Next até que o botão Finish fique ativo e clique nele. Após o assistente fechar, salve o modelo.

Na barra de ferramenta do ambiente, clique no botão Initialize Run. Deverá aparecer uma tela similar à Figura 14. Clicando no botão Start, o tempo de simulação (em destaque na Figura 14) ficará negativo pois não há nada escalonado.

Figura 14 – Visualização do estado inicial do modelo

 

Escalonamento de métodos

O tempo pode ser usado de forma síncrona, em que se considera que todos os agentes da simulação mudam simultaneamente, e de forma assíncrona, padrão utilizado pela plataforma, em que os agentes mudam considerando a realidade deixada pelo agente anterior [Crooks, 2007]. Classificado como um simulador de eventos discretos, o Repast organiza a execução dos eventos por meio de um escalonador. De maneira geral, o escalonador representa o tempo de simulação. O escalonamento de alguma ação pode ser feito de três formas: Annotations são comentários especiais inseridos na descrição do modelo e usados para escalonar estaticamente algum evento; Watchers são anotações especiais usadas para escalonar dinamicamente algum evento. Esse escalonamento requer maiores conhecimentos sobre o domínio do problema em estudo pois não ocorrer em períodos pré-estabelecidos; O escalonamento direto é feito por meio do objeto Schedule do pacote “repast.simphony.engine.schedule” que através do método schedule() organiza a execução de uma ação de agente.

A simulação somente ocorre se o modelador definir quais serão os passos que o ambiente de simulação deverá fazer e portanto, nesta parte do tutorial iremos

definir quais serão esses passos que o Repast deverá seguir para que a simulação ocorra.

Começaremos pelo objeto “Celula”, no Package Explorer dê um duplo clique sobre o nome “celula.java”, encontre o método execute() e insira a linha em negrito (anotação) antes da declaração do método conforme apresentado no Código 12. Essa anotação informa ao Repast que esse método será escalonado iniciando no tempo start, a cada intervalo interval e com prioridade priority.

// Método que será executado pelo escalonador

@ScheduledMethod(start = 1, interval = 1, priority = 1)

public void execute()

{

        …

}

Código 12 – Escalonamento do método execute()

Como feito para o objeto “Celula”, faremos de maneira igual para os objetos “Chuva” e “Fluxo”. Portanto, insira a mesma anotação para esses dois objetos.

Neste momento, se executarmos a simulação o resultado apresentado pela GUI será idêntico àquele mostrado pela Figura 14. Para que a GUI apresente corretamente as linhas de drenagem, é necessário informar ao Repast como os valores de quantidade de água deverão ser pintado.

Criando estilos para os agentes

Um estilo define algumas característica visuais que permitirão ao Repast pintar corretamente um agente. Essas características podem ser forma geométrica, cor, tamanho, etc. Em nosso modelo, iremos definir a cor, o formato e o tamanho de uma célula.

Para começar, crie uma nova classe como o nome “EstiloAguaCelula”. Copie o Código 13 e sobrescreva o conteúdo do arquivo.

package drenagem;

 

import drenagem.Celula;

import java.awt.Color;

import repast.simphony.visualizationOGL2D.DefaultStyleOGL2D;

import saf.v3d.ShapeFactory2D;

import saf.v3d.scene.VSpatial;

public class EstiloAguaCelula extends DefaultStyleOGL2D

{

        private ShapeFactory2D shapeFactory;

        private SimpleColorMap simpleColorMap;

 

        // Mapas de tons de azul

        privatefinalstatic Color[] colorMap = new Color[]

        {

                new Color(255, 255, 255, 255),

                new Color(170, 255, 255, 255),

                new Color(  0, 170, 255, 255),

                new Color(  0,85, 255, 255),

                new Color(  0,0, 255, 255),

                new Color(  0,0, 127, 255)

        };

 

        @Override

        publicvoid init(ShapeFactory2D factory)

        {

                this.shapeFactory = factory;

                simpleColorMap = new SimpleColorMap(

                               colorMap,

                               0,              // minLevel

                               305,            // maxLevel

                               new Color(255, 255, 255, 255), // minColor

                               new Color(  0,  0, 127, 255));// maxColor

        }

         @Override

        public Color getColor(Object agent)

        {

                if (agent instanceof Celula)

                {

                        Celula c = (Celula) agent;

                        // Mapea o valor da quantidade de água em uma cor

                        Color color = simpleColorMap.getColor(c.qtdAgua * Drenagem.VAL2COR);

                        return color;

                }

                returnnew Color(255, 255, 255, 255);

        }

        @Override

        public VSpatial getVSpatial(Object agent, VSpatial spatial) {

                if (spatial == null) {

                        spatial = shapeFactory.createRectangle(15, 15);

                }

                return spatial;

        }

}

Código 13 – Estilo que será usado para pintar um objeto “Celula”

O Repast não possui nenhuma forma pré-definida para mapear valores em cores. Por isso, utilizando outro simulador, de código livre, chamado Mason (http://cs.gmu.edu/~eclab/projects/mason/), será o foco de um próximo post, encontramos um conjunto de classes que permitirá fazer facilmente esse mapeamento. Essas classes podem ser baixadas aqui e devem ser salvas na mesma pasta onde os arquivos do modelo estão (Diretório padrão: C:\RepastSimphony-2.0\workspace\drenagem\src\drenagem).

Após criarmos o estilo, devermos informar ao Repast que este estilo dever ser utilizado. Assim, salve as modificações feitas no arquivo ” EstiloAguaCelula.java” e execute novamente a simulação clicando em “Run As…” e depois em “drenagem Model”. Abrirá o ambiente Repast, procure pela opção “Displays” na listagem à esquerda, clique em “A Display” com o botão direito e depois em delete. Agora, novamente clique em “Displays” e depois em “Add Display”. No assistente que surge, insira o nome “Grid Display”, depois selecione “grid”, clique na seta para a direita e em seguida, em Next. Na próxima tela, selecione “Celula” na lista que aparece à direita, clique na seta para a direita e depois em Next. Neste momento, você verá uma tela similar ao apresentado pela Figura 15.

Figura 15 – Assistente de configuração – Definindo o estilo

 

Na caixa de combinação, selecione o estilo “drenagem.EstiloAguaCelula” e depois em Next. Na tela seguinte, desmarque a opção “Show Grid” e depois clique em Next, Next e por fim em Finish. Salve as alterações feitas.

Para finalizarmos, inicialize a simulação (botão Initialize Run) e clique em Start. Neste momento, a simulação deverá iniciar e as linhas de drenagem devem emergir em tons de azul na GUI do ambiente.

O código fonte completo do exemplo por ser encontrado nesse link (drenagem.zip).

 

Referencias

ALLAN, R. J. Survey of Agent Based Modelling and Simulation Tools. . [S.l: s.n.]. Disponível em: <http://epubs.cclrc.ac.uk/work-details?w=50398>, 2010.

COLLIER, N. RePast: An Extensible Framework for Agent Simulation. . [S.l: s.n.]. Disponível em: <http://www.econ.iastate.edu/tesfatsi/RepastTutorial.Collier.pdf>, 2002.

CROOKS, A. T. The repast simulation/modelling system for geospatial simulation. . [S.l: s.n.]. Disponível em: <http://discovery.ucl.ac.uk/15176/>. Acesso em: 18 out. 2012, 2007.

HOWE, T. R. et al. CONTAINING AGENTS: CONTEXTS, PROJECTIONS, AND AGENTS. Proceedings of the Agent 2006 Conference on Social Agents Results and Prospects, p. 107-114, 2006.

THE MENDELEY SUPPORT TEAM. Getting Started with Mendeley. Mendeley Desktop. London: Mendeley Ltd. Disponível em: <http://www.mendeley.com>, 2011.

REPAST SIMPHONY TEAM. Repast Simphony Reference. Disponível em < http://repast.sourceforge.net/docs.html>, 2012.

 

 

[1] http://repast.sourceforge.net/

[2] Swarm é primeira ferramenta de modelagem baseada em agente. Mais informações em http://www.swarm.org/

[3] http://groovy.codehaus.org/"
O TerraLAB realizou o primero Meetup DevOps em Ouro Preto,http://www2.decom.ufop.br/terralab/wp-content/uploads/2019/12/meetupOP.jpg,"
No dia 6 de dezembro, ocorreu no auditório do Departamento de Ciência da Computação (DECOM) da Universidade Federal de Ouro Preto o primeiro Meetup DevOps Bootcamp de Ouro Preto. O DevOps Bootcamp é um projeto de capacitação e mentoria em educação corporativa, focada em Inovação, Automação, Segurança, Gestão e aplicação das melhores Ferramentas e Tecnologias existentes no mercado.

Tivemos o prazer de receber em nosso auditório os profissionais Amanda Pinto e Zandler Oliveira, integrantes do DevOps BH, que generosamente nos trouxeram muito conhecimento e novidades que prometem agitar a cidade de Ouro Preto.  Nossa gratidão e reconhecimento à dedicação e ao talento vocês é desmedida!

Nós tamém gostaríamos de agradecer imensamente o apoio das nossas empresas parceiras: Silingue, GerenciaNet, Usemobile e Cachaça Gestor. Sem o apoio de vocês, o evento não teria tanto brilho.

Fique atento às nossas redes sociais e ao nosso blog para não perder nenhuma novidade na área de tecnologia e inovação!

 

Nosso auditório ficou lotado! O próximo meetup terá de ser ainda maior…"
Encerramento do Programa Trainee TerraLAB 2019/1,http://www2.decom.ufop.br/terralab/wp-content/uploads/2019/12/WhatsApp-Image-2019-12-06-at-21.19.202.jpeg,"É com muito orgulho e alegria que anunciamos o fim e os resultados da primeira edição do Programa de Trainee TerraLAB que se iniciou no dia 23 de setembro. Dentre os 25 candidatos inscritos, apenas 7 foram selecionados. Os trainees iniciaram sua jornada no TerraLAB,no dia 21 de Outubro, onde vivenciaram todas as etapas do desenvolvimento de um produto de software.

Nosso time trabalhou duro durante 8 semanas, venceu barreiras e desafios, aprendeu novas tecnologias e entregou a primeira versão beta do produto com versão para WEB e Android. Durante todo o processo, desde a concepção do produto até a fase de testes, tivemos como mentores algumas das principais empresas geradoras de tecnologia do Brasil como GerenciaNet, Usemobile, Cachaça Gestor e Stillingue.

Os trainees tiveram a oportunidade de desenvolver aplicativos móveis na plataforma React-Native portáveis para Android e IOS. Também desenvolveram aplicativos WEB nas plataformas Angular JS e React. Além disso, eles desenvolveram serviços na nuvem AWS utilizando a plataforma NodeJS integrada a um banco de dados não estruturado em MongoDB. Para o desenvolvimento destes produtos, o time seguiu um processo ágil que misturas os aspectos mais fortes do processo SCRUM e as diretivas de gestão de projetos enunciadas no PMBOK (Project Management Body of Knowledge). A qualidade dos produtos foi garantida por adoção das técnicas BDD (Behavior Driven Development) para a concepção de histórias de usuário e para o projeto de cenários de testes, além da adoção de testes regressivos automatizados em diferentes níveis, contemplando desde verificações unitárias e funcionais até verificações de integração entre  módulos e de aceitação sistêmica por parte dos clientes. Para deploy dos produtos, todo ciclo CI/CD (continuous integration / continuous delivery) foi automatizado por meio do ferramenta para versionamento de software GitLAB.

Gostaríamos de agradecer imensamente a todas as empresas parceiras pelo apoio e parabenizar os trainees pelo empenho e seriedade.

Se você é um aluno de graduação ou pós-graduação e tem interesse em trabalhar com profissionais experientes, aprender a desenvolver aplicações a nível de mercado e aumentar sua empregabilidade, fique atento ao nosso blog e redes sociais para mais informações sobre o processo Trainee TerraLAB 2020/1.

Abaixo, destacamos as opiniões de três de nossos trainees sobre a experiência vivenciada:

——————-

“Estando perto de formar, já tive contato com bastante coisa na UFOP e, dentre elas, poucas me instigaram a querer aprender algo quanto o tempo no TerraLAB.

A experiencia é unica dentre todas as oferecidas na faculdade e extremamente enriquecedora e me sinto privilegiado de poder aproveitar disso antes de me formar.

Antes, eu sentia um certo receio da entrada no mercado de trabalho e uma duvida sobre como começar. O tempo no laboratório acabou resolvendo isso e servindo como preparo para coisas bem maiores do que até então a faculdade tinha oferecido. Se você deseja ficar na área cientifica, talvez todo o foco em pesquisa seja satisfatório porem, para alguém que deseja seguir para o mercado, o despreparo é enorme.

Aprender a trabalhar em grupo, enfrentar os problemas decorrentes do convívio e os problemas que surgem do próprio processo de produção do software são, ao menos para mim, experiencias mais importantes e interessantes do que aprender as tecnologias. Com um grupo ou sem, eu teria aprendido React-Native lendo as centenas de tutoriais na internet porem isso nunca teria me dado todo o resto da experiencia pratica de um ambiente coletivo.

Fico muito feliz pela oportunidade de participar do laboratório e a sensação de lançamento do software é algo que não tinha sentido antes.”

(Arilton é estudante graduação em Ciência da Computação/UFOP com expectativa de 1 ano para sua formatura)

————-

“Minha experiencia no TerraLab foi muito positiva , tive contato com muitas tecnologias que nao conhecia,além de ter tido a vivencia de participar  de um projeto de perto.O TerraLab tambem me motivou no proprio curso,pois vivenciei areas do curso no laboratório.”

(Guilherme é aluno do primeiro semetre do curso em Ciência da Computação/UFOP)

————-

“A lacuna entre o conhecimento desenvolvido na universidade
e o mercado de trabalho não nos causa espanto.

Mas me causou inicialmente surpresa e agora admiração a forma
como um laboratório universitário se posiciona frente essa lacuna.

Mais autodidada, voltei a academia sobretudo por questões
pessoais que por necessidade. Áqui vejo, pela primeira vez no meio acadêmico,
a construção de uma ponte sobre essa lacuna.

Acessível a estudantes interessados desde já em fazer acontecer.

Tudo isso sem se distanciar do embasamento teórico que podemos
(e devemos) mesclar com o conhecimento prático”

(Kleiber é aluno do quinto semestre do curso de Ciência da Computação/UFOP, recém chegado a Ouro Preto e traz consigo três anos de experiência como desenvolvedor fullstack em Belo Horizonte, MG)

—————-

Parabéns aos nossos novos colaboradores! 

Siga nossa redes sociais para  por dentro das novidades do processo seletivo Trainee TerraLAB 2020/1.

 "
Programa Trainee TerraLAB 2019.2,http://www2.decom.ufop.br/terralab/wp-content/uploads/2014/10/3.jpg,"Trabalhe com profissionais experientes e aprenda a desenvolver aplicativos móveis e web integrados serviços na nuvem.

 Inscrições de 11 a 18 de setembro de 2019

Graduando e pós-graduandos da UFOP:  Aumente sua empregabilidade e domine as mais atuais tecnologias da industria de software.

Conheça detalhes desta iniciativa.

Faça sua inscrição preenchendo o seguinte formulário.

A proposta está aberta aos estudantes dos mais variados cursos de graduação e pós-graduação da UFOP.

POR FAVOR, AJUDE-NOS A DIVULGAR ESTA INCIATIVA!!!"
"Marketing de Geolocalização, saiba mais sobre este assunto",http://www2.decom.ufop.br/terralab/wp-content/uploads/2018/08/uber-para-lavanderia.jpg,"Marketing de Geolocalização : Entendendo a Segmentação geográfica, Geo-Fencing e Beacons

O termo marketing de geolocalização é muito usado , mas muitas equipes  têm apenas uma vaga idéia do que isso significa.

Qualquer um pode atacar: o marketing de geolocalização provavelmente tem algo a ver com a personalização baseada em localização. Mas o que exatamente os profissionais de marketing personalizam com os dados de localização do usuário? E como coletamos esses dados em primeiro lugar?

Continue lendo para ter uma visão geral rápida do que é marketing de geolocalização e por que é importante.

O que é Marketing de Geolocalização?

De um modo geral, o marketing de geolocalização refere-se a dados sobre a localização física de uma pessoa. Os dados são geralmente fornecidos através de satélites GPS. Se você já abriu um aplicativo de mapa e ampliou o zoom para ver a precisão do pequeno ponto azul, os dados de geolocalização fornecidos pelo GPSdo.

Se o GPS do telefone estiver desligado (ou se não pu estão funcionander encontrar um sinal), os dados de localização serão triangulados das torres de celular . Este método é menos preciso, mas funciona em um aperto. Se você abriu seu mapa no subsolo ou no meio de uma viagem, provavelmente obteve seus dados de localização de uma torre de celular.

Até agora, só falamos sobre marketing de geolocalização da perspectiva do usuário – o dispositivo pinga uma torre de satélite ou celular para determinar em que parte do mundo está. Mas assim que o dispositivo obtiver essas informações, ele poderá compartilhá-las comaplicativos  (como um mapa ou um banco de dados de restaurantes locais).

Como as equipes móveis podem usar o marketing de geolocalização?

Você pode segmentar usuários com base nos dados de localização de algumas maneiras diferentes. Estes são os três mais comuns:

 Segmentação geográfica

 A segmentação geográfica é o ato de alcançar alguém com base em sua localização. Esses segmentos geralmente usam o endereço IP do visitante, em vez da localização por GPS, portanto, a segmentação por área geográfica é realmente anterior à mobilidade. Desde os primeiros dias da internet, os sites usavam o endereço IP do visitante para servir conteúdo personalizado. Um exemplo seria personalizar a moeda exibida em um site de varejo com base no país do visitante.

Naturalmente, os endereços IP não são muito precisos e é complicado para o profissional de marketing segmentar bairros específicos com base em blocos de endereços IP. É por isso que esse tipo de segmentação por área geográfica é mais comumente usado para regiões amplas, como uma cidade inteira (ou estado). Para as equipes de marketing que querem ser mais precisas, é melhor segmentar os usuários com base em uma delimitação geográfica.

Geo-Fencing

Geo-esgrima é a resposta da era móvel para geo-targeting tradicional. Esse tipo de segmentação usa a localização GPS do dispositivo em vez de seu endereço IP, portanto, os dados são muito mais precisos. Ele também é atualizado enquanto a pessoa está em trânsito, por isso é adequado para mensagens móveis.

Uma geo-fence pode ser tão ampla quanto uma cidade, mas é mais eficaz ao segmentar regiões menores, como bairros ou ruas específicas. Essas metas são especialmente úteis para aplicativos que desejam direcionar o tráfego de pedestres para lojas físicas.

Beacons

Os beacons são os mais estreitos dos três métodos de segmentação por local. Um beacon é um objeto físico pequeno que recebe dados de localização de dispositivos próximos via Bluetooth. Por ser baseado em Bluetooth, os beacons podem ser implantados em áreas com pouca recepção de células, como o interior de uma loja.

Os dados do Beacon informam ao aplicativo precisamente onde, na loja, os clientes estão caminhando, o que ajuda os editores a otimizar a experiência na loja. Mas a desvantagem é que o sinal do Bluetooth do usuário deve estar ligado. Além disso, os beacons são difíceis de usar fora de sua propriedade, porque eles devem estar fisicamente colocados também.

Qual é a melhor maneira de melhorar o engajamento de aplicativos com a segmentação por área geográfica?

Para equipes móveis em busca de táticas de marketing que aumentam o engajamento, a geofencing é um bom lugar para começar. A precisão dos públicos geográficos torna-os perfeitos para campanhas móveis, mas eles não exigem uma presença física para serem eficazes.

Por exemplo, um aplicativo de viagem pode querer alertar os panfletos que seu portão mudou por meio de notificação por push. Em vez de acionar a notificação com base no horário, o editor do aplicativo poderia estabelecer uma delimitação geográfica em torno de um aeroporto e acionar a mensagem com base na localização. Desta forma, eles vão entregar a mensagem com um timing perfeito.

Da mesma forma, um aplicativo que organiza restaurantes ou eventos locais pode acionar recomendações com base na vizinhança do usuário. Em vez de oferecer sugestões amplas (por exemplo, “Restaurantes Populares em sua cidade”), o geofencing possibilita sugestões pessoais e imediatamente valiosas (por exemplo, “Bem-vindo a [bairro]! Veja o que você precisa ver”).

Como posso começar com marketing de geolocalização?

A localização geográfica é intuitiva de uma perspectiva de marketing, mas pode ser difícil de implementar a partir de uma perspectiva de engenharia. A maneira mais fácil de começar é com uma plataforma de marketing para celular que ofereça suporte a campanhas baseadas em localização.

Para uma rápida olhada em como a Leanplum gerencia o marketing de geolocalização, leia nossa solução de personalização . Com as ferramentas certas, é fácil envolver seu público com mensagens baseadas em localização.

Para saber mais sobre geolocalização entre em contato com a Usemobile."
SIGHabitar: Um sistema de inteligência de negócios para a gestão municipal,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/10/arquitetura_SIGHabitar.png,"Interessado em GERIR seu município eficientemente, em conformidade com o governo federal e usando sistemas de informação geográfica para tomada de decisão?

Segundo estudo realizado pela Fundação Getúlio Vargas, no ano de 2007, dentre 3359 municípios brasileiros avaliados, somente 2,82% pode ser considerado eficiente na gestão do seu território [1]. No entanto, já existe tecnologia disponível para gerir os recursos e serviços de um município de maneira integrada, a partir de informações continuamente atualizadas, resultantes da análise de diversas fontes de dados alfanuméricos, imagens de satélite e mapas digitais. Informações importantes para as diversas secretarias de um município podem ser reunidas em um único repositório para apoiar o processo decisório em análises customizadas às necessidades de cada secretaria: Fazenda, Obra, Segurança, Saúde, Educação, etc. Neste contexto, apresentamos o sistema de inteligência de negócio para gestão de dados espaço-temporais, SIGHabitar, e apresentamos um estudo de caso realizado no município de Ouro Preto (OP), MG.

O convênio SIGHabitar em Ouro Preto e a portaria 511 de 2009 do Ministério das Cidades

O convênio SIGHabitar firmado entre a Universidade Federal de Ouro Preto (UFOP) e a Prefeitura Municipal de Ouro Preto – PMOP, através do Laboratório para Modelagem e Simulação do Sistema Terrestre (TerraLAB) do Departamento de Computação (DECOM), levou ao desenvolvimento de uma metodologia e de um conjunto de programas de computador que qualquer município pode utilizar para coletar, integrar, visualizar e analisar dados que servirão ao gerenciamento e ao planejamento do município. Todos os programas são gratuitos e estão disponíveis no site do laboratório. Para realizar o download clique: AQUI. Dentro desta inciativa, o projeto de mesmo nome, SIGHabitar, surgiu para atender às recomendações do Ministério das Cidades, que no ano de 2009 criou a portaria 511 [2], com diretrizes para criar, instituir e manter o Cadastro Técnico Multifinalitário no território nacional.

Por meio dos programas desenvolvidos é possível promover (i) uma tributação mais eficaz e justa, (ii) o uso racional dos recursos municipais, (iii) a manutenção e o desenvolvimento de serviços mais eficientes. Por exemplo, é possível dimensionar os serviços de saúde, segurança e educação. Manter atualizado o cadastro de imóveis. Planejar ações da defesa civil, do corpo de bombeiros, das secretarias de trânsito e obra. Além, de gerenciar o uso dos recursos naturais e a coleta de resíduos.

O que é um Cadastro Técnico Multifinalitário?

O Cadastro Técnico Multifinalitário (CTM) é um cadastro, ou banco de dados, capaz de manter informações alfanuméricas e espaciais sobre o território de um município  e de diversos fatos a ele relacionados. Além de contemplar aspectos do tradicional cadastro imobiliário concernentes aos imóveis, é possível relacioná-las com informações de bancos de dados heterogêneas e até mesmo desconectadas sobre uma diversidade de assuntos, como: fatores sociais (trabalho, lazer, saúde, educação e segurança); econômicos (produção e comercialização de bens e serviços) e urbanos (sistemas de abastecimento de água e coleta de esgoto, transporte e rede de comunicação). Isso permite conhecer detalhadamente qualquer porção do território, promover a distribuição justa de recursos e tornar o planejamento urbano mais acurado [3][4]. A figura ao lado ilustra o conceito CTM.

O que é um sistema de Inteligência de Negócios (ou Business Intelligence)?

 

Um Sistema de Inteligência de Negócios é um sistema de informação, que fornece ferramentas e métodos que subsidiam a tomada de decisão para o planejamento estratégico e tático de uma organização. Ele permite extrair, armazenar, explorar e transformar e analisar um grande volume de dados, de maneira rápida e simples para o decisor. Elementos fundamentais são: armazém de dados (DW – data warehouse), ferramentas extração, transformação e carga  (ETL – extract, transform & load), ferramentas para processamento analítico online (OLAP – online analytical processing)  e ferramenta de mineração de dados (DM – data mining).

Qual a filosofia do projeto SIGHabitar?

A filosofia do SIGHabitar é não mexer no que já está funcionando. Implanta-lo deve exigir pouco investimento tecnológico e permitir a continuidade dos trabalhos da prefeitura. Sua implantação deve ser incremental de forma a integrar tudo que já existe causando o mínimo de distúrbio possível, exigindo apenas que o município se adapte a uma cultura baseada na organização e sistematização da coleta de dados a seu respeito. Qualquer município, mesmo que precariamente, possui um passivo em termos tecnológicos que já existe, já foi pago e está em operação. Os bancos de dados, as planilhas, os documentos, as imagens, os mapas e os sistemas de informação já existentes devem permanecer onde estão e continuar em operação. O SIGHabitar deve apenas integra-los afim de permitir análises completas e atualizadas.

A despeito de todos os domínios de problemas administrados por meio do SIGHabitar, ele deve ser capaz de lidar com a dinâmica de uma cidade. Deve estar preparado para mudanças e evoluir com elas e ser capaz de atualizar frequentemente um volume massivo de dados espaciais e temporais. Por isso, flexibilidade para absorver mudanças com pouco esforço e retrabalho mínimo é uma característica indispensável.  Ferramentas que coletam, transformam e analisam os dados do CTM não dependem de seu modelo de dados. Para que resultem em decisões mais acuradas, suas ferramentas permitem análises sobre dados sumarizados em hierarquias espaciais, como país, cidade e imóvel, e hierarquias temporais, como ano, mês e dia.

O desenvolvimento um de CTM pode ser dispendioso. Entretanto, a sustentabilidade do SIGHabitar é garantida. O desenvolvimento do CTM pode envolver a contratação e treinamento de uma equipe multidisciplinar e especializada, a realização contínua de coleta de dados em campo e a aquisição de dados espaciais onerosos, como imagens de satélite de alta resolução, levantamentos topográficos, mapas de arruamento e de imóveis. A necessidade, quantidade e o nível de detalhe dos dados devem ser ponderados frente aos benefícios que resultarão de seu custo. É preciso desenvolver um CTM cujo custo de desenvolvimento seja pago nos primeiros anos após sua implantação através da ampliação do cadastro de imóveis registrados e da cobrança mais eficaz de taxas e impostos imobiliários. Nos anos posteriores, os ganhos serão menores, mas deverão financiar a manutenção e evolução do sistema.

No Brasil, para estarem em conformidade com as diretrizes nacionais para Governo Eletrônico, projetos de desenvolvimento de Sistemas de Informação públicos devem priorizar soluções, programas e serviços baseados em software livre, adotar padrões abertos no desenvolvimento de tecnologia de informação, garantir a auditabilidade plena e a segurança dos sistemas, restringir o crescimento do legado baseado em tecnologia proprietária, além de promover a capacitação de servidores públicos para utilização de software livre. Por estas razões, o SIGHabitar  priorizou o uso de tecnologias livres e de código aberto cujo estágio de maturidade garantem a produtividade do projeto e sua continuidade à longo prazo [5].

Como o sistema SIGHabitar funciona? Qual a inovação?

Apesar das tecnologias para o desenvolvimento e implantação de CTMs existir há, pelo menos, uma década, a maioria dos municípios brasileiros não utilizam  sistemas de computação que apóiem seu planejamento ou a gestão de seus recursos e serviços. Este fato pode ser justificado porque os investimentos em tecnologia são bastante restritos na maioria dos municípios de pequeno e médio porte.  Além disso, a ausência de uma metodologia padronizada e reconhecida para a construção e manutenção de um CTM é uma limitação que desencoraja muitos municípios a investir recursos na reforma do cadastro imobiliário, pelo fato de não terem garantia de sucesso. No Brasil, essa situação é agravada porque o número de profissionais capacitados nessa área é insuficiente para um país com dimensões continentais. O sistema SIGHabitar inova ao fornecer métodos sistematizados e ferramentas de automação destinados à construção e a manutenção de um CTM arquitetado como um sistema de inteligência de negócios.

INTEGRAÇÃO COM SISTEMAS LEGADOS

No SIGHabitar, o CTM é organizado como um armazém de dados espaço-temporal (STDW  – Spatio-temporal Data Warehouse) no qual ferramentas de atualização, implementadas como fluxos ETL, extraem dados de diversas fontes, tais como planilhas eletrônicas, arquivos texto, arquivos shape e bancos de dados relacionais transformando-os em informações de entrada para o CTM. Esses fluxos são executados periodicamente (de hora em hora, toda noite) ou sob demanda (ao clicar do mouse). Os fluxos ETL para integração de dados aparecem em vermelho na figura acima. Para facilitar o desenvolvimento e a manutenção dos  fluxos ETL pela própria equipe da prefeitura, o projeto SIGHabitar integra-se com a ferramenta GeoKettle, que permite que tais fluxos sejam definidos graficamente, por meio de diagramas como o apresentado à esquerda.

DISPOSITIVOS MÓVEIS APOIAM A COLETA DE DADOS EM CAMPO

Para ajudar a equipe a georreferenciar os dados de sistemas legados, ferramentas de coleta de dados foram desenvolvidas como aplicações para dispositivos móveis. A figura ao lado apresenta a tela de uma destas ferramentas. Ela apóia as pesquisas de campo, nas quais equipes percorrem a cidade levantando os limites, as coordenadas geográfica ou os endereços de pontos de interesse como, por exemplo, imóveis, equipamentos urbanos (escolas, hospitais, pontos de ônibus, etc), áreas de preservacão ambiental, árvores, postes, etc.

SANEAMENTO CÍCLICO E INCREMENTAL DOS DADOS MUNICIPAIS

Durante a integração dos dados para a construção do CTM, os trabalhos de campo ajudam a equipe da prefeitura a conhecer melhor a realidade do território municipal. À medida que os endereços e limites de  imóveis ou equipementos urbanos são conhecidos, estes dados são padronizados e inseridos no CTM. Posteriormente, são associados a informações provenientes de outras fontes da dados da prefeitura. Porém, para que a associação aconteça de maneira correta, os dados do município também precisam ser corrigidos. Por exemplo, em Ouro Preto a rua “Conde de Borbadela” é conhecida popularmente como  rua “Direita”. Contudo, muitas vezes os registros e documentos mantidos pelo municipio se referem a esta rua como rua “Conde Borb.”, rua “Dir.”, ou rua “Direita”. Então, para que os dados possam ser associados, a equipe da prefeitura pecisa definir o nome que será utilizado pelo CTM e corrigir os dados antigos da prefeitura. Esse processo pode ser automatizado e é  representado pelos fluxos ETL que aparecem em azul na figura acima. Desta maneira, todo o sistemas de informação do município evolui, cíclica e incrementalmente.

FERRAMENTAS PARA ANÁLISE DE DADOS ESPAÇO TEMPORAIS – MAPAS DINÂMICOS

A análise de dados espaço-temporais são baseadas em ferramentas que combinam técnicas de mineração de dados, OLAP e GIS. Elas são integradas a ferramentas como Google Maps, Google Earth e Microsoft Bing. Elas permitem ao decisor analisar séries temporais de mapas que mostram medidas agregadas a cerca de suas regiões de interesse. Por exemplo, a média de crimes por bairro, o soma do número de casos de dengue por setor censitário, o IPTU mínimo e máximo de determinadas regiões, etc. A figura abaixo ilustra o resultado da primeira campanha de campo e a integração destas ferramentas com o Google Earth. OS DADOS APRESENTADOS SÃO FICTÍCIOS!

A figura a seguir mostra a tela da ferramenta DW-ReportMaker cujos relatórios mostram análises de medidas espaciais agregadas.

A figura a seguir mostra um relatório no qual uma série temporal de mapas gerados por uma das ferramentas de análise desenvolvidas neste projeto. É possível observar a evolução do valor do IPTU nos anos de 2009, 2010 e 2011 para a região próxima à UFOP, conhecida como “Bauxita”.

Quais são as lições aprendidas?

Durante a realização deste trabalho, as seguintes lições foram aprendidas:

Lição 1 – É preciso uma equipe multidisciplinar: Dispor de uma equipe multidisciplinar é o principal fator determinante para sucesso de um CTM. Além do conhecimento de especialistas na administração do município, é preciso mão de obra capacitada nas seguintes áreas: edificações e engenharia civil, computação e geoprocessamento. Durante os trabalhos de campo, especialistas em construção civil apresentaram maior facilidade no uso de mapas, em entender o processo de urbanização a partir de imagens defasadas da realidade e em registrar a geometria de imóveis e logradouros. O desenvolvimento de um banco de dados geográfico requer especialistas em geoprocessamento. É ideal que especialistas em computação se responsabilizem pela gestão do CTM e pela customização das ferramentas de coleta, atualização e análise de dados.
Lição 2 – É preciso envolvimento direto dos administradores municipais (stakeholders): O empenho de tomadores de decisão, técnicos administrativos e especialista nos sistemas legados é vital para a avaliação de versões do CTM e fluxos ETL produzidos. Eles devem assegurar a evolução incremental do CTM. O conhecimento sobre o território do município e sua história é essencial para o saneamento dos dados.
Lição 3 – É possível desenvolver um sistema de BI espaço temporal com ferramentas livres. Contudo, a produtividade da equipe é afetada: Existem excelentes frameworks para o desenvolvimento de sistemas de BI que fornecem ferramentas ETL, DM e OLAP para dados espaciais integradas a SIGs GeoKettle e Talend. No entanto, a deficiência ou inexistência de serviços de suporte técnico e treinamento, a escassez de documentação exemplificada e de casos de sucesso para serem copiados limitam a desempenho da equipe de desenvolvimento.
Lição 4 – É preciso contrabalancear a precisão temporal e a precisão espacial do CTM: O dinamismo da ocupação do solo torna dispendiosa e extenuante a tarefa de manter atualizado um CTM baseado em polígonos que representam as parcelas do território e as unidades habitacionais. No entanto, representar as parcelas e os respectivos objetos que as sobrepõe, por meio de pontos simplifica o processo de atualização, permite o mapeamento eficiente de construções verticais e permite que métodos de análises espaciais sejam aplicados a dados de sistemas legados: estimadores de kernel, krigagem, formação de grupamentos espaciais, etc. Por isso, os gestores do CTM devem considerar o esforço envolvido na atualização dos dados e retorno trazido pela representação espacial escolhida. É preciso balancear precisão espacial e precisão temporal.
Lição 5 – Ferramentas móveis para coleta de dados trazem enorme ganho de produtividade. Durante os trabalhos de campo, o uso de plataformas móveis com GPS e conexão de dados em banda larga permite a navegação assistida por mapas digitais e reduz erros introduzidos por processos manuais de coleta de dados. Resultados de mesma qualidade obtidos por processos manuais requerem, em geral, mais que o dobro do esforço.
Lição 6: É viável abordar um CTM como um sistema para inteligência de negócio:  O uso de tecnologias de Inteligência de Negócios atendeu às expectativas da pesquisa. Elas permitem a análise de volumes massivos de dados. Foi possível atender as diretrizes nacionais para governo eletrônico, a portaria 511 do Ministério das Cidades e as diretivas do projeto Cadastre 2014. Os tomadores de decisão reportaram enorme vantagem em conhecer os valores de indicadores sumarizados por diferentes hierarquias temporais (ano, mês, dia) e espaciais (distrito, bairro, quadra). A continuidade do CTM é assegurada pela maturidade das ferramentas de BI e por serviços para customização de ferramentas através de diagramas e interfaces gráficas. O uso de fluxos ETL se mostrou uma estratégia flexível e eficiente na atualização do CTM. Estima-se que devido à ampliação da base imobiliária cadastrada e de uma tributação mais efetiva, cinquenta por cento do investimento neste projeto tenha sido recuperado no primeiro ano em que ele foi utilizado para cobranças de taxas e impostos. Isto demonstra a sustentabilidade da abordagem proposta. No entanto, a partir de certo momento, o esforço de campo não resultará na evolução do CTM até que ocorra a reorganização do espaço geográfico (estabelecer limites, oficializar logradouros, normatizar numeração, etc.) e o recadastramento de imóveis. É necessário recobrir todo universo do cadastro, para que as políticas sociais sejam efetivas.
Lição 7: O SIGHabitar vs outras abordagens: Os trabalhos encontrados na literatura não apresentam uma metodologia consolidada, para criar e manter um CTM.  A abordagem SIGHabitar consolidou em um estudo de caso real, um conjunto de métodos, softwares livres e modelo de banco de dados multidimensional, para instituir o CTM em municípios bresileiros de pequeno e médio porte. Esse conjunto permitiu georreferenciar informações em campo, integrá-las aos sistemas legados em um DW e criar fluxos automatizados para manter o DW atualizado. Com isso, é possível explorar com maior eficiência um grande volume de dados e gerar relatórios mais completos e atualizados. A utilização de softwares livres torna a solução mais barata, o que permite ser utilizada por municípios que possuem menos poder de investimento destinado às evoluções tecnológicas.
Contato:

Se gostou do trabalho entre em contato. O TerraLAB pode ofecer uma solução eficiente, pessoal capacitado e resultados em curto prazo (6 meses aproximadamente). Certamente poderemos lhe ajudar a GERIR seu município eficientemente, em conformidade com o governo federal e usando mapas para tomada de decisão.

Mais informações podem ser obtidas no wiki do TerraLAB. Este trabalho é resultado da dissertação de mestrado em Ciência da Computação, do DECOM – UFOP, de autoria do Msc. João Tácio Corrêa da Silva [6]. foi publicado na Conferência Internacional “Computational Science and Its Applications, 2012” [7].

Contato com os professores Tiago Carneiro e Joubert Lima: tiago@iceb.ufop.br e joubertlima@gmail.com.

Referências

[1] Fundação Getúlio Vargas (2007). “Estudo Comprova Eficiência Fiscal dos Municípios do PNAFM”. Disponível em:  http://www.ucp.fazenda.gov.br/PNAFM/, Acesso: Novembro de 2011.

[2] Ministério das Cidades (2009). “Portaria 511: Diretrizes para a Criação, Instituição e Atualização do Cadastro Territorial Multifinalitário (CTM) nos Municípios Brasileiros”. 2009. ISSN 1676-2339. Disponível em: http://www.cidades.gov.br. Acesso Agosto, 2011

[3] Loch, C. and Erba, D. A. (2007). “Cadastro Técnico Multifinalitário – Rural e Urbano”. Cambridge, MA: Lincoln Institute of Land Policy, 2007. 142p.

[4] Pereira N. E. C. (2002). “Repensando o valor do cadastro técnico urbano”. In: Congresso Brasileiro de Cadastro Técnico Multifinalitário, 5, Florianópolis. Anais. 2002.

[5] Governo Eletrônico (2012). “Diretrizes”. Disponível em: http://www.governoeletronico. gov.br/o-gov.br/principios/, Acesso: Julho de 2012.

[6] da Silva,  João Tácio  Corrêa. “SIGHabitar – Uma abordagem baseada em Inteligência de Negócios para o desenvolvimento de Sistemas de Informação Territorial: O Cadastro Técnico Multifinalitário do Município de Ouro Preto, MG“, Dissertação de mestrado apresentado ao Programa de Pós-Graduação em Ciência da Computação. Departamento de Comutação – DECOM. Universidade Federal de Ouro Preto – UFOP, 2012. Orientador: Tiago Garcia de Senna Carneiro. Co-Orientador: Joubert de Castro Lima.

[7] Silva, J. T. C. ; Rezende, J. F. V. ; Fidêncio, E. ; Melo, T. ; Neves, B. ; LIMA, J. ; Carneiro, Tiago . SIGHabitar Business Intelligence based approach for the development of Land Information Systems: The Multipurpose Technical Cadastre of Ouro Preto, Brazil. In: ICCSA – International Conference on Computational Science and Its Applications, 2012, Salvador,BA. International Conference on Computational Science and Its Applications, 2012.

 

 "
TerraME: A software platform for modeling and simulation of nature-society interactions,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/amazon_deforestation.gif,"Translated to English language by Tamyres

On Earth, several social and natural systems interact and co-evolve. The understanding of natural phenomena challenges science since its emergence. Various scientists tried to know and describe, using mathematical formulas, the rules that govern these phenomena behaviors, in other words, they constructed mathematical models of these phenomena. On physics and chemistry areas, various models evolved to represent realistically the observed phenomena. In 1807, Dalton modeled the atom as a spherical and indivisible particle without any electric load. In 1913, Bohr demonstrated that the atom had a positive nucleus surrounded by electrons with negative loads and organized in orbits with different energy levels. In past, the Earth was considered flattened. The mathematical models known by science allow the construction of future predictions about several natural systems. To understand the importance the continuous evolution of these models, consider the utility of climatic models to the agribusiness and to the early warning systems of nature disasters.

Despite efforts of anthropology and sociology, the social systems functioning are still not understood. However, currently the social systems are seen as the main driver of changes acting upon nature systems. On the other hand, the nature systems are the main conditioners of social systems behavior. Thus, the social systems affect and are affected by nature systems. So, computer-mathematical models able to simulate the interactions between social and natural systems constitute essential tools to the stakeholders responsible for the definition of politics that aim to regulate the use of Earth’s resources.

Indeed, any decision of governments or private enterprises can be benefited by computational models able to simulate scenarios that evaluate impacts of different strategies: (1) of resource use – space, time, vegetation, water, etc; or (2) of control and response to risks – epidemics, fires, floods, etc. In Brazil, The National Institute of Spatial Researches (INPE) uses TerraME to answer questions related to Land Use and  Cover Change (LUUC) in Amazon [1][2][3] region, to answer questions related to Monitoring, Risk Analysis and Early Warning, and to answer questions about Emission of Greenhouse Effect Gases [4]. Researches of Osvaldo Cruz Foundation (FIOCRUZ) used TerraME to evaluate questions about Dengue Control [5].

TerraME Overview

The TerraME – Terra Modeling Environment – (www.terrame.org) is a toolkit that supports all the phases of environmental model development, namely, model that reproduce the behavior of social and natural processes, showing their interactions and  different changes they promote on each geographic space location. In TerraME, an environmental model is seen as micro-world, namely, a virtual world whose landscape represents a determined region on the Earth, and whose agents, automata and systems that live on it are able to simulate real actors and process of change. TerraME tools are developed by the TerraLAB – Earth Systems Modeling and Simulation Laboratory of the Computer Science Department (DECOM) of the Federal University of Ouro Preto (UFOP) in partnership with the National Institute of Spatial Research (INPE). They are distributed freely to Linux, Windows, and Mac platforms (32 and 64 bits).

The main differential of TerraME in relation with other environment modeling platforms currently available is on the support services for the development of models that consider multiple spatial-temporal scales and on the support to the simultaneous use to multiple modeling paradigms: Systems General Theory, Agents Theory and Cellular Automata Theory. Thus, the modeler does not feel restricted by the choice of a single modeling paradigm and can realistically represent the different scales in which changes occur and in which act the different driving forces of these changes. For instance, in Amazon, the deforestation process is affected locally by the economic situation of small producers, while that globally is affected by commodities prices as soybeans and cattle.

TerraME Graphical Interface for Modeling and Simulation

To support the conception and design of environmental models, TerraME offers a integrated development environment (IDE) known as TerraME GIMS – Graphical Interface for Modeling and Simulation. It allows modelers visually describe their models using diagrams. These diagrams explicit the entities considered in the model, the way how they are hierarchically organized and how they interact with each other. This way, TerraME GIMS facilitates the verification of assumptions in which the models are based. It communicate these assumptions to the decision makers and to the members of the development team that is, routinely, interdisciplinary. The TerraME GIMS is distributed freely as a plugin to the Eclipse platform (www.eclipse.org).

TerraME Interpreter

To support the development of realistic models, TerraME simulator is integrated with Geographical Information System (SIG) that allows modelers to parameterize models from data stored in geographical databases, i.e., temporal series of satellite images and digital maps describing the studied regions. The simulator TerraME is distributed as an interpreter that receives, as entry, the program that represents the environmental model and, so, executes it producing output data that are automatically stored in geographical databases. To facilitate the management and evolution of environmental models, TerraME models are represented on a high level modeling language named TerraML – TerraME Modeling Language, which extends LUA programming language. It was designed to make the job of describe spatial properties, the actors and the processes that changes them. In this language, a complex model can be easily decomposed on simpler models that represent different scales of a same process.

TerraME – Modeling in Multiple Scales

To support model debugging and the analysis of model outcomes, TerraME has a set of components called TerraME Observers. These components are able to show model outcomes, in real-time, as dynamic graphics and maps, on two or three dimensions.

TerraME Observers 

To support the analysis of model sensibility and the projection of simulated scenarios, TerraME simulation nucleus offers a high performance version called TerraME HPA – High Performance Architecture. This simulator version is able to take advantage from the aggregated processing and storage power of multiprocessor and multicomputer hardware architectures, as well as, from computer networks aggregated power. This way, experiments for model sensibility analysis or for the simulation of different scenarios of change can be executed in parallel and in a reduced time.

To support the calibration of environmental models, TerraME offers a set of methods to measure the adjustment between real and simulated maps, in addition to a set of methods to optimize the model parameters so that they maximize the adjustment between the simulated outcomes and the observed data."
"Configurando uma rede linux com servidor web (NAT, DNS, DHCP)",http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/linux-networking.jpg,"Nesse tutorial usaremos como exemplo a rede do TerraLAB que utiliza o OpenSuse 12.1 devido as facilidades que suas ferramentas de configuração oferecem.

Antes de mais nada alguns pré-requisitos:
* OpenSuse 12.1 (atualizado até a data de hoje 20/09/2012)
* Apache 2.2.21
* PHP 5.3.8
* 2 Placas de rede

No nosso caso a interface Eth0 foi configurada para a rede interna no IP (Internet Protocol) 192.168.0.1 e Eth1 configurada via DHCP (Dynamic Host Configuration Protocol), porém ela recebe um IP estático do provedor de acesso à Internet  o qual possui o registro do domínio: www.terralab.ufop.br.

Usando a ferramenta YaST2, clique em Network Settings e confira se está como a imagem a seguir:

Como utilizamos um computador da rede interna como servidor FTP (File Transfer Protocol)  e de alguns serviços do laboratório precisamos ativar o redirecionamento de IP e fazer algumas configurações de NAT (Network Address Translator). Para ativar o redirecionamento escolha a aba Routing e clique em Enable IP Forwarding.

Para configurar o NAT edite as configurações de Firewall do YaST2 da seguinte maneira:


Vamos agora configurar nosso servidor HTTP (Hypertext Transfer Protocol), também chamado de servidor WEB ou WWW, que no OpenSuse fica no diretório /srv/www/htdocs/ de maneira bem simples. No YaST2 clique em HTTP Server e deixe como na imagem a baixo:

Nesse ponto nós já temos o servidor HTTP  funcionando na porta 80 (padrão web) e um redirecionamento para o IP 192.168.0.234 quando acessado na porta 8080, agora vamos configurar o servidor DHCP para que todas as máquinas da rede interna recebam automaticamente um IP inválido ao serem ligadas. Elas terão acesso  à Internet por meio do roteador (gateway) 192.168.0.1 que estamos configurando. Este roteador também terá  o serviço de firewall executando. Desta maneira, buscamos a aumentar a segurança da rede interna.

Para configurar o servidor DHCP clique em DHCP Server, selecione Global Options e clique em edit

Configure da seguinte maneira:

Logo após configure a subnet da seguinte maneira:

Nossa rede está quase pronta ! Para facilitar o acesso aos computadores o YaST também disponibiliza uma ferramenta para configuração de um servidor DNS
Clique em DNS Sever e siga os passos:


Na imagem acima escolhemos servidores DNS Globais para resolução de nomes de domínios da Internet, e não na rede interna. Os dois primeiros são servidores disponibilizados no nosso provedor de acesso, o último é o do Google.

Crie um novo domínio para ser usado na rede interna, no nosso caso, “terralab”.

insira um novo computador para o domínio:

Pronto ! temos nossa rede interna com servidor linux completamente operante e um servidor HTTP com redirecionamento de porta.

Dúvidas, sugestões, críticas deixe seu comentário abaixo."
Como desenvolver geo-aplicações em realidade virtual sobre a plataforma TerraVR,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/rv.jpg,"Oaplicativo TerraVR é um interpretador destinado ao desenvolvimento de aplicações em realidade virtual (RV).  Ele recebe como entrada  e executa um script escrito na linguagem de programação Lua [Ierusalimschy et. Al, 1996]. Este programa em linguagem de alto nível define todas as propriedades da aplicação de realidade virtual: objetos que comporão a cena tridimensional, topografia do terreno do ambiente virtual, animações e a interatividade do usuário. Para isso, o interpretador TerraVR estende a linguagem de programação Lua com novos tipos, objetos e algoritmos especialmente projetados para aumentar a produtividade dos programadores de aplicações em RV e, consequentemente, tornar mais eficiente o processo de desenvolvimento. Para ilustrar, suponhamos o que o código fonte da aplicação em RV esteja armazenado no arquivo de script “Aplicacao.lua”. Então comando “c:\> TerraVR Aplicacao.lua” coloca a aplicação em execução.

O TerraVR é um exemplo de que a Computação provê soluções para:

Empresas e organizações:

O TerraVR permite a criação de jogos sérios para visualização e monitoramento de operações industruais em espaços abertos ou fechados,  como grandes obras, para visualização arquitetural, para treinamentos, e para reconstituição de acidentes. Veja o video de uma aplicação para treinamento de operadores de guindastes em plataformas oceânicas: P-16.

Indivíduos:

O TerraVR permite a criação de   jogos, animações, apresentações e demais soluções de entreterimento.

 

 

 

1. Histórico do projeto TerraVR

O projeto TerraVR teve inicio com uma parceria firmada entre o laboratório TerraLAB do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP) e a empresa especializada em animações digitais Visual Virtual sediada em Belo Horizonte, MG. Esta primeira iniciativa foi documentada na monografia de conclusão de curso do estudante Douglas Oliveira Matoso, em 2007.  Após dois anos de desenvolvimento e contando com uma equipe formada por três ex-alunos do laboratório TerraLAB e vários profissionais de jogos, design 3D e comunicação da empresa Visual Virtual, a primeira aplicação do framework TerraVR foi comercializada, dando origem à startup MineInside.  A aplicação de mesmo nome, MineInside, permitia que as operações da maior mina de minério de ferro à céu aberto do mundo fossem acompanhadas em tempo real por meio de animações tridimensionais com as quais os usuários podiam interagir através de equipamentos em realidade virtual, como joysticks e capacetes com visores acoplados. Na aplicação MineInside, equipamentos de lavra como caminhões fora de estrada, escavadeiras e perfuratrizes são posicionados em tempo real em um ambiente virtual georreferenciado (mina) a partir de dados fornecidos por equipamentos GPS (Global Position System) embarcados.  Mais tarde, a empresa MineInside foi vendida para o grupo DEVEX que levou esta tecnologia a patamares de qualidade e retorno de investimento sem precedentes.

2. Funcionamento básico de uma aplicação em realidade virtual

Antes de começarmos a falar sobre como é composta uma aplicação TerraVR, devemos ter uma noção de como é o fluxo de execução de um script. O código fonte da aplicação de RV (script Lua) é dividido em duas partes. A primeira parte é chamada inicialização, onde o usuário define os cenários, cria entidades (terreno, prédios, carros, personagens, etc). É a parte declarativa do código da aplicação. O usuário cria e  posiciona tudo em seu lugar. A segunda parte, chamada laço principal, é executada repetidamente até o fim da execução da aplicação, é onde está a lógica da simulação, as transformações que as entidades sofrem, suas ações e reações.

O laço principal da aplicação RV é executado o maior número de vezes possível por segundo. O tempo consumido na execução de uma iteração do laço principal, isto é, o tempo  consumido para recalcular e  enviar a cena tridimensional para a placa de vídeo, é chamado tempo de simulação, e depende da complexidade da simulação, da capacidade de processamento do hardware (processador) onde a aplicação estiver sendo executada e da capacidade de comunicação do barramento da placa gráfica. Quando somado ao tempo que a placa gráfica consome para renderizar a cena tridimensional, chamado tempo de renderização, juntos passam a definir a taxa na qual a aplicação RV consegue responder a qualquer interação com o usuário.  Essa taxa é geralmente expressa em número de quadros por segundo (qps), e deve ser mantida em um patamar sempre igual ou superior a 10 qps para garantir ao usuário da aplicação RV a sensação de imersão [Pullen, 1995]. Contudo, idealmente, a taxa de quadros por segundo de uma aplicação RV deve ser, no minimo, igual a 30 qps. Assim, será garantida a continuidade dos movimentos das entidades presentes na cena tridimensional, mesmo os mais rápidos como o deslocamento de um avião. Essa é a taxa de quadros utilizados por filmes na indústria do cinema.

3. Código fonte de uma aplicação mínima em realidade virtual

A mais simples aplicação RV que pode ser desenvolvida na plataforma TerraVR ilustra como um desenvolvedor pode criar um ambiente virtual estático e não interativo  na área central de sua janela principal. O código dessa aplicação é apresentado na figura 1. O trecho de código compreendido entre as linhas 1 a 11 define a interface com o usuário (GUI) da aplicação RV e inicializa seu motor gráfico.

Figura 1. Código fonte para uma aplicação RV mínima: uma câmera para o ambiente virtual.

                Através da chamada ao método createWindow() a janela principal da aplicação é criada e armazena na variável window. A janela principal da aplicação RV conterá toda a interface da aplicação RV, inclusive uma ou mais janelas de renderização, criadas através do método createRenderView(), que delimitam áreas da janela principal onde a cena tridimensional será projetada em duas dimensões .

O método createWindow() é responsável pela inicialização do motor gráfico Ogre 3D, seus parâmetros são: (a) e (b) o tamanho da janela principal da aplicação RV, no código o tamanho da janela de renderização é definido em 800X600, (b) um valor lógico que em caso verdadeiro indica que a resolução máxima do monitor de vídeo deve ser utilizada para definir o tamanho da janela de renderização, e (c) o sistema de renderização que será utilizado em tempo de execução, que pode variar entre os valores OPENGL ou DIRECT3D.

Em seguida, é criado um frame para aplicação RV que agirá como um container para todos os elementos da GUI da aplicação RV. Esse frame é armazenado na variável frame3d (Qt::QFrame). Um layout vertical, variável layout (Qt::QVBoxLayout), é criado para organizar os elementos de interface no interior do frame. Então, por meio da chamada ao método createRenderView(), uma única janela de renderização é criada e inserida no layout vertical e, automaticamente, registrada no motor gráfico Ogre 3D sob o identificador único “render”. Finalmente, o objeto frame3d é definido como frame central da janela principal, por meio de uma chamada ao método setCentralWidget() do próprio objeto janela principal.

No trecho de código definido pelas linhas 13 e 14, o modelo tridimensional que representa o ambiente virtual é carregado em memória a partir do arquivo “apartamento.scene”,  é possivel fazer o download do arquivo utilizado clicando aqui, por meio do método loadScene(). Arquivos do tipo “.scene” são criados através de um plugin chamado OgreMax para o software Autodesk 3DStudio Max que não será abordado nesse documento. O segundo parâmetro do método loadScene() é uma referência para a janela de renderização, render, onde os elementos da cena serão carregados.

Para visualizar o ambiente virtual é necessário um objeto do tipo Camera. O trecho de código entre as linhas 16 e 25 cria a câmera cam usando o construtor Camera(), que recebe como parâmetro uma string que, em tempo de execução, será utilizada para identificar unicamente a câmera criada. Na figura 1, a câmera e posicionada na coordenada (0,50,0) e rotacionada em 90 graus em torno do eixo Y. Em seguida, o objeto cam é anexado ao objeto render pelo método setCamera(). O método setRange() é utilizado para definir o campo de visão da câmera. Nesse exemplo, somente objetos entre 1 e 2500 unidades de medida (que pode ser centímetros, metros, etc dependendo da escala da cena gráfica) serão visualizados.

O último trecho de código, entre as linhas 27 e 30, define o método updateWorld() que deve OBRIGATORIAMENTE ser definido pelo desenvolvedor da aplicação RV. Nesse exemplo, como não há animações na cena tridimensional apresentada, nem qualquer forma interatividade com o usuário definida, função updateWorld() deve apenas retornar o valor true indicado seu termino com êxito. O valor false indicaria o térmíno a aplicação em uma condição de falha.

Figura 2. Resultado da aplicação RV mínima: Uma câmera para o ambiente virtual

                Esse exemplo, apesar de simples, apresenta uma visão geral de como uma aplicação RV pode ser desenvolvida por meio do uso do framework TerraVR.

 

4. Arquitetura de Software do framework TerraVR

O framework TerraVR foi desenvolvido em uma arquitetura em camadas, onde cada camada de software oferece serviços para que a camada de software no nível imediatamente superior possa utilizá-los para  implementar seus próprios serviços. A arquitetura de software do framework TerraVR é estruturada em 4 camadas:

Figura 1.1. Arquitetura de software do framework TerraVR: organizada em camadas.

Na primeira camada, chamada camada de jogos, as bibliotecas C++ Ogre 3D, OIS, OpenAL e PhysX implementam os serviços básicos necessários ao desenvolvimento de aplicações em realidade virtual, cujas demandas são similares àquelas presentes em projetos de desenvolvimento de jogos tridimensionais. A biblioteca Ogre 3D fornece serviços para gerenciamento de cenário, como carga de objetos modelados em sistemas CAD (Computer Aidded Design), serviços para animação de cenários, como operações de rotação e translação de objetos tridimensionais e serviços para renderização da cena gráfica, isto é, serviços para a definição de texturas e de iluminação dos objetos que formam a cena e serviços para transformar a cena tridimensional em imagens bidimensionais que serão continuamente apresentadas ao usuário da aplicação em realidade virtual. A biblioteca OIS oferece serviços para o tratamento de eventos gerados por dispositivos de entrada e saída como joystiks, mouse ou teclado. Esses são serviços críticos para o desenvolvimento de ambientes virtuais interativos onde o usuário da aplicação em RV pode agir sobre os objetos presentes no ambiente virtual e observar a reação desses objetos. Essas reações são modeladas através dos serviços fornecidos pela biblioteca de simulação física chamada PhisX.  A biblioteca OpenAL fornece serviços para gerenciamento de áudio como, por exemplo, a definição de fontes emissoras de som e a renderização do som em 3 dimensões. Desta maneira, o som percebido pelo usuário da aplicação em RV dependerá da sua posição no ambiente virtual e da posição da fonte emissora. A biblioteca Qt fornece serviços para a definição de interfaces gráficas com o usuário (GUI) da aplicação em RV, exemplos de interfaces construídas como a biblioteca Qt são caixas de diálogos, menus popups, botões, etc.

Na segunda camada, chamada camada de realidade virtual, os serviços fornecidos pela camada de jogos são integrados em um único módulo de software chamado kernel TerraVR  desenvolvido na linguagem C++. Portanto, a interface de programação (Application Programming Interface – API) desse módulo integra todos os serviços necessários para a definição, gerência, simulação e renderização (visual e áudio), em tempo-real, dos objetos presentes no ambiente virtual.

A terceira camada, chamada camada Lua, exporta todas as suas funcionalidades (API) do kernel TerraVR para o ambiente de execução da linguagem de programação Lua.  Desta maneira, o desenvolvedor de aplicações em RV pode codificar toda a aplicação em uma linguagem de alto-nível e ainda se beneficiar do alto-desempenho fornecido pelos serviços implementados em C++.

Na quarta e última camada, chamada camada de aplicação, se encontram as aplicações em RV desenvolvidas pelo usuário através do uso da linguagem Lua  e das funcionalidades oferecidas pelo framework TerraVR.

Referências

MATOSO, Douglas Oliveira. Interatividade em Tempo-Real com Simulações Tridimensionais e Georeferenciadas. 2007. Trabalho de Conclusão de Curso. (Graduação em Bacharelado em Ciência da Computação) – Universidade Federal de Ouro Preto. Orientador: Tiago Garcia de Senna Carneiro.

IERUSALIMSCHY, Roberto, “Programming in LUA”, lua.org, 2a. Edição, 1996

PULLEN, J. M. & Wood, D. C., “Networking Technology and DIS”, Proc. IEEE, vol. 83, no. 8, agosto, 1995, p.1156-1167.

 "
DengueME: Um software para modelagem e simulação da transmissão do vírus dengue.,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/download.jpg,"A dengue acomete cerca de 50 a 100 milhões de pessoas no mundo anualmente [WHO, 2012].  A doença é causa pela picada dos mosquitos Aedes aegypti e Aedes albopictus, que podem transmitir quatro tipos de vírus caso o mosquito esteja infectado: DENV-1, DENV-2, DENV-3 e DENV-4. No Brasil, o Aedes aegypti é o principal vetor da dengue. Como ainda não existe uma vacina eficaz contra nenhum destes vírus, o combate à dengue acontece principalmente pelo controle das populações deste mosquito.

O ciclo de vida do Aedes aegypti se dá em quatro estágios representados pela figura 1. Os estágios de ovo, larva e pupa acontecem na água, de preferência limpa. Portanto, qualquer recipiente que possa acumular água da chuva serve como criadouro para o mosquito.

 

Figura 1 – Ciclo de vida do mosquito Aedes aegypti. Fonte: portalsaofrancisco.com.br

 No Brasil, três formas de controle populacional do vetor são amplamente utilizadas: (1) controle químico por larvicida, (2) controle químico por adulticida e (3) controle mecânico através da remoção dos criadouros (Donalísio e Glasser, 2002). Os controles feitos por inseticidas, ao mesmo tempo em que são indispensáveis, causam resistência da população de mosquitos (Macoris et al. 1997; Braga e Valle, 2007) e impacto ambiental, muitas vezes comprometendo a efetividade da aplicação (Barreto, 2005). Por outro lado, várias pesquisas têm demonstrado que a remoção e destruição de criadouros é a maneira mais eficiente de controlar da doença. Desta forma, uma vez que o mosquito possui hábitos urbanos, a participação sistemática dos cidadãos no controle das populações do A. aegypti é indispensável.

Contudo, a falta de um programa de conscientização contínuo da população é um dos maiores agravantes dos índices de infestação do A. aegypti. As campanhas de erradicação são feitas basicamente nas épocas de aumento da população do mosquito com a chegada das chuvas no verão, o contrário do que muitos autores e pesquisas sugerem: as campanhas devem ser sistemáticas ao longo do ano [Otero et al., 2008].

O uso de modelos computacionais e matemáticos tem sido adotado por diversos estudos [Focks et al. 1993a; Yang et al. 2009; Lana et al. 2011] com o objetivo de buscar padrões de transmissão do vírus da dengue e de determinar quais os fatores ambientais, sociais e climáticos que, de fato, contribuem para essa dinâmica. Além disto, as simulações de tais modelos permitem avaliar a relação custo/benefício das diversas formas de controle da doença. As simulações podem ser utilizadas para determinar a melhor combinação dessas formas de controle, a intensidade e a frequência de cada uma. Entretanto, a busca pela estratégia ideal de controle da doença ainda está em aberto e muitas questões ainda precisam ser respondidas. A figura 2 mostra mapas de infestação de A. aegypti para o bairro de Higienópolis, no Rio de Janeiro, RJ. É possível comprar os mapas simulados (com fundo preto) ccom mapas construídos a partir de dados coletados em campo por meio de armadilhas (fundo branco). As simulações subestimam a infestação no período de seca e sobrestimam no período de chuva. No entanto, capturam o padrão espacial de infestação.

Figura 2. Resultados das simulações de infestação por Aedes aegypti no bairro de Higienópolis, Rio de Janeiro, RJ.

 

Dentro desse contexto, o DengueME é um ambiente computacional para simulação da dispersão da dengue e de populações de Aedes aegypti em um espaço geográfico real. Ele permite a projeção de cenários de risco e intervenção, a partir da aplicação de modelos matemáticos de dinâmica populacional e de transmissão da doença sobre representações realistas dos espaços urbanos, que incorporam imagens de satélites e mapas digitais armazenados em sistemas de informação geográfica.

Do ponto de vista técnico, o DengueME é um aplicativo desenvolvido sobre o ambiente de modelagem TerraME que fornece serviços específicos para o estudo da Dengue. Entre estes serviços destacam-se:

a) parametrização interativa dos modelos por meio de interfaces gráficas (entrada de dados de chuva, temperatura, características do ambiente urbano, etc);

b) customização interativa dos componentes de modelos (um modelo é formado por vários componentes cujas versões em uso podem variar de acordo com as necessidades de seu usuário, por exemplo, modelo de crescimento das populações de mosquitos, modelos de crescimento das populações humanas, modelo de mobilidade das populações humanas, modelos de dispersão viral, etc);

c) execução interativa dos modelos integrados para projeção de cenários e risco e intervenção.

É importante ressaltar que o DengueME tem como objetivo ser de fácil manipulação para permitir a qualquer pesquisador ou agente de saúde realizar análises voltadas ao combate da dengue. Essa iniciativa tem um objetivo maior de viabilizar e incentivar estudos nessa área, disponibilizando informações relevantes para as Secretarias de Saúde que poderão planejar suas ações de controle da doença.

O projeto DengueME vem sendo desenvolvido no âmbito da Rede PRONEX de Modelagem em Dengue financiada pelo Conselho Nacional de Pesquisa (CNPq). Os desenvolvimento do software é realizado pelo laboratório TerraLAB do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP) , em parceria com o Programa de Computação Científica (PROCC) da  Fundação Osvaldo Cruz  (FIOCRUZ)  e o Centro de Ciência do Sistema Terrestre (CCST) do Istituto Nacional de Pesquisas Espaciais (INPE).

Em breve contaremos um pouco mais sobre modelos para Dengue!

Referências:

Barreto C.F. 2005. Aedes aegypti – Resistencia aos Inseticidas Químicos e as Novas Alternativas de Controle. Revista Eletronica Faculdades Monte Belo, Goiás, ISSN 1808-8597, v.1, n.2, p. 62-73

Braga I.A., Valle D. 2007. Aedes aegypti: histórico do controle no Brasil. Epidemiologia e Serviços de Saúde; v. 16, p. 113-118.

Donalísio M.R., Glasser C.M. 2002. Vigilância Entomológica e Controle de Vetores do Dengue, Rev. Bras. Epidemiol. V. 5,p. 259-272

Focks D.A., Haile D.C., Daniels E., Moun G.A. 1993a. Dynamics life table model for Aedes aegypti: Analysis of the literature and model development. J. Med. Entomol. v.30, p. 1003–1018.

Lana, R.M., Carneiro, T.G.S., Honório, N.A. & Codeço, C.T. 2011. Multiscale Analysis and Modelling of Aedes Aegyti Population Spatial Dynamics. Journal of Information and Data Management, Brazilian Computer Society, v. 2, p. 211-221.

Macoris M.L.G., Mazine C.A.B., Andrighetti M.T.M., Yasumaro S. 1997. Factors favoring houseplant container infestation with Aedes aegypti larvae in Marília SP, Brazil. Rev Panam Salud Publica v. 1, p. 280-286.

Otero M., Schweigmann N., Solari H.G.2008. AStochastic Spatial Dynamical Model for Aedes aegypti, Bulletin of Mathematical Biology.

World Health Organization, 2012. http://www.who.int/mediacentre/factsheets/fs117/en/

Yang, H.M., Macoris, M.L.G., Galvani, K.C., Andrighetti, M.T.M. & Wanderley, D.M.V. 2009. Assessing the effects of temperature on the population of Aedes aegypti, the vector of dengue. Epidemiol. Infect., v. 137, p. 1188-1202."
Oportunidade: Inscrições abertas para o Mestrado em Análise e Modelagem de Sistemas Ambientais,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/modelagemUFMG.gif," 

Estão abertas as inscrições para o processo de seleção 2013 do programa de Mestrado em Análise e Modelagem de Sistemas Ambientais da Universidade Federal de Minas Gerais (UFMG).

O curso de mestrado em Análise e Modelagem de Sistemas Ambientais enfatiza a aplicação do instrumental do geoprocessamento, sensoriamento remoto e modelagem computacional para o estudo de sistemas ambientais – sistemas com expressão territorial -, sob uma visão integrada das inter-relações entre seus componentes socioeconômicos, políticos, culturais e naturais – meios físico e biótico.

Sob o comitê multidisciplinar da CAPES, o mestrado em Análise e Modelagem de Sistemas Ambientais abre oportunidades para graduados em diversos campos do saber, como no exemplo de ecólogos, engenheiros ambientais, analistas de sistema, matemáticos computacionais, estatísticos, geógrafos, geólogos, arquitetos, biólogos e muitos outros mais profissionais; todos candidatos a se tornarem analistas e/ou modeladores de sistemas ambientais.

Portanto, como objetivo, o mestrado busca formar profissionais capacitados para análise espacial e modelagem de sistemas, incluindo aí a concepção e solução de uma série de métodos analíticos e de simulação orientados à avaliação qualitativa e quantitativa de complexos temas ambientais em escalas local, regional e global, ou seja, um profissional com perfil de modelador capaz de transitar em diversos campos do saber em busca de soluções às questões ambientais e dirigidas à organização, planejamento e gestão do território.

Maiores informações no site"
Modelos de Crescimento e a Dinâmica de Populações,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/graficoCrescimentoCaotico2.png," 

Modelos de crescimento populacionais de tempo contínuo

A dinâmica de populações trata das variações, no tempo e no espaço, das densidades e tamanhos de população. Seu estudo visa à melhor compreensão da variação do número de indivíduos de uma determinada população e também, dos fatores que a influenciam em tais variações. Para isso, é necessário o conhecimento das taxas em que se verificam perdas e ganhos de indivíduos e identificar os processos que regulam a variação da população. O interesse neste estudo não é apenas teórico, sendo importante para o controle de pragas, criação de animais, etc (Rachide 2006).

Crescimento linear

O modelo mais simples de crescimento de uma população pode ser definido através de uma função de crescimento linear, onde o incremento da população responde a uma taxa fixa de crescimento (Figura 25), não correlacionada com o tamanho da população em questão, onde N é igual à população e r a taxa constante de incremento (Equação 1). Por exemplo, um rebanho bovíno em que a população cresce por inseminação artificial e o produtor realiza um número fixo deste procedimento a cada ano.

                              
Equação 1

 

 


Gráfico de crescimento linear.

 

Crescimento exponencial

O modelo exponencial de crescimento populacional foi descrito por Malthus (1798). Sua dinâmica surge de processos cumulativos (retroalimentação positiva ou de reforço). Esses processos ocorrem quando a variação liquida do sistema é proporcional ao seu estado atual, reforçando a tendência existente. Neste modelo uma população cresce de acordo com a taxa de natalidade constante r. O crescimento populacional exponencial é definido pela seguinte equação:

                         

Equação 2

 

onde dN / dt é a taxa instantânea de mudança populacional  e r é a taxa constante de mudança.

Curva de crescimento exponencial

 

Crescimento logístico

O matemático belga Pierre F. Verhurst propôs em 1837 um modelo que supõe que uma população poderá crescer até um limite máximo, a partir do qual tende a se estabilizar. O modelo proposto por Verhurst atende a uma condição em que a taxa de crescimento efetiva de uma população varia ao longo do tempo. Esse modelo é uma alternativa ao modelo de crescimento exponencial em que a taxa de crescimento é constante e não há limitação para o crescimento do tamanho da população.

Quase todos os textos introdutórios de ecologia usam a versão de tempo contínuo do modelo logístico como o modelo que descreve o crescimento populacional. Esse modelo é uma ferramenta útil para entender como funcionam várias populações, mas não descreve a dinâmica de algumas populações reais. Essas populações exibem comportamento mais complexo e suas taxas de crescimento também estão sob os efeitos de outras populações.

Sob as condições do modelo de tempo contínuo, o fluxo de crescimento se ajusta instantaneamente para desacelerar o crescimento populacional quando a população, N, se aproxima da capacidade de suporte, k, do ambiente que a envolve . Por isso, dificilmente uma população ultrapassa essa capacidade suporte. Qualquer perturbação que cause o crescimento acima desse limite, por exemplo, a entrada instantânea de novos indivíduos na população, é absorvida por um mecanismo de retroalimentação negativa que anula o fluxo de crescimento e permite que o fluxo de mortes rapidamente restaure a população ao nível k (Equação 3).

                 
   Equação 3

 

Curva de crescimento logístico

 

Modelos de crescimento populacionais de tempo discreto

Os modelos de tempo discreto evoluem em intervalos de tempo, geralmente, fixos e chamados passos. Presume-se que cada passo o sistema representado possa mudar instantaneamente seu estado. No modelo de tempo contínuo não existem passos, mudanças acontecem continuamente.

Dessa forma, a principal diferença entre modelos populacionais de tempo discreto e contínuo é que o modelo de tempo discreto descreve o número de indivíduos no próximo intervalo temporal, enquanto que o modelo de tempo contínuo descreve a taxa de mudança do tamanho populacional. Em ambos modelos logísticos, a constante k determina a capacidade de suporte do ambiente, ou seja, o número máximo de indivíduos que um habitat é capaz de sustentar.

No modelo populacional de tempo discreto, é mais concreta a possibilidade de uma população ultrapassar a capacidade de suporte de seu ambiente. Neste caso não existe o ajuste instantâneo no fluxo de crescimento populacional. O modelo de tempo discreto descreve uma retroalimentação negativa baseada na dependência da densidade populacional, que não é instantânea, ela acontece após atrasos no tempo. Esses atrasos podem ser entendidos como uma demora na resposta da população, ou sistema, em relação à aproximação da capacidade suporte. Por exemplo, em populações de plantas anuais ou insetos, os indivíduos crescem e reproduzem simultaneamente, mas os jovens não germinam ou eclodem até o próximo ano. Por isso, após um ano em que muitos indivíduos foram produzidos, a população pode ultrapassar a capacidade de suporte do ambiente.

No caso das formigas, onde podemos observar altas taxas de reprodução associadas a um tempo de geração extremamente curto, podemos perceber fortes associações com os modelos de tempo discreto.

Caos determinístico em ecologia de populações

Até recentemente os sistemas dinâmicos eram classificados em três categorias, segundo o padrão de variação no tempo das grandezas que caracterizam os seus estados:

a) estáveis, convergindo para um valor fixo;

b) periódicos, estabelecendo-se em oscilações periódicas; ou

c) imprevisíveis, caracterizado por flutuações irregulares, também denominados aleatórios ou ruidosos.

Porém, em 1963, Lorenz fez uma descoberta que surpreendeu o mundo, enquanto estudava um modelo de previsão do tempo. Seu modelo seguiu um curso que não se enquadrava como aleatório, periódico ou convergente, exibindo um comportamento bastante complexo, embora fosse definido apenas por poucas e simples equações diferenciais. A dinâmica gerada pelo modelo exibia uma característica não usual: dois pontos localizados a uma distância ínfima seguiam trajetórias bastante divergentes. Esta observação levou Lorenz a concluir que a previsão do tempo em um intervalo de tempo longo não seria possível. Sistemas como o de Lorenz são denominados “caótico determinísticos” ou simplesmente “caóticos”; ou seja, embora apresentem um comportamento aperiódico e imprevisível, a sua dinâmica é governada por equações diferenciais determinísticas simples.

A sensibilidade crítica às condições iniciais é a característica fundamental que diferencia os sistemas caóticos determinísticos dos sistemas que apresentam respostas aleatórias ou estocásticas. Para esses últimos sistemas, a mesma condição inicial pode conduzi-los a estados bastante distintos em pequenos intervalos de tempo, o que não ocorre nos sistemas caóticos determinísticos (Bricmont 1996).

Após a descoberta desse fenômeno nos estudos de sistemas físicos a evolução de sua aplicabilidade para a descrição de outros tipos de sistemas se mostrou extremamente interessante, em especial para os sistemas ecológicos. Em 1976, Robert May, trabalhando com modelos de crescimento populacional extremamente simples, não lineares e com atraso na resposta (discretos), mostrou que eles podiam ter um comportamento dinâmico fantasticamente complexo. Este comportamento incluía flutuações populacionais aparentemente aleatórias que eram geradas por modelos determinísticos, o chamado caos determinístico. As descobertas alcançadas por May na ecologia, e por vários outros pesquisadores em uma ampla variedade de outras ciências, provocaram uma das maiores revoluções científicas e filosóficas do século XX (Fernandez 2004).

Partindo de uma equação logística de tempo discreto (Equação 4), May estudou as possibilidades de flutuações populacionais para diferentes valores de r, onde cada valor representaria diferentes populações.

                Equação 4

           

 

Variando-se o valor da constante r, a iteração desta equação em Nt pode conduzir a soluções estáveis, periódicas ou caóticas . Na figura abaixo, em (a), observa-se uma solução estável. Em (b) tem-se oscilações tendendo a estabilidade. Em (c) tem-se soluções periódicas de período 2. Já em (d), observa-se uma solução aperiódica e imprevisível, característica dos sistemas caóticos. Nos anos que seguiram, os estudos realizados pelo físico matemático Mitchell Feigenbaum (1983) revelaram o processo de duplicação de períodos através do qual os sistemas dinâmicos passavam de um regime laminar e bem comportando para um regime de desordem ou caótico.

Gráficos de flutuações populacionais gerados a partir da equação logística em tempo discreto, nos quais Pop = Nt e  k = 100: (a) r = 1,2, (b) r = 3,0, (c) r =  3,5 e (d) r = 4,0.

Autoria 

Texto extraído e adaptado da dissertação de mestrado em Ecologia de Biomas Tropicias do biólogo Msc. Alexandre Bahia Gontijo, orienatado pelos professores Dr. Sérvio Ribeiro Pontes e Dr. Tiago Garcia de Senna Carneiro.

Referências Bibliográficas

 

R. RACHIDE, “Dinâmica de Populações: Um Breve Histórico”, Universidade Federal de Viçosa, III Bienal de SBM, 2006, Brasil

MALTHUS, T. 1798. An Essay on the Principle of Population. Printed for J. Johnson, inSt. Paul’s Church-Yard,London

VERHULTt, P.F. 1838. Notice sur la loi que la population suit dans son accroissement. Correspondances Mathematiques et Physiques, 10, 113-121.

LORENZ, E. 1963 Deterministic nonperiodic flow. J. Atmospheric Sci. 20, 130-141.

BRICMONT, J. 1996. Science of chaos or chaos in science?, em: http://xyz.lanl.gov/abs/chaodyn/9603009.

MAY, R.M. 1976. Simple mathematical models with very complicated dynamics. Nature, Vol. 261, p.459.

FERNANDEZ, F. 2004. O poema imperfeito: Crônicas de biologia, conservação da natureza e seus heróis. 2ª EDIÇÃO. Editora UFPR.

FEIGENBAUM, M. J. 1983. Universal behavior in nonlinear systems, in Order in chaos, Los Alamos, N.M., 1982, Phys. D 7 (1-3) (), 16-39.

Alexandre Bahia Gontijo. Estudo e modelagem das dinâmicas estruturais de assembleias de formigas tropicais em diferentes escalas ecológicas. 2008. Dissertação (Mestrado em Ecologia de Biomas Tropicais) – Universidade Federal de Ouro Preto, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Co-Orientador: Tiago Garcia de Senna Carneiro."
TerraME: Uma plataforma para modelagem e simulação das interações sociedade-natureza,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/amazon_deforestation.gif,"Na Terra, diversos sistemas naturais e sociais interagem e co-evoluem. A busca pelo entendimento dos fenômenos naturais desafia a ciência desde seu surgimento. Muitos cientistas buscaram conhecer e descrever  por meio de fórmulas matemáticas as regras que regem o comportamento destes fenômenos, ou seja, eles contruíram os modelos matemáticos desses fenômenos.  Nos campos da Física e da Química, muitos modelos evoluíram para representar de maneira realista os fenômenos observados. Dalton em 1807, modelou o átomo como uma partícula esférica e indivisível sem qualquer carga elétrica. Em 1913, Borh demonstrou que o átomo possuia um núcelo positivo cercado por eletrons com cargas negativas e organizados em órbitas com diferentes níveis de energia. A Terra já foi considerada achatada. Os modelos matemáticos conhecidos pela ciência permitem a construção de prognósticos futuros sobre muitos sistemas naturais. Para entender a importância da continua evolução de tais modelos, considere a utilidade dos modelos climáticos para o agronegócio e  para os sistemas de alerta de desastres naturais.

Apesar dos esforços da antropologia e da sociologia, o funcionamento dos sistemas sociais continua mal entendido. Contudo, atualmente os sistemas sociais são vistos como os principais direcionadores das mudanças por que passam os sistemas naturais. Em contrapartida, os sistemas naturais são os principais condicionadores do comportamento dos sistemas sociais. Desta maneira, os sistemas sociais afetam e são afetados pelos sistema naturais. Portanto, modelos matemático-computacionais capazes de simular as interações entre os sistemas sociais e naturais constituem ferramentas essenciais para os responsáveis pela definição de políticas que regulam o uso dos recursos Terrestres.

Na verdade, qualquer decisor do setor público ou privado pode se beneficiar de modelos computacionais capazes de simular cenários que avaliam os impactos de diferentes estratégias: (1) de uso de recursos – espaço, tempo, vegetação, água, etc.; ou (2) de controle e resposta a riscos – epidemias, incêndios, inundações, etc. No Brasil, o Instituto Nacional de Pesquisas Espaciais (INPE) utiliza o TerraME para responder questões relacionadas às Mudanças de Uso e Cobertura do Solo na região Amazônica [1][2][3], para responder questões relativas ao Monitoramento, Análise e Alerta de Riscos, e para responder questões relacionadas à Emissão de Gases de Efeito Estufa [4]. Pesquisadores da Fundação Osvaldo Cruz (FIOCRUZ) utilizam o TerraME para avaliar questões relativas ao Controle da Dengue [5].

TerraME Overview

O TerraME – TerraME Modeling Environment – (www.terrame.org) é um conjunto de ferramentas que apoiam todas as fases do processo de desenvolvimento de modelos ambientais, isto é, modelos que reproduzem o comportamento de processos naturais e sociais, mostrando suas interações e as mudança que promovem de forma diferenciada em cada localização do espaço geográfico. No TerraME, um modelo ambiental é visto como um micro-mundo, isto é, um mundo virtual cuja paisagem representa uma determinada região do planeta Terra.e cujos agentes, autômatos  ou sistemas que o habitam são capazes de simular atores e processos de mudanças reais. As ferramentas TerraME são desenvolvidas pelo TerraLAB – Laboratório para Modelagem e Simulação do Sistemas Terrestre do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP) em parceria com o Instituto Nacional de Pesquisas Espaciais (INPE). Elas são distribuídas gratuitamente para as plataformas Linux, Windows e Mac de 32 e 64 bits.

O principal diferencial do TerraME com relação as plataformas de modelagem ambiental disponíveis atualmente está no suporte ao desenvolvimento de modelos que consideram múltiplas escalas espaço-temporais e no suporte ao uso simultâneo a múltiplos paradigmas de modelagem: Teoria Geral de Sistemas, Teoria de Agentes e Teoria de Autômatos Celulares. Desta maneira, o modelador não se vê restringido pela a escolha de um único paradigma de modelagem e pode representar de maneira realista as diferentes escalas nas quais mudanças ocorrem e nas quais agem as forças direcionadoras destas mudança. Por exemplo, na Amazônia, o processo de desmatamento é influenciado localmente pela situação econômica dos pequenos produtores, enquanto que globalmente é influenciado pelos preços de commodities como soja e gado.

TerraME Graphical Interface for Modeling and Simulation

Para apoiar a concepção e o projeto de modelos ambientais, o TerraME oferece um ambiente integrado de desenvolvimento chamado TerraME GIMS – Graphical Interface for Modeling and Simulation que permite ao modelador descrever seu modelo visualmente, isto é, por meio de diagramas.  Estes diagramas explicitam as entidades consideradas no modelo, a maneira como estão organizadas hierarquicamente e como interagem umas com as outras. Desta forma, o TerraME GIMS facilita a verificação das premissas nas quais os modelos se baseiam. Ele as comunica aos tomadores de decisão e aos membros da equipe de desenvolvimento que é, rotineiramente, interdisciplinar. O TerraME GIMS é distribuído gratuitamente na forma de um plugin para a plataforma Eclipse (www.eclipse.org).

TerraME Interpreter

Para apoiar a construção de modelos realistas, o simulador TerraME é integrado com Sistema de Informação Geográfica (SIG) que permite ao modelador parametrizar os modelos a partir de bancos de dados geográficos, no qual séries temporais de imagens de satélites e de mapas digitais descrevem a região sob estudo. O simulador TerraME é distribuído na forma de um interpretador que recebe como entrada o programa que representa o modelo ambiental e, então, o executa produzindo dados de saída que são automaticamente armazenados no banco de dados geográfico. Para facilitar a manutenção e evolução dos modelos ambientais, os modelos TerraME são representados em uma linguagem de modelagem de altíssimo nível, chamada TerraML – TerraME Modeling Language, que estende a linguagem de programação Lua  e na qual é fácil descrever as propriedades do espaço e os atores e processos que as alteram. Nesta linguagem, um modelo pode ser facilmente decomposto em modelos mais simples que representam diferentes escalas de um mesmo processo.

TerraME – Modeling in Multiple Scales

Para apoiar a depuração dos modelos e a análise dos resultados por eles produzidos, o TerraME possui uma série de componentes denominados TerraME Observers. Estes componentes são capazes de exibir os resultados, em tempo-real, na forma de gráficos e mapas dinâmicos em duas e três dimensões.

TerraME Observers 

Para apoiar a análise de sensibilidade do modelo e a projeção de cenários simulados, o núcleo de simulação TerraME oferece uma versão de alto desempenho chamada TerraME  HPA – High Performance Architecture. Esta versão do simulador é capaz de tirar proveito do poder de processamento e armazenamento de arquiteturas de hardware multiprocessadas ou de rede de computadores. Desta forma, experimentos para análise de sensibilidade dos modelos ou para a simulação de diferentes cenários de mudança podem ser executas em paralelo e em menor tempo.

Para apoiar a calibração dos modelos ambientais, o TerraME oferece um conjunto de métodos para aferir o ajuste entre mapas reais e mapas simulados, além de um conjunto de métodos para otimizar os parâmetros dos modelos de forma que eles maximizem o ajuste entre os resultados das simulações e os dados observados.

 

Referências Bibliográficas

 

[1] G. Câmara, A. P. Aguiar, M. I. Escada, et al. Amazon Deforestation Models. Science, vol 307, 15 February 2005, pp. 1043-1044.

[2] A.P. Aguiar, G. Câmara, M.I.S. Escada, Spatial statistical analysis of land-use determinants in the Brazilian Amazon: exploring intra-regional heterogeneity. Ecological Modelling, vol 209(1-2):169–188, 2007.

[3] Evaldinólia Moreira, Sergio Costa, Ana Paula Aguiar, Gilberto Camara, Tiago Carneiro. Dynamical coupling of multiscale land change models, Landscape Ecology, 24(9), p. 1183-1194, 2009.

[4] Aguiar APD, Ometto JP, Nobre CA, Lapola DM, Almeida C, Vieira IC, Soares JV, Alvala R, Saatchi S, Valeriano V, Castilla-Rubio JC (2012) Modeling the spatial and temporal heterogeneity of deforestation-driven carbon emissions: the INPE-EM framework applied to the Brazilian Amazon. Global Change Biology (Print)

[5]  LANA, R. M. ; Carneiro, T. G. S. ; Honório, N. A. ; Codeço, C. T. . Multiscale analysis and modeling of Aedes aegypti population spatial dynamics. Journal of Information and Data Management,2(2), p.211-220, 2011."
"Estimativa de emissões brasileiras dos gases do efeito estufa baseiam-se em tecnologia do TerraLAB, o simulador TerraME",http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/download.jpg," 

O sistema INPE-EM (INPE – Emission Model) é um novo serviço do Instituto Nacional de Pesquisas Espaciais (INPE) que visa tornar disponíveis estimativas anuais de emissões de gases do efeito estufa (GEE) por mudanças de cobertura da terra no Brasil.

O arcabouço de modelagem INPE-EM foi construído no ambiente de modelagem TerraME, desenvolvido pelo TerraLAB em parceira com a Divisão de Processamento de Imagem (DPI) o Centro de Ciência do Sistema Terrestre (CCST) do INPE.

A plataforma de modelagem ambiental TerraME pemite que simulações sejam utilizadas para avaliar cenários futuros de mudanças e também para avaliar os impactos de diferentes políticas públicas. Ele oferece suporte simultâneo a vários paradigmas de modelagem ambiental: baseado em agentes, automatos celulares, teoria geral de sistemas, teoria de jogos, etc. É possivel densenvolver simulações em multiplas escalas espaciais e temporais, nas quais agentes autônomos e interativos alteram propriedades do espaço geográfico. Para ofercer mais realismo às simulações, o espaço geográfico pode ser representado por bancos de dados geográficos armazenados em Sistemas de Informação Geográfica, como a biblioteca TerraLib.

Para fazer download do TerraME clique aqui."
Workshop VANTs: ArDrone E Aplicações,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/IMG_20120814_140355.jpg,"No dia 22/08 foi organizado entre os laboratórios  TerraLAB e iMobilis o workshop de VANTs e aplicações. Vants são veículos terrestres, aquáticos e subaquáticos autônomos e aéreos não tripulados.

O aluno de graduação Johnnatan Messias fez uma apresentação sobre o “brinquedo”/plataforma ArDrone, um VANT comercial de baixo custo.

O ArDrone, fabricado pela empresa Parrot [http://www.parrot.com/usa/], é vendido a baixo custo, comparado com outros veículos de mesmo porte. Movido por quatro hélices, o veículo pode ser acionado pelo próprio computador e pilotado por controle remoto. Associado a ele existem varios jogos de realidade ampliada, que permite a manipulação do quadricoptero via tablet ou celular.

O ArDrone possui codigo livre, com sistema operacional Linux e uma API de programação para desenvolvimento de jogos e aplicações de realidade ampliada.

Durante o workshop, os pesquisadores do TerraLAB apresentaram propostas de integração entre o ArDrone e suas plataformas de simulação ambiental.

Os slides da apresentação [Slides:Projeto Imobilis – ArDrone ]

 

[youtube width=”600″ height=”365″ video_id=”aMwLjRYIjbU&amp;feature”]

 

 

 "
INPE e UFOP desenvolvem tecnologias que fundamentam políticas públicas em bases científicas,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/clue_amazonia_baseline.jpg,"O INPE e o TerraLAB – Laboratório para Modelagem e Simulação do Sistema Terrestre, do DECOM/UFOP são responsáveis por desenvolvimentos  tecnológicos que permitem que a definição de políticas públicas brasileiras seja feita com base em critérios científicos. Estas tecnologias vêm sendo utilizadas para definir as politicas nacionais de uso do solo na Amazônia e para definir as políticas externas com relação a emissões de gases de efeito estufa.

Nesta sexta-feira (18), no workshop do Global Land Project (GLP) – Land Use Transitions in South America: framing the present, preparing for a sustainable future, foi lançado oficialmente e disponibilizado na Internet o LuccME, uma ferramenta de código aberto para a construção e customização de modelos de mudança de uso e cobertura da terra. O LuccME é uma extensão do ambiente de modelagem TerraME.

O reconhecimento internacional a esses esforços vem com a mudança do escritório internacional do GLP para o Brasil. O GLP é uma iniciativa do International Human Dimensions Programme on Global Environmental Change (IHDP) e do International Geosphere–Biosphere Programme (IGBP), que possui escritório regional no INPE, para o estudo da interação entre homem e ambiente terrestre em prol do melhor entendimento científico das mudanças globais.

Veja matéria completa aqui."
Workshop de Metodologia de Testes: TerraLAB e IMobilis,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/IMG_20120814_140355.jpg," 

No dia 14/08/2012, os membros dos laboratórios  TerraLab e  iMobilis, coordenado pelo Prof. Dr. Tiago Garcia de Senna Carneiro e Prof. Dr. Ricardo Rabelo, respectivamente, reuniram-se para um workshop de metodologia de testes, no qual foram apresentados estudos sobre diferentes métodos feitos pelos participantes do TerraLab e os métodos de teste de software do iMobilis.

A Programação:

14:00 – 14:40h  – How Google Test Software (James Whittaker, Jason Arbon, Jeff Carollo) — Apresentador: Henrique – TerraLab [Slides:  How Google Tests Software cap 1]
14:50 – 15:30h –  Effective Methods for Software Testing (William E. Perry) — Apresentador: Breno – TerraLab  [Slides:  Effective Methods for Software Testing ]
15:40 – 16:20h – Metodologia de testes para sistemas embarcados – Uso de caso sistema uGuide — Apresentador: Gustavo Quintão – iMobilis [Slides: Metodologia de testes]
16:20 – 16:40  – COFFE BREAK
16:40 – 17:20h – The Agile Samurai: How Agile Masters Deliver Great Software (Jonathan Rasmusson) — Apresentador: Rafael – – TerraLab [Slides: AgileSamurai]
17:30 – 18:10h – Agile Testing: A Practical Guide for Testers and Agile Teams  (Lisa Crispin, Janet Gregory)  – Apresentador: Washington – TerraLab [Slides: Agile Testing]

Fotos do Workshop:

 

 

 

 

 

 

 

 "
O bem estar no ambiente de trabalho – TerraLAB,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/2012-08-22-20.20.10.jpg,"Ambiente de trabalho saudável é essencial para para qualidade de vida. Saúde e bem-estar no ambiente de trabalho exercem grande influência sobre a motivação, permanência, criatividade e produtividade de qualquer equipe. Contudo, sentir-se pressionado é um fato normal.

Novas atividades trazem novos desafios. Um aumento temporário da carga de trabalho ou um prazo que se aproxima podem incomodar. Por isso,  ambiente de trabalho deve ser  confortavel e organizado, previlegiar a criatividade  e bem estar membros das equipes de trabalho.

Pensando nisso,  o TerraLAB se organizou para mudar e crescer melhor. 

Fizemos uma nova decoração. Mapas dos século XVII enfeitam nossas paredes, uma mesinha de café espera pelos visitantes e a arte está por ali.

Nélia, a mascote do laboratório, veio dar vida ao ambiente.
Esperamos que gostem…"
Blog do TerraLAB foi lançado!,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/31.jpg,Bem vindo ao novo blog do TerraLAB. Encotre aqui novidades e muito mais!
